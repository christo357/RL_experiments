{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b8ec7399",
   "metadata": {},
   "source": [
    "## Different approaches to experience collection( n-step lookahead) from simple to complex."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cc8670e",
   "metadata": {},
   "source": [
    "### 1. Simple Rolling Buffer (most common)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55f2f67c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import gymnasium as gym\n",
    "from collections import deque\n",
    "\n",
    "def compute_n_step_returns(rewards, gamma, n_steps):\n",
    "    \"\"\"\n",
    "    Compute n-step discounted returns. \n",
    "    rewards: list of rewards [r_t, r_{t+1}, ..., r_{t+n-1}]\n",
    "    \"\"\"\n",
    "    result = 0.0\n",
    "    for i, r in enumerate(rewards):\n",
    "        result += (gamma ** i) * r\n",
    "    return result\n",
    "\n",
    "class SimpleNStepCollector:\n",
    "    \"\"\"\n",
    "    Minimal n-step experience collector.\n",
    "    Uses a rolling window approach.\n",
    "    \"\"\"\n",
    "    def __init__(self, gamma=0.99, n_steps=10):\n",
    "        self.gamma = gamma\n",
    "        self.n_steps = n_steps\n",
    "        \n",
    "        # Rolling buffers for the last n_steps\n",
    "        self. states = deque(maxlen=n_steps)\n",
    "        self.actions = deque(maxlen=n_steps)\n",
    "        self.rewards = deque(maxlen=n_steps)\n",
    "        \n",
    "        self.episode_rewards = []\n",
    "        self.current_episode_reward = 0.0\n",
    "    \n",
    "    def add(self, state, action, reward, done):\n",
    "        \"\"\"\n",
    "        Add a transition and return experience if ready.\n",
    "        \n",
    "        Returns:\n",
    "            experience dict or None\n",
    "        \"\"\"\n",
    "        self. states.append(state)\n",
    "        self.actions.append(action)\n",
    "        self.rewards.append(reward)\n",
    "        self.current_episode_reward += reward\n",
    "        \n",
    "        experience = None\n",
    "        \n",
    "        # Case 1: Episode ended - flush all remaining transitions\n",
    "        if done:\n",
    "            # Process all buffered transitions\n",
    "            experiences = []\n",
    "            for i in range(len(self.states)):\n",
    "                n_step_return = compute_n_step_returns(\n",
    "                    list(self.rewards)[i: ], \n",
    "                    self.gamma, \n",
    "                    self.n_steps\n",
    "                )\n",
    "                experiences.append({\n",
    "                    'state': self. states[i],\n",
    "                    'action': self.actions[i],\n",
    "                    'return': n_step_return,\n",
    "                    'done': True\n",
    "                })\n",
    "            \n",
    "            # Reset for next episode\n",
    "            self.episode_rewards. append(self.current_episode_reward)\n",
    "            self.current_episode_reward = 0.0\n",
    "            self.states.clear()\n",
    "            self.actions.clear()\n",
    "            self.rewards.clear()\n",
    "            \n",
    "            return experiences\n",
    "        \n",
    "        # Case 2: Buffer is full - emit oldest transition\n",
    "        elif len(self.states) == self.n_steps:\n",
    "            n_step_return = compute_n_step_returns(\n",
    "                list(self.rewards), \n",
    "                self.gamma, \n",
    "                self.n_steps\n",
    "            )\n",
    "            experience = {\n",
    "                'state': self.states[0],\n",
    "                'action': self.actions[0],\n",
    "                'return': n_step_return,\n",
    "                'done':  False\n",
    "            }\n",
    "            return [experience]\n",
    "        \n",
    "        # Case 3: Still accumulating\n",
    "        return None\n",
    "\n",
    "# Usage\n",
    "env = gym.make(\"CartPole-v1\")\n",
    "collector = SimpleNStepCollector(gamma=0.99, n_steps=10)\n",
    "\n",
    "state, _ = env.reset()\n",
    "batch = []\n",
    "\n",
    "for _ in range(1000):\n",
    "    action = env.action_space.sample()  # Replace with policy\n",
    "    next_state, reward, terminated, truncated, _ = env. step(action)\n",
    "    done = terminated or truncated\n",
    "    \n",
    "    experiences = collector.add(state, action, reward, done)\n",
    "    \n",
    "    if experiences:\n",
    "        batch. extend(experiences)\n",
    "    \n",
    "    if len(batch) >= 8:\n",
    "        # Train here\n",
    "        print(f\"Training on {len(batch)} experiences\")\n",
    "        batch.clear()\n",
    "    \n",
    "    if done:\n",
    "        state, _ = env.reset()\n",
    "    else:\n",
    "        state = next_state"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a9a8d0a",
   "metadata": {},
   "source": [
    "### 2. Using generators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca2e5ce2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def n_step_experience_generator(env, policy, gamma=0.99, n_steps=10):\n",
    "    \"\"\"\n",
    "    Generator that yields n-step experiences. \n",
    "    \n",
    "    Yields:\n",
    "        dict with 'state', 'action', 'return', 'done'\n",
    "    \"\"\"\n",
    "    while True:\n",
    "        # Start new episode\n",
    "        state, _ = env.reset()\n",
    "        done = False\n",
    "        \n",
    "        # Buffers for current episode\n",
    "        states = []\n",
    "        actions = []\n",
    "        rewards = []\n",
    "        \n",
    "        step = 0\n",
    "        \n",
    "        while not done:\n",
    "            # Get action from policy\n",
    "            action = policy(state)\n",
    "            \n",
    "            # Execute\n",
    "            next_state, reward, terminated, truncated, _ = env. step(action)\n",
    "            done = terminated or truncated\n",
    "            \n",
    "            # Store\n",
    "            states.append(state)\n",
    "            actions.append(action)\n",
    "            rewards.append(reward)\n",
    "            \n",
    "            # Yield experience if we have enough steps\n",
    "            if len(states) >= n_steps:\n",
    "                idx = 0  # Oldest transition\n",
    "                n_step_return = sum(\n",
    "                    (gamma ** i) * rewards[idx + i] \n",
    "                    for i in range(n_steps)\n",
    "                )\n",
    "                \n",
    "                yield {\n",
    "                    'state': states[idx],\n",
    "                    'action': actions[idx],\n",
    "                    'return': n_step_return,\n",
    "                    'done': False,\n",
    "                    'steps':  n_steps\n",
    "                }\n",
    "                \n",
    "                # Remove oldest\n",
    "                states.pop(0)\n",
    "                actions.pop(0)\n",
    "                rewards.pop(0)\n",
    "            \n",
    "            state = next_state\n",
    "            step += 1\n",
    "        \n",
    "        # Episode ended - flush remaining transitions\n",
    "        while len(states) > 0:\n",
    "            # Compute return for remaining steps\n",
    "            n_step_return = sum(\n",
    "                (gamma ** i) * rewards[i] \n",
    "                for i in range(len(rewards))\n",
    "            )\n",
    "            \n",
    "            yield {\n",
    "                'state': states[0],\n",
    "                'action':  actions[0],\n",
    "                'return': n_step_return,\n",
    "                'done': True,\n",
    "                'steps': len(states)\n",
    "            }\n",
    "            \n",
    "            # Remove oldest\n",
    "            states.pop(0)\n",
    "            actions.pop(0)\n",
    "            rewards.pop(0)\n",
    "\n",
    "# Usage\n",
    "def random_policy(state):\n",
    "    return env.action_space.sample()\n",
    "\n",
    "env = gym.make(\"CartPole-v1\")\n",
    "exp_gen = n_step_experience_generator(env, random_policy, gamma=0.99, n_steps=10)\n",
    "\n",
    "batch = []\n",
    "for exp in exp_gen:\n",
    "    batch.append(exp)\n",
    "    \n",
    "    if len(batch) >= 8:\n",
    "        # Train\n",
    "        print(f\"Batch of {len(batch)} experiences\")\n",
    "        batch.clear()\n",
    "        \n",
    "    if len(batch) > 100:  # Stop after some experiences\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e3b05df",
   "metadata": {},
   "source": [
    "### using Vectorized/Batch Approach (Most Efficient)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce19f894",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "class VectorizedNStepBuffer:\n",
    "    \"\"\"\n",
    "    Efficiently handles n-step returns using numpy arrays.\n",
    "    Best for multiple parallel environments.\n",
    "    \"\"\"\n",
    "    def __init__(self, buffer_size, state_shape, gamma=0.99, n_steps=10):\n",
    "        self.buffer_size = buffer_size\n",
    "        self.gamma = gamma\n",
    "        self.n_steps = n_steps\n",
    "        self.ptr = 0\n",
    "        self.size = 0\n",
    "        \n",
    "        # Pre-allocate arrays\n",
    "        self.states = np. zeros((buffer_size, *state_shape), dtype=np.float32)\n",
    "        self.actions = np.zeros(buffer_size, dtype=np. int64)\n",
    "        self.rewards = np.zeros(buffer_size, dtype=np.float32)\n",
    "        self. dones = np.zeros(buffer_size, dtype=np.bool_)\n",
    "        \n",
    "        # Precompute gamma powers for efficiency\n",
    "        self.gamma_powers = np.array([gamma ** i for i in range(n_steps)])\n",
    "    \n",
    "    def add(self, state, action, reward, done):\n",
    "        \"\"\"Add single transition\"\"\"\n",
    "        self.states[self. ptr] = state\n",
    "        self. actions[self.ptr] = action\n",
    "        self.rewards[self. ptr] = reward\n",
    "        self. dones[self.ptr] = done\n",
    "        \n",
    "        self.ptr = (self. ptr + 1) % self.buffer_size\n",
    "        self.size = min(self.size + 1, self.buffer_size)\n",
    "    \n",
    "    def compute_n_step_returns(self):\n",
    "        \"\"\"\n",
    "        Compute n-step returns for all transitions in buffer.\n",
    "        Returns arrays of (states, actions, returns, dones)\n",
    "        \"\"\"\n",
    "        if self. size < self.n_steps:\n",
    "            return None\n",
    "        \n",
    "        n_samples = self.size - self.n_steps + 1\n",
    "        returns = np.zeros(n_samples, dtype=np. float32)\n",
    "        \n",
    "        for i in range(n_samples):\n",
    "            # Check if episode ends within n_steps\n",
    "            done_idx = None\n",
    "            for j in range(self.n_steps):\n",
    "                if self.dones[i + j]:\n",
    "                    done_idx = j + 1\n",
    "                    break\n",
    "            \n",
    "            # Compute n-step return\n",
    "            if done_idx is not None: \n",
    "                # Episode ended, use only rewards up to termination\n",
    "                returns[i] = np.sum(\n",
    "                    self.gamma_powers[: done_idx] * self.rewards[i: i+done_idx]\n",
    "                )\n",
    "            else:\n",
    "                # Full n-step return\n",
    "                returns[i] = np.sum(\n",
    "                    self.gamma_powers * self.rewards[i:i+self.n_steps]\n",
    "                )\n",
    "        \n",
    "        return (\n",
    "            self.states[: n_samples]. copy(),\n",
    "            self.actions[: n_samples].copy(),\n",
    "            returns,\n",
    "            self.dones[:n_samples].copy()\n",
    "        )\n",
    "    \n",
    "    def sample_batch(self, batch_size):\n",
    "        \"\"\"Sample random batch of n-step experiences\"\"\"\n",
    "        data = self.compute_n_step_returns()\n",
    "        if data is None: \n",
    "            return None\n",
    "        \n",
    "        states, actions, returns, dones = data\n",
    "        \n",
    "        # Random sampling\n",
    "        indices = np.random.choice(len(states), size=batch_size, replace=False)\n",
    "        \n",
    "        return {\n",
    "            'states': states[indices],\n",
    "            'actions': actions[indices],\n",
    "            'returns': returns[indices],\n",
    "            'dones': dones[indices]\n",
    "        }\n",
    "    \n",
    "    def clear(self):\n",
    "        \"\"\"Clear buffer\"\"\"\n",
    "        self.ptr = 0\n",
    "        self.size = 0\n",
    "\n",
    "# Usage\n",
    "env = gym.make(\"CartPole-v1\")\n",
    "state_shape = env.observation_space. shape\n",
    "\n",
    "buffer = VectorizedNStepBuffer(\n",
    "    buffer_size=1000,\n",
    "    state_shape=state_shape,\n",
    "    gamma=0.99,\n",
    "    n_steps=10\n",
    ")\n",
    "\n",
    "# Collect experience\n",
    "state, _ = env.reset()\n",
    "for _ in range(500):\n",
    "    action = env.action_space.sample()\n",
    "    next_state, reward, terminated, truncated, _ = env.step(action)\n",
    "    done = terminated or truncated\n",
    "    \n",
    "    buffer. add(state, action, reward, done)\n",
    "    \n",
    "    if done:\n",
    "        state, _ = env.reset()\n",
    "    else:\n",
    "        state = next_state\n",
    "\n",
    "# Sample batch for training\n",
    "batch = buffer.sample_batch(batch_size=32)\n",
    "if batch: \n",
    "    print(f\"Sampled batch with shapes:\")\n",
    "    print(f\"  States: {batch['states'].shape}\")\n",
    "    print(f\"  Actions: {batch['actions'].shape}\")\n",
    "    print(f\"  Returns: {batch['returns'].shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8066750a",
   "metadata": {},
   "source": [
    "### simple episodic logic(clearest )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc429b92",
   "metadata": {},
   "outputs": [],
   "source": [
    "def collect_n_step_episode(env, policy_fn, gamma=0.99, n_steps=10):\n",
    "    \"\"\"\n",
    "    Collect full episode and compute n-step returns.\n",
    "    Returns list of (state, action, n_step_return) tuples.\n",
    "    \"\"\"\n",
    "    trajectory = []\n",
    "    state, _ = env.reset()\n",
    "    done = False\n",
    "    \n",
    "    while not done:\n",
    "        action = policy_fn(state)\n",
    "        next_state, reward, terminated, truncated, _ = env. step(action)\n",
    "        done = terminated or truncated\n",
    "        \n",
    "        trajectory.append({\n",
    "            'state': state,\n",
    "            'action': action,\n",
    "            'reward': reward,\n",
    "            'done':  done\n",
    "        })\n",
    "        \n",
    "        state = next_state\n",
    "    \n",
    "    # Compute n-step returns for each transition\n",
    "    experiences = []\n",
    "    for i in range(len(trajectory)):\n",
    "        # Look ahead up to n_steps or end of episode\n",
    "        horizon = min(n_steps, len(trajectory) - i)\n",
    "        \n",
    "        # Compute discounted return\n",
    "        n_step_return = sum(\n",
    "            (gamma ** j) * trajectory[i + j]['reward']\n",
    "            for j in range(horizon)\n",
    "        )\n",
    "        \n",
    "        experiences.append({\n",
    "            'state': trajectory[i]['state'],\n",
    "            'action': trajectory[i]['action'],\n",
    "            'return': n_step_return,\n",
    "            'done': trajectory[i]['done'],\n",
    "            'n_steps_used': horizon\n",
    "        })\n",
    "    \n",
    "    return experiences\n",
    "\n",
    "# Usage - clearest pattern\n",
    "env = gym.make(\"CartPole-v1\")\n",
    "\n",
    "def policy(state):\n",
    "    # Your policy here\n",
    "    return env.action_space.sample()\n",
    "\n",
    "batch = []\n",
    "for episode in range(10):\n",
    "    experiences = collect_n_step_episode(env, policy, gamma=0.99, n_steps=10)\n",
    "    batch.extend(experiences)\n",
    "    \n",
    "    if len(batch) >= 32:\n",
    "        # Train on batch\n",
    "        print(f\"Training on {len(batch)} experiences\")\n",
    "        # ...  training code ...\n",
    "        batch.clear()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
