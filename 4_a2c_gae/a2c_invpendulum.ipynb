{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ba147fe6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym \n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import typing as tt\n",
    "import torch  \n",
    "import torch.nn as nn \n",
    "import torch.nn.functional as F\n",
    "import wandb\n",
    "from collections import deque\n",
    "import os\n",
    "\n",
    "from gymnasium.wrappers import NormalizeObservation, NormalizeReward\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6753808a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # GAE already computed\n",
    "# advantages = gae_advantages.detach()\n",
    "\n",
    "# # Value target\n",
    "# value_target = advantages + values.detach()\n",
    "\n",
    "# # Policy loss\n",
    "# policy_loss = -(log_probs * advantages).mean()\n",
    "\n",
    "# # Value loss\n",
    "# value_loss = 0.5 * (values - value_target).pow(2).mean()\n",
    "\n",
    "# # Entropy bonus\n",
    "# entropy_loss = -entropy.mean()\n",
    "\n",
    "# # Total loss\n",
    "# loss = policy_loss + value_coef * value_loss + entropy_coef * entropy_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4c0216b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device : mps\n"
     ]
    }
   ],
   "source": [
    "HIDDEN_LAYER1  = 128\n",
    "# ALPHA = 0.95\n",
    "GAMMA = 0.9 # DISCOUNT FACTOR\n",
    "LAMBDA = 0.95 # FOR GAE\n",
    "LR = 1e-3\n",
    "# N_STEPS = 20\n",
    "ENV_ID = 'InvertedPendulum-v5'\n",
    "N_ENV = 1\n",
    "BATCH_SIZE = 64\n",
    "\n",
    "ENTROPY_BETA = 0.01\n",
    "ENTROPY_BETA_MIN = 0.001\n",
    "entropy_smoothing_factor = 0.05\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    device = 'cuda'\n",
    "elif torch.backends.mps.is_available():\n",
    "    device = 'mps'\n",
    "else:\n",
    "    device = 'cpu' \n",
    "print(f'Using device : {device}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "194e2732",
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make(ENV_ID)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "45c0b12a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PolicyNet(nn.Module):\n",
    "    def __init__(self, input_size, fc, action_dim, log_std_min, log_std_max):\n",
    "        super().__init__()\n",
    "        self.input_size = input_size\n",
    "        self.fc = fc\n",
    "        self.action_dim = action_dim\n",
    "        self.log_std_min = log_std_min\n",
    "        self.log_std_max = log_std_max\n",
    "        \n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(self.input_size, self.fc), \n",
    "            nn.ReLU(), \n",
    "            nn.Linear(self.fc, self.fc), \n",
    "            nn.ReLU()\n",
    "        )\n",
    "        \n",
    "        self.mu = nn.Linear(self.fc, self.action_dim)\n",
    "        \n",
    "        self.log_std = nn.Parameter(torch.zeros(self.action_dim))\n",
    "        \n",
    "        self.critic_head = nn.Linear(self.fc, 1)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.net(x)\n",
    "        \n",
    "        mu = self.mu(x)\n",
    "        std = torch.exp(torch.clamp(self.log_std, self.log_std_min, self.log_std_max))\n",
    "        std = std.expand_as(mu)\n",
    "        \n",
    "        val = self.critic_head(x)\n",
    "        return mu, std, val\n",
    "    \n",
    "class BetaScheduler:\n",
    "    def __init__(self, target_reward, beta_start, beta_min=1e-4, smoothing_factor=0.01):\n",
    "        self.target = target_reward\n",
    "        self.start = beta_start\n",
    "        self.min = beta_min\n",
    "        self.alpha = smoothing_factor\n",
    "        self.ema_reward = None  # Exponential Moving Average of Reward\n",
    "        self.current_beta = beta_start\n",
    "\n",
    "    def update(self, reward):\n",
    "        # 1. Update EMA of Reward\n",
    "        if self.ema_reward is None:\n",
    "            self.ema_reward = reward\n",
    "        else:\n",
    "            self.ema_reward = (self.ema_reward * (1 - self.alpha)) + (reward * self.alpha)\n",
    "        \n",
    "        # 2. Calculate Progress (0.0 to 1.0) based on EMA\n",
    "        # If ema_reward is negative, treat progress as 0\n",
    "        progress = max(0.0, min(1.0, self.ema_reward / self.target))\n",
    "        \n",
    "        # 3. Decay Beta linearly with progress\n",
    "        self.current_beta = self.start * (1.0 - progress)\n",
    "        \n",
    "        # 4. Clamp to minimum\n",
    "        self.current_beta = max(self.current_beta, self.min)\n",
    "        \n",
    "        return self.current_beta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "90e91e62",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_gae(deltas, dones, gamma, lam):\n",
    "    deltas_t = torch.tensor(deltas, dtype=torch.float32)\n",
    "    dones_t = torch.tensor(dones, dtype=torch.float32)\n",
    "\n",
    "    mask = 1.0 - dones_t\n",
    "\n",
    "    T = deltas_t.shape[0]\n",
    "    adv = torch.zeros_like(deltas_t)\n",
    "    gae = 0.0\n",
    "\n",
    "    for t in reversed(range(T)):\n",
    "        gae = deltas_t[t] + gamma * lam * mask[t] * gae\n",
    "        adv[t] = gae\n",
    "\n",
    "    return adv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c02d52fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy import dtype\n",
    "\n",
    "\n",
    "class NStepCollector:\n",
    "    def __init__(self, env, policy, gamma, lam, batch_size, device):\n",
    "        # super().__init__(self,)\n",
    "        self.env = env\n",
    "        self.policy = policy\n",
    "        self.gamma = gamma\n",
    "        self.lam = lam\n",
    "        self.batch_size = batch_size\n",
    "        self.device = device\n",
    "        \n",
    "        self.ep_reward = 0\n",
    "        \n",
    "        self.state, _ = env.reset()\n",
    "        action_low = torch.tensor(env.action_space.low, dtype=torch.float32, device=device)\n",
    "        action_high = torch.tensor(env.action_space.high, dtype=torch.float32, device=device)\n",
    "        self.action_bias = (action_high + action_low) / 2\n",
    "        self.action_scale = (action_high - action_low) / 2\n",
    "                \n",
    "        self.states = deque(maxlen=batch_size)\n",
    "        self.rawactions = deque(maxlen=batch_size)\n",
    "        self.rewards = deque(maxlen=batch_size)\n",
    "        self.terms = deque(maxlen=batch_size)\n",
    "        self.next_states = deque(maxlen=batch_size)\n",
    "        self.deltas = deque(maxlen=batch_size)\n",
    "        self.values = deque(maxlen=batch_size)\n",
    "        # sel\n",
    "\n",
    "            \n",
    "    def rollout(self):\n",
    "        while True:\n",
    "            # print(f\"state: {self.state}\")\n",
    "            \n",
    "            state_t = torch.tensor(self.state, dtype=torch.float32, device=device).unsqueeze(0)\n",
    "            \n",
    "            with torch.no_grad():\n",
    "                mu, std, _ = self.policy(state_t)\n",
    "            \n",
    "            # print('mu', mu)\n",
    "            # print('std', std)\n",
    "            dist = torch.distributions.Normal(mu,std)\n",
    "            u = dist.sample()\n",
    "            a = torch.tanh(u)\n",
    "            action = a*self.action_scale + self.action_bias\n",
    "            action_env = action.squeeze(0).detach().cpu().numpy()\n",
    "            \n",
    "            next_state, rew, term, trunc, info = self.env.step(action_env)\n",
    "            # self.next_state_t = torch.Tensor(next_state, dtype=torch.float32, device=device).unsqueeze(0)\n",
    "            self.ep_reward += rew\n",
    "            done = term or trunc\n",
    "            next_state_t = torch.tensor(next_state, dtype=torch.float32, device=device).unsqueeze(0)\n",
    "\n",
    "            if not term:\n",
    "                with torch.no_grad():\n",
    "                    _, _ , v_t = self.policy(state_t)\n",
    "                    _, _, v_t1 = self.policy(next_state_t)\n",
    "                v_t = v_t.item()\n",
    "                v_t1 = v_t1.item()\n",
    "            else:\n",
    "                with torch.no_grad():\n",
    "                    _, _ , v_t = self.policy(state_t)\n",
    "                v_t = v_t.item()\n",
    "                v_t1 = 0\n",
    "            \n",
    "            \n",
    "            delta = rew + self.gamma*v_t1 - v_t\n",
    "            \n",
    "            self.states.append(state_t)\n",
    "            self.rewards.append(rew)\n",
    "            self.terms.append(term)\n",
    "            self.next_states.append(next_state)\n",
    "            self.deltas.append(delta)\n",
    "            self.rawactions.append(u)\n",
    "            self.values.append(v_t)\n",
    "            if len(self.states)>=self.batch_size:\n",
    "                yield {\n",
    "                        'states':list(self.states), \n",
    "                        'actions':list(self.rawactions), \n",
    "                        'dones':list(self.terms), \n",
    "                        'deltas':list(self.deltas),\n",
    "                        'ep_reward': self.ep_reward if done else None,\n",
    "                        'values':list(self.values)\n",
    "                }\n",
    "            \n",
    "            else: \n",
    "                yield None\n",
    "                \n",
    "            self.state = next_state\n",
    "            if term or trunc:\n",
    "                # print(\"reset\")\n",
    "                self.state, _ = self.env.reset()\n",
    "                self.ep_reward = 0\n",
    "            \n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "802a0679",
   "metadata": {},
   "outputs": [],
   "source": [
    "# batch_states = []\n",
    "# batch_actions = []\n",
    "# batch_gae = []\n",
    "# batch_values = []\n",
    "\n",
    "\n",
    "policy = PolicyNet(\n",
    "    env.observation_space.shape[0], \n",
    "    HIDDEN_LAYER1, \n",
    "    env.action_space.shape[0], \n",
    "    log_std_min=-20, \n",
    "    log_std_max=1,\n",
    ").to(device)\n",
    "\n",
    "total_updates = 20000\n",
    "optimizer = torch.optim.Adam(policy.parameters(), lr = LR)\n",
    "scheduler = torch.optim.lr_scheduler.LambdaLR(\n",
    "    optimizer, lr_lambda=lambda upd: 1.0 - min(upd, total_updates) / total_updates\n",
    ")\n",
    "\n",
    "current_beta = ENTROPY_BETA\n",
    "beta_scheduler = BetaScheduler(\n",
    "    target_reward=950, \n",
    "    beta_start=ENTROPY_BETA, \n",
    "    beta_min=ENTROPY_BETA_MIN, \n",
    "    smoothing_factor=entropy_smoothing_factor\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "3dfa7a53",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode : 0 | step: 63 | episode reward : 6 | mean reward/100 eps : 6.0\n",
      "episode : 1 | step: 68 | episode reward : 4 | mean reward/100 eps : 5.0\n",
      "episode : 2 | step: 72 | episode reward : 3 | mean reward/100 eps : 4.333333333333333\n",
      "episode : 3 | step: 75 | episode reward : 2 | mean reward/100 eps : 3.75\n",
      "episode : 4 | step: 80 | episode reward : 4 | mean reward/100 eps : 3.8\n",
      "episode : 5 | step: 83 | episode reward : 2 | mean reward/100 eps : 3.5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/j2/56dmz8kj7h5fyr879140wlzr0000gn/T/ipykernel_75858/2499438311.py:42: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  batch_adv = torch.tensor(adv_list, dtype = torch.float32, device=device)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode : 6 | step: 94 | episode reward : 10 | mean reward/100 eps : 4.428571428571429\n",
      "episode : 7 | step: 124 | episode reward : 29 | mean reward/100 eps : 7.5\n",
      "episode : 8 | step: 158 | episode reward : 33 | mean reward/100 eps : 10.333333333333334\n",
      "episode : 9 | step: 194 | episode reward : 35 | mean reward/100 eps : 12.8\n",
      "episode : 10 | step: 220 | episode reward : 25 | mean reward/100 eps : 13.909090909090908\n",
      "episode : 11 | step: 225 | episode reward : 4 | mean reward/100 eps : 13.083333333333334\n",
      "episode : 12 | step: 230 | episode reward : 4 | mean reward/100 eps : 12.384615384615385\n",
      "episode : 13 | step: 234 | episode reward : 3 | mean reward/100 eps : 11.714285714285714\n",
      "episode : 14 | step: 237 | episode reward : 2 | mean reward/100 eps : 11.066666666666666\n",
      "episode : 15 | step: 240 | episode reward : 2 | mean reward/100 eps : 10.5\n",
      "episode : 16 | step: 243 | episode reward : 2 | mean reward/100 eps : 10.0\n",
      "episode : 17 | step: 246 | episode reward : 2 | mean reward/100 eps : 9.555555555555555\n",
      "episode : 18 | step: 249 | episode reward : 2 | mean reward/100 eps : 9.157894736842104\n",
      "episode : 19 | step: 252 | episode reward : 2 | mean reward/100 eps : 8.8\n",
      "episode : 20 | step: 255 | episode reward : 2 | mean reward/100 eps : 8.476190476190476\n",
      "episode : 21 | step: 258 | episode reward : 2 | mean reward/100 eps : 8.181818181818182\n",
      "episode : 22 | step: 261 | episode reward : 2 | mean reward/100 eps : 7.913043478260869\n",
      "episode : 23 | step: 264 | episode reward : 2 | mean reward/100 eps : 7.666666666666667\n",
      "episode : 24 | step: 267 | episode reward : 2 | mean reward/100 eps : 7.44\n",
      "episode : 25 | step: 270 | episode reward : 2 | mean reward/100 eps : 7.230769230769231\n",
      "episode : 26 | step: 273 | episode reward : 2 | mean reward/100 eps : 7.037037037037037\n",
      "episode : 27 | step: 276 | episode reward : 2 | mean reward/100 eps : 6.857142857142857\n",
      "episode : 28 | step: 279 | episode reward : 2 | mean reward/100 eps : 6.689655172413793\n",
      "episode : 29 | step: 282 | episode reward : 2 | mean reward/100 eps : 6.533333333333333\n",
      "episode : 30 | step: 285 | episode reward : 2 | mean reward/100 eps : 6.387096774193548\n",
      "episode : 31 | step: 288 | episode reward : 2 | mean reward/100 eps : 6.25\n",
      "episode : 32 | step: 291 | episode reward : 2 | mean reward/100 eps : 6.121212121212121\n",
      "episode : 33 | step: 294 | episode reward : 2 | mean reward/100 eps : 6.0\n",
      "episode : 34 | step: 297 | episode reward : 2 | mean reward/100 eps : 5.885714285714286\n",
      "episode : 35 | step: 300 | episode reward : 2 | mean reward/100 eps : 5.777777777777778\n",
      "episode : 36 | step: 303 | episode reward : 2 | mean reward/100 eps : 5.675675675675675\n",
      "episode : 37 | step: 306 | episode reward : 2 | mean reward/100 eps : 5.578947368421052\n",
      "episode : 38 | step: 309 | episode reward : 2 | mean reward/100 eps : 5.487179487179487\n",
      "episode : 39 | step: 312 | episode reward : 2 | mean reward/100 eps : 5.4\n",
      "episode : 40 | step: 315 | episode reward : 2 | mean reward/100 eps : 5.317073170731708\n",
      "episode : 41 | step: 318 | episode reward : 2 | mean reward/100 eps : 5.238095238095238\n",
      "episode : 42 | step: 321 | episode reward : 2 | mean reward/100 eps : 5.162790697674419\n",
      "episode : 43 | step: 324 | episode reward : 2 | mean reward/100 eps : 5.090909090909091\n",
      "episode : 44 | step: 327 | episode reward : 2 | mean reward/100 eps : 5.022222222222222\n",
      "episode : 45 | step: 330 | episode reward : 2 | mean reward/100 eps : 4.956521739130435\n",
      "episode : 46 | step: 333 | episode reward : 2 | mean reward/100 eps : 4.8936170212765955\n",
      "episode : 47 | step: 336 | episode reward : 2 | mean reward/100 eps : 4.833333333333333\n",
      "episode : 48 | step: 339 | episode reward : 2 | mean reward/100 eps : 4.775510204081633\n",
      "episode : 49 | step: 342 | episode reward : 2 | mean reward/100 eps : 4.72\n",
      "episode : 50 | step: 345 | episode reward : 2 | mean reward/100 eps : 4.666666666666667\n",
      "episode : 51 | step: 348 | episode reward : 2 | mean reward/100 eps : 4.615384615384615\n",
      "episode : 52 | step: 351 | episode reward : 2 | mean reward/100 eps : 4.566037735849057\n",
      "episode : 53 | step: 354 | episode reward : 2 | mean reward/100 eps : 4.518518518518518\n",
      "episode : 54 | step: 357 | episode reward : 2 | mean reward/100 eps : 4.472727272727273\n",
      "episode : 55 | step: 360 | episode reward : 2 | mean reward/100 eps : 4.428571428571429\n",
      "episode : 56 | step: 363 | episode reward : 2 | mean reward/100 eps : 4.385964912280702\n",
      "episode : 57 | step: 366 | episode reward : 2 | mean reward/100 eps : 4.344827586206897\n",
      "episode : 58 | step: 369 | episode reward : 2 | mean reward/100 eps : 4.305084745762712\n",
      "episode : 59 | step: 372 | episode reward : 2 | mean reward/100 eps : 4.266666666666667\n",
      "episode : 60 | step: 375 | episode reward : 2 | mean reward/100 eps : 4.229508196721311\n",
      "episode : 61 | step: 378 | episode reward : 2 | mean reward/100 eps : 4.193548387096774\n",
      "episode : 62 | step: 381 | episode reward : 2 | mean reward/100 eps : 4.158730158730159\n",
      "episode : 63 | step: 384 | episode reward : 2 | mean reward/100 eps : 4.125\n",
      "episode : 64 | step: 387 | episode reward : 2 | mean reward/100 eps : 4.092307692307692\n",
      "episode : 65 | step: 390 | episode reward : 2 | mean reward/100 eps : 4.0606060606060606\n",
      "episode : 66 | step: 393 | episode reward : 2 | mean reward/100 eps : 4.029850746268656\n",
      "episode : 67 | step: 396 | episode reward : 2 | mean reward/100 eps : 4.0\n",
      "episode : 68 | step: 399 | episode reward : 2 | mean reward/100 eps : 3.971014492753623\n",
      "episode : 69 | step: 402 | episode reward : 2 | mean reward/100 eps : 3.942857142857143\n",
      "episode : 70 | step: 405 | episode reward : 2 | mean reward/100 eps : 3.915492957746479\n",
      "episode : 71 | step: 408 | episode reward : 2 | mean reward/100 eps : 3.888888888888889\n",
      "episode : 72 | step: 411 | episode reward : 2 | mean reward/100 eps : 3.863013698630137\n",
      "episode : 73 | step: 414 | episode reward : 2 | mean reward/100 eps : 3.8378378378378377\n",
      "episode : 74 | step: 417 | episode reward : 2 | mean reward/100 eps : 3.8133333333333335\n",
      "episode : 75 | step: 420 | episode reward : 2 | mean reward/100 eps : 3.789473684210526\n",
      "episode : 76 | step: 423 | episode reward : 2 | mean reward/100 eps : 3.7662337662337664\n",
      "episode : 77 | step: 426 | episode reward : 2 | mean reward/100 eps : 3.7435897435897436\n",
      "episode : 78 | step: 429 | episode reward : 2 | mean reward/100 eps : 3.721518987341772\n",
      "episode : 79 | step: 432 | episode reward : 2 | mean reward/100 eps : 3.7\n",
      "episode : 80 | step: 435 | episode reward : 2 | mean reward/100 eps : 3.6790123456790123\n",
      "episode : 81 | step: 438 | episode reward : 2 | mean reward/100 eps : 3.658536585365854\n",
      "episode : 82 | step: 441 | episode reward : 2 | mean reward/100 eps : 3.63855421686747\n",
      "episode : 83 | step: 444 | episode reward : 2 | mean reward/100 eps : 3.619047619047619\n",
      "episode : 84 | step: 447 | episode reward : 2 | mean reward/100 eps : 3.6\n",
      "episode : 85 | step: 450 | episode reward : 2 | mean reward/100 eps : 3.5813953488372094\n",
      "episode : 86 | step: 453 | episode reward : 2 | mean reward/100 eps : 3.5632183908045976\n",
      "episode : 87 | step: 456 | episode reward : 2 | mean reward/100 eps : 3.5454545454545454\n",
      "episode : 88 | step: 459 | episode reward : 2 | mean reward/100 eps : 3.5280898876404496\n",
      "episode : 89 | step: 462 | episode reward : 2 | mean reward/100 eps : 3.511111111111111\n",
      "episode : 90 | step: 465 | episode reward : 2 | mean reward/100 eps : 3.4945054945054945\n",
      "episode : 91 | step: 468 | episode reward : 2 | mean reward/100 eps : 3.4782608695652173\n",
      "episode : 92 | step: 471 | episode reward : 2 | mean reward/100 eps : 3.4623655913978495\n",
      "episode : 93 | step: 474 | episode reward : 2 | mean reward/100 eps : 3.4468085106382977\n",
      "episode : 94 | step: 477 | episode reward : 2 | mean reward/100 eps : 3.431578947368421\n",
      "episode : 95 | step: 480 | episode reward : 2 | mean reward/100 eps : 3.4166666666666665\n",
      "episode : 96 | step: 483 | episode reward : 2 | mean reward/100 eps : 3.402061855670103\n",
      "episode : 97 | step: 486 | episode reward : 2 | mean reward/100 eps : 3.3877551020408165\n",
      "episode : 98 | step: 489 | episode reward : 2 | mean reward/100 eps : 3.3737373737373737\n",
      "episode : 99 | step: 492 | episode reward : 2 | mean reward/100 eps : 3.36\n",
      "episode : 100 | step: 495 | episode reward : 2 | mean reward/100 eps : 3.32\n",
      "episode : 101 | step: 498 | episode reward : 2 | mean reward/100 eps : 3.3\n",
      "episode : 102 | step: 501 | episode reward : 2 | mean reward/100 eps : 3.29\n",
      "episode : 103 | step: 504 | episode reward : 2 | mean reward/100 eps : 3.29\n",
      "episode : 104 | step: 507 | episode reward : 2 | mean reward/100 eps : 3.27\n",
      "episode : 105 | step: 510 | episode reward : 2 | mean reward/100 eps : 3.27\n",
      "episode : 106 | step: 513 | episode reward : 2 | mean reward/100 eps : 3.19\n",
      "episode : 107 | step: 516 | episode reward : 2 | mean reward/100 eps : 2.92\n",
      "episode : 108 | step: 519 | episode reward : 2 | mean reward/100 eps : 2.61\n",
      "episode : 109 | step: 522 | episode reward : 2 | mean reward/100 eps : 2.28\n",
      "episode : 110 | step: 525 | episode reward : 2 | mean reward/100 eps : 2.05\n",
      "episode : 111 | step: 528 | episode reward : 2 | mean reward/100 eps : 2.03\n",
      "episode : 112 | step: 531 | episode reward : 2 | mean reward/100 eps : 2.01\n",
      "episode : 113 | step: 534 | episode reward : 2 | mean reward/100 eps : 2.0\n",
      "episode : 114 | step: 537 | episode reward : 2 | mean reward/100 eps : 2.0\n",
      "episode : 115 | step: 540 | episode reward : 2 | mean reward/100 eps : 2.0\n",
      "episode : 116 | step: 543 | episode reward : 2 | mean reward/100 eps : 2.0\n",
      "episode : 117 | step: 546 | episode reward : 2 | mean reward/100 eps : 2.0\n",
      "episode : 118 | step: 549 | episode reward : 2 | mean reward/100 eps : 2.0\n",
      "episode : 119 | step: 552 | episode reward : 2 | mean reward/100 eps : 2.0\n",
      "episode : 120 | step: 555 | episode reward : 2 | mean reward/100 eps : 2.0\n",
      "episode : 121 | step: 558 | episode reward : 2 | mean reward/100 eps : 2.0\n",
      "episode : 122 | step: 561 | episode reward : 2 | mean reward/100 eps : 2.0\n",
      "episode : 123 | step: 564 | episode reward : 2 | mean reward/100 eps : 2.0\n",
      "episode : 124 | step: 567 | episode reward : 2 | mean reward/100 eps : 2.0\n",
      "episode : 125 | step: 570 | episode reward : 2 | mean reward/100 eps : 2.0\n",
      "episode : 126 | step: 573 | episode reward : 2 | mean reward/100 eps : 2.0\n",
      "episode : 127 | step: 576 | episode reward : 2 | mean reward/100 eps : 2.0\n",
      "episode : 128 | step: 579 | episode reward : 2 | mean reward/100 eps : 2.0\n",
      "episode : 129 | step: 582 | episode reward : 2 | mean reward/100 eps : 2.0\n",
      "episode : 130 | step: 585 | episode reward : 2 | mean reward/100 eps : 2.0\n",
      "episode : 131 | step: 588 | episode reward : 2 | mean reward/100 eps : 2.0\n",
      "episode : 132 | step: 591 | episode reward : 2 | mean reward/100 eps : 2.0\n",
      "episode : 133 | step: 594 | episode reward : 2 | mean reward/100 eps : 2.0\n",
      "episode : 134 | step: 597 | episode reward : 2 | mean reward/100 eps : 2.0\n",
      "episode : 135 | step: 600 | episode reward : 2 | mean reward/100 eps : 2.0\n",
      "episode : 136 | step: 603 | episode reward : 2 | mean reward/100 eps : 2.0\n",
      "episode : 137 | step: 606 | episode reward : 2 | mean reward/100 eps : 2.0\n",
      "episode : 138 | step: 609 | episode reward : 2 | mean reward/100 eps : 2.0\n",
      "episode : 139 | step: 612 | episode reward : 2 | mean reward/100 eps : 2.0\n",
      "episode : 140 | step: 615 | episode reward : 2 | mean reward/100 eps : 2.0\n",
      "episode : 141 | step: 618 | episode reward : 2 | mean reward/100 eps : 2.0\n",
      "episode : 142 | step: 621 | episode reward : 2 | mean reward/100 eps : 2.0\n",
      "episode : 143 | step: 624 | episode reward : 2 | mean reward/100 eps : 2.0\n",
      "episode : 144 | step: 627 | episode reward : 2 | mean reward/100 eps : 2.0\n",
      "episode : 145 | step: 630 | episode reward : 2 | mean reward/100 eps : 2.0\n",
      "episode : 146 | step: 633 | episode reward : 2 | mean reward/100 eps : 2.0\n",
      "episode : 147 | step: 636 | episode reward : 2 | mean reward/100 eps : 2.0\n",
      "episode : 148 | step: 639 | episode reward : 2 | mean reward/100 eps : 2.0\n",
      "episode : 149 | step: 642 | episode reward : 2 | mean reward/100 eps : 2.0\n",
      "episode : 150 | step: 645 | episode reward : 2 | mean reward/100 eps : 2.0\n",
      "episode : 151 | step: 648 | episode reward : 2 | mean reward/100 eps : 2.0\n",
      "episode : 152 | step: 651 | episode reward : 2 | mean reward/100 eps : 2.0\n",
      "episode : 153 | step: 654 | episode reward : 2 | mean reward/100 eps : 2.0\n",
      "episode : 154 | step: 657 | episode reward : 2 | mean reward/100 eps : 2.0\n",
      "episode : 155 | step: 660 | episode reward : 2 | mean reward/100 eps : 2.0\n",
      "episode : 156 | step: 663 | episode reward : 2 | mean reward/100 eps : 2.0\n",
      "episode : 157 | step: 666 | episode reward : 2 | mean reward/100 eps : 2.0\n",
      "episode : 158 | step: 669 | episode reward : 2 | mean reward/100 eps : 2.0\n",
      "episode : 159 | step: 672 | episode reward : 2 | mean reward/100 eps : 2.0\n",
      "episode : 160 | step: 675 | episode reward : 2 | mean reward/100 eps : 2.0\n",
      "episode : 161 | step: 678 | episode reward : 2 | mean reward/100 eps : 2.0\n",
      "episode : 162 | step: 681 | episode reward : 2 | mean reward/100 eps : 2.0\n",
      "episode : 163 | step: 684 | episode reward : 2 | mean reward/100 eps : 2.0\n",
      "episode : 164 | step: 687 | episode reward : 2 | mean reward/100 eps : 2.0\n",
      "episode : 165 | step: 690 | episode reward : 2 | mean reward/100 eps : 2.0\n",
      "episode : 166 | step: 693 | episode reward : 2 | mean reward/100 eps : 2.0\n",
      "episode : 167 | step: 696 | episode reward : 2 | mean reward/100 eps : 2.0\n",
      "episode : 168 | step: 699 | episode reward : 2 | mean reward/100 eps : 2.0\n",
      "episode : 169 | step: 702 | episode reward : 2 | mean reward/100 eps : 2.0\n",
      "episode : 170 | step: 705 | episode reward : 2 | mean reward/100 eps : 2.0\n",
      "episode : 171 | step: 708 | episode reward : 2 | mean reward/100 eps : 2.0\n",
      "episode : 172 | step: 711 | episode reward : 2 | mean reward/100 eps : 2.0\n",
      "episode : 173 | step: 714 | episode reward : 2 | mean reward/100 eps : 2.0\n",
      "episode : 174 | step: 717 | episode reward : 2 | mean reward/100 eps : 2.0\n",
      "episode : 175 | step: 720 | episode reward : 2 | mean reward/100 eps : 2.0\n",
      "episode : 176 | step: 723 | episode reward : 2 | mean reward/100 eps : 2.0\n",
      "episode : 177 | step: 726 | episode reward : 2 | mean reward/100 eps : 2.0\n",
      "episode : 178 | step: 729 | episode reward : 2 | mean reward/100 eps : 2.0\n",
      "episode : 179 | step: 732 | episode reward : 2 | mean reward/100 eps : 2.0\n",
      "episode : 180 | step: 735 | episode reward : 2 | mean reward/100 eps : 2.0\n",
      "episode : 181 | step: 738 | episode reward : 2 | mean reward/100 eps : 2.0\n",
      "episode : 182 | step: 741 | episode reward : 2 | mean reward/100 eps : 2.0\n",
      "episode : 183 | step: 744 | episode reward : 2 | mean reward/100 eps : 2.0\n",
      "episode : 184 | step: 747 | episode reward : 2 | mean reward/100 eps : 2.0\n",
      "episode : 185 | step: 750 | episode reward : 2 | mean reward/100 eps : 2.0\n",
      "episode : 186 | step: 753 | episode reward : 2 | mean reward/100 eps : 2.0\n",
      "episode : 187 | step: 756 | episode reward : 2 | mean reward/100 eps : 2.0\n",
      "episode : 188 | step: 759 | episode reward : 2 | mean reward/100 eps : 2.0\n",
      "episode : 189 | step: 762 | episode reward : 2 | mean reward/100 eps : 2.0\n",
      "episode : 190 | step: 765 | episode reward : 2 | mean reward/100 eps : 2.0\n",
      "episode : 191 | step: 768 | episode reward : 2 | mean reward/100 eps : 2.0\n",
      "episode : 192 | step: 771 | episode reward : 2 | mean reward/100 eps : 2.0\n",
      "episode : 193 | step: 774 | episode reward : 2 | mean reward/100 eps : 2.0\n",
      "episode : 194 | step: 777 | episode reward : 2 | mean reward/100 eps : 2.0\n",
      "episode : 195 | step: 780 | episode reward : 2 | mean reward/100 eps : 2.0\n",
      "episode : 196 | step: 783 | episode reward : 2 | mean reward/100 eps : 2.0\n",
      "episode : 197 | step: 786 | episode reward : 2 | mean reward/100 eps : 2.0\n",
      "episode : 198 | step: 789 | episode reward : 2 | mean reward/100 eps : 2.0\n",
      "episode : 199 | step: 792 | episode reward : 2 | mean reward/100 eps : 2.0\n",
      "episode : 200 | step: 795 | episode reward : 2 | mean reward/100 eps : 2.0\n",
      "episode : 201 | step: 798 | episode reward : 2 | mean reward/100 eps : 2.0\n",
      "episode : 202 | step: 801 | episode reward : 2 | mean reward/100 eps : 2.0\n",
      "episode : 203 | step: 804 | episode reward : 2 | mean reward/100 eps : 2.0\n",
      "episode : 204 | step: 807 | episode reward : 2 | mean reward/100 eps : 2.0\n",
      "episode : 205 | step: 810 | episode reward : 2 | mean reward/100 eps : 2.0\n",
      "episode : 206 | step: 813 | episode reward : 2 | mean reward/100 eps : 2.0\n",
      "episode : 207 | step: 816 | episode reward : 2 | mean reward/100 eps : 2.0\n",
      "episode : 208 | step: 819 | episode reward : 2 | mean reward/100 eps : 2.0\n",
      "episode : 209 | step: 822 | episode reward : 2 | mean reward/100 eps : 2.0\n",
      "episode : 210 | step: 825 | episode reward : 2 | mean reward/100 eps : 2.0\n",
      "episode : 211 | step: 828 | episode reward : 2 | mean reward/100 eps : 2.0\n",
      "episode : 212 | step: 831 | episode reward : 2 | mean reward/100 eps : 2.0\n",
      "episode : 213 | step: 834 | episode reward : 2 | mean reward/100 eps : 2.0\n",
      "episode : 214 | step: 837 | episode reward : 2 | mean reward/100 eps : 2.0\n",
      "episode : 215 | step: 840 | episode reward : 2 | mean reward/100 eps : 2.0\n",
      "episode : 216 | step: 843 | episode reward : 2 | mean reward/100 eps : 2.0\n",
      "episode : 217 | step: 846 | episode reward : 2 | mean reward/100 eps : 2.0\n",
      "episode : 218 | step: 849 | episode reward : 2 | mean reward/100 eps : 2.0\n",
      "episode : 219 | step: 852 | episode reward : 2 | mean reward/100 eps : 2.0\n",
      "episode : 220 | step: 855 | episode reward : 2 | mean reward/100 eps : 2.0\n",
      "episode : 221 | step: 858 | episode reward : 2 | mean reward/100 eps : 2.0\n",
      "episode : 222 | step: 861 | episode reward : 2 | mean reward/100 eps : 2.0\n",
      "episode : 223 | step: 864 | episode reward : 2 | mean reward/100 eps : 2.0\n",
      "episode : 224 | step: 867 | episode reward : 2 | mean reward/100 eps : 2.0\n",
      "episode : 225 | step: 870 | episode reward : 2 | mean reward/100 eps : 2.0\n",
      "episode : 226 | step: 873 | episode reward : 2 | mean reward/100 eps : 2.0\n",
      "episode : 227 | step: 876 | episode reward : 2 | mean reward/100 eps : 2.0\n",
      "episode : 228 | step: 879 | episode reward : 2 | mean reward/100 eps : 2.0\n",
      "episode : 229 | step: 882 | episode reward : 2 | mean reward/100 eps : 2.0\n",
      "episode : 230 | step: 885 | episode reward : 2 | mean reward/100 eps : 2.0\n",
      "episode : 231 | step: 888 | episode reward : 2 | mean reward/100 eps : 2.0\n",
      "episode : 232 | step: 891 | episode reward : 2 | mean reward/100 eps : 2.0\n",
      "episode : 233 | step: 894 | episode reward : 2 | mean reward/100 eps : 2.0\n",
      "episode : 234 | step: 897 | episode reward : 2 | mean reward/100 eps : 2.0\n",
      "episode : 235 | step: 900 | episode reward : 2 | mean reward/100 eps : 2.0\n",
      "episode : 236 | step: 903 | episode reward : 2 | mean reward/100 eps : 2.0\n",
      "episode : 237 | step: 906 | episode reward : 2 | mean reward/100 eps : 2.0\n",
      "episode : 238 | step: 909 | episode reward : 2 | mean reward/100 eps : 2.0\n",
      "episode : 239 | step: 912 | episode reward : 2 | mean reward/100 eps : 2.0\n",
      "episode : 240 | step: 915 | episode reward : 2 | mean reward/100 eps : 2.0\n",
      "episode : 241 | step: 918 | episode reward : 2 | mean reward/100 eps : 2.0\n",
      "episode : 242 | step: 921 | episode reward : 2 | mean reward/100 eps : 2.0\n",
      "episode : 243 | step: 924 | episode reward : 2 | mean reward/100 eps : 2.0\n",
      "episode : 244 | step: 927 | episode reward : 2 | mean reward/100 eps : 2.0\n",
      "episode : 245 | step: 930 | episode reward : 2 | mean reward/100 eps : 2.0\n",
      "episode : 246 | step: 933 | episode reward : 2 | mean reward/100 eps : 2.0\n",
      "episode : 247 | step: 936 | episode reward : 2 | mean reward/100 eps : 2.0\n",
      "episode : 248 | step: 939 | episode reward : 2 | mean reward/100 eps : 2.0\n",
      "episode : 249 | step: 942 | episode reward : 2 | mean reward/100 eps : 2.0\n",
      "episode : 250 | step: 945 | episode reward : 2 | mean reward/100 eps : 2.0\n",
      "episode : 251 | step: 948 | episode reward : 2 | mean reward/100 eps : 2.0\n",
      "episode : 252 | step: 951 | episode reward : 2 | mean reward/100 eps : 2.0\n",
      "episode : 253 | step: 954 | episode reward : 2 | mean reward/100 eps : 2.0\n",
      "episode : 254 | step: 957 | episode reward : 2 | mean reward/100 eps : 2.0\n",
      "episode : 255 | step: 960 | episode reward : 2 | mean reward/100 eps : 2.0\n",
      "episode : 256 | step: 963 | episode reward : 2 | mean reward/100 eps : 2.0\n",
      "episode : 257 | step: 966 | episode reward : 2 | mean reward/100 eps : 2.0\n",
      "episode : 258 | step: 969 | episode reward : 2 | mean reward/100 eps : 2.0\n",
      "episode : 259 | step: 972 | episode reward : 2 | mean reward/100 eps : 2.0\n",
      "episode : 260 | step: 975 | episode reward : 2 | mean reward/100 eps : 2.0\n",
      "episode : 261 | step: 978 | episode reward : 2 | mean reward/100 eps : 2.0\n",
      "episode : 262 | step: 981 | episode reward : 2 | mean reward/100 eps : 2.0\n",
      "episode : 263 | step: 984 | episode reward : 2 | mean reward/100 eps : 2.0\n",
      "episode : 264 | step: 987 | episode reward : 2 | mean reward/100 eps : 2.0\n",
      "episode : 265 | step: 990 | episode reward : 2 | mean reward/100 eps : 2.0\n",
      "episode : 266 | step: 993 | episode reward : 2 | mean reward/100 eps : 2.0\n",
      "episode : 267 | step: 996 | episode reward : 2 | mean reward/100 eps : 2.0\n",
      "episode : 268 | step: 999 | episode reward : 2 | mean reward/100 eps : 2.0\n",
      "episode : 269 | step: 1002 | episode reward : 2 | mean reward/100 eps : 2.0\n",
      "episode : 270 | step: 1005 | episode reward : 2 | mean reward/100 eps : 2.0\n",
      "episode : 271 | step: 1008 | episode reward : 2 | mean reward/100 eps : 2.0\n",
      "episode : 272 | step: 1011 | episode reward : 2 | mean reward/100 eps : 2.0\n",
      "episode : 273 | step: 1014 | episode reward : 2 | mean reward/100 eps : 2.0\n",
      "episode : 274 | step: 1017 | episode reward : 2 | mean reward/100 eps : 2.0\n",
      "episode : 275 | step: 1020 | episode reward : 2 | mean reward/100 eps : 2.0\n",
      "episode : 276 | step: 1023 | episode reward : 2 | mean reward/100 eps : 2.0\n",
      "episode : 277 | step: 1026 | episode reward : 2 | mean reward/100 eps : 2.0\n",
      "episode : 278 | step: 1029 | episode reward : 2 | mean reward/100 eps : 2.0\n",
      "episode : 279 | step: 1032 | episode reward : 2 | mean reward/100 eps : 2.0\n",
      "episode : 280 | step: 1035 | episode reward : 2 | mean reward/100 eps : 2.0\n",
      "episode : 281 | step: 1038 | episode reward : 2 | mean reward/100 eps : 2.0\n",
      "episode : 282 | step: 1041 | episode reward : 2 | mean reward/100 eps : 2.0\n",
      "episode : 283 | step: 1044 | episode reward : 2 | mean reward/100 eps : 2.0\n",
      "episode : 284 | step: 1047 | episode reward : 2 | mean reward/100 eps : 2.0\n",
      "episode : 285 | step: 1050 | episode reward : 2 | mean reward/100 eps : 2.0\n",
      "episode : 286 | step: 1053 | episode reward : 2 | mean reward/100 eps : 2.0\n",
      "episode : 287 | step: 1056 | episode reward : 2 | mean reward/100 eps : 2.0\n",
      "episode : 288 | step: 1059 | episode reward : 2 | mean reward/100 eps : 2.0\n",
      "episode : 289 | step: 1062 | episode reward : 2 | mean reward/100 eps : 2.0\n",
      "episode : 290 | step: 1065 | episode reward : 2 | mean reward/100 eps : 2.0\n",
      "episode : 291 | step: 1068 | episode reward : 2 | mean reward/100 eps : 2.0\n",
      "episode : 292 | step: 1071 | episode reward : 2 | mean reward/100 eps : 2.0\n",
      "episode : 293 | step: 1074 | episode reward : 2 | mean reward/100 eps : 2.0\n",
      "episode : 294 | step: 1077 | episode reward : 2 | mean reward/100 eps : 2.0\n",
      "episode : 295 | step: 1080 | episode reward : 2 | mean reward/100 eps : 2.0\n",
      "episode : 296 | step: 1083 | episode reward : 2 | mean reward/100 eps : 2.0\n",
      "episode : 297 | step: 1086 | episode reward : 2 | mean reward/100 eps : 2.0\n",
      "episode : 298 | step: 1089 | episode reward : 2 | mean reward/100 eps : 2.0\n",
      "episode : 299 | step: 1092 | episode reward : 2 | mean reward/100 eps : 2.0\n",
      "episode : 300 | step: 1095 | episode reward : 2 | mean reward/100 eps : 2.0\n",
      "episode : 301 | step: 1098 | episode reward : 2 | mean reward/100 eps : 2.0\n",
      "episode : 302 | step: 1101 | episode reward : 2 | mean reward/100 eps : 2.0\n",
      "episode : 303 | step: 1104 | episode reward : 2 | mean reward/100 eps : 2.0\n",
      "episode : 304 | step: 1107 | episode reward : 2 | mean reward/100 eps : 2.0\n",
      "episode : 305 | step: 1110 | episode reward : 2 | mean reward/100 eps : 2.0\n",
      "episode : 306 | step: 1113 | episode reward : 2 | mean reward/100 eps : 2.0\n",
      "episode : 307 | step: 1116 | episode reward : 2 | mean reward/100 eps : 2.0\n",
      "episode : 308 | step: 1119 | episode reward : 2 | mean reward/100 eps : 2.0\n",
      "episode : 309 | step: 1122 | episode reward : 2 | mean reward/100 eps : 2.0\n",
      "episode : 310 | step: 1125 | episode reward : 2 | mean reward/100 eps : 2.0\n",
      "episode : 311 | step: 1128 | episode reward : 2 | mean reward/100 eps : 2.0\n",
      "episode : 312 | step: 1131 | episode reward : 2 | mean reward/100 eps : 2.0\n",
      "episode : 313 | step: 1134 | episode reward : 2 | mean reward/100 eps : 2.0\n",
      "episode : 314 | step: 1137 | episode reward : 2 | mean reward/100 eps : 2.0\n",
      "episode : 315 | step: 1140 | episode reward : 2 | mean reward/100 eps : 2.0\n",
      "episode : 316 | step: 1143 | episode reward : 2 | mean reward/100 eps : 2.0\n",
      "episode : 317 | step: 1146 | episode reward : 2 | mean reward/100 eps : 2.0\n",
      "episode : 318 | step: 1149 | episode reward : 2 | mean reward/100 eps : 2.0\n",
      "episode : 319 | step: 1152 | episode reward : 2 | mean reward/100 eps : 2.0\n",
      "episode : 320 | step: 1155 | episode reward : 2 | mean reward/100 eps : 2.0\n",
      "episode : 321 | step: 1158 | episode reward : 2 | mean reward/100 eps : 2.0\n",
      "episode : 322 | step: 1161 | episode reward : 2 | mean reward/100 eps : 2.0\n",
      "episode : 323 | step: 1164 | episode reward : 2 | mean reward/100 eps : 2.0\n",
      "episode : 324 | step: 1167 | episode reward : 2 | mean reward/100 eps : 2.0\n",
      "episode : 325 | step: 1170 | episode reward : 2 | mean reward/100 eps : 2.0\n",
      "episode : 326 | step: 1173 | episode reward : 2 | mean reward/100 eps : 2.0\n",
      "episode : 327 | step: 1176 | episode reward : 2 | mean reward/100 eps : 2.0\n",
      "episode : 328 | step: 1179 | episode reward : 2 | mean reward/100 eps : 2.0\n",
      "episode : 329 | step: 1182 | episode reward : 2 | mean reward/100 eps : 2.0\n",
      "episode : 330 | step: 1185 | episode reward : 2 | mean reward/100 eps : 2.0\n",
      "episode : 331 | step: 1188 | episode reward : 2 | mean reward/100 eps : 2.0\n",
      "episode : 332 | step: 1191 | episode reward : 2 | mean reward/100 eps : 2.0\n",
      "episode : 333 | step: 1194 | episode reward : 2 | mean reward/100 eps : 2.0\n",
      "episode : 334 | step: 1197 | episode reward : 2 | mean reward/100 eps : 2.0\n",
      "episode : 335 | step: 1200 | episode reward : 2 | mean reward/100 eps : 2.0\n",
      "episode : 336 | step: 1203 | episode reward : 2 | mean reward/100 eps : 2.0\n",
      "episode : 337 | step: 1206 | episode reward : 2 | mean reward/100 eps : 2.0\n",
      "episode : 338 | step: 1209 | episode reward : 2 | mean reward/100 eps : 2.0\n",
      "episode : 339 | step: 1212 | episode reward : 2 | mean reward/100 eps : 2.0\n",
      "episode : 340 | step: 1215 | episode reward : 2 | mean reward/100 eps : 2.0\n",
      "episode : 341 | step: 1218 | episode reward : 2 | mean reward/100 eps : 2.0\n",
      "episode : 342 | step: 1221 | episode reward : 2 | mean reward/100 eps : 2.0\n",
      "episode : 343 | step: 1224 | episode reward : 2 | mean reward/100 eps : 2.0\n",
      "episode : 344 | step: 1227 | episode reward : 2 | mean reward/100 eps : 2.0\n",
      "episode : 345 | step: 1230 | episode reward : 2 | mean reward/100 eps : 2.0\n",
      "episode : 346 | step: 1233 | episode reward : 2 | mean reward/100 eps : 2.0\n",
      "episode : 347 | step: 1236 | episode reward : 2 | mean reward/100 eps : 2.0\n",
      "episode : 348 | step: 1239 | episode reward : 2 | mean reward/100 eps : 2.0\n",
      "episode : 349 | step: 1242 | episode reward : 2 | mean reward/100 eps : 2.0\n",
      "episode : 350 | step: 1245 | episode reward : 2 | mean reward/100 eps : 2.0\n",
      "episode : 351 | step: 1248 | episode reward : 2 | mean reward/100 eps : 2.0\n",
      "episode : 352 | step: 1251 | episode reward : 2 | mean reward/100 eps : 2.0\n",
      "episode : 353 | step: 1254 | episode reward : 2 | mean reward/100 eps : 2.0\n",
      "episode : 354 | step: 1257 | episode reward : 2 | mean reward/100 eps : 2.0\n",
      "episode : 355 | step: 1260 | episode reward : 2 | mean reward/100 eps : 2.0\n",
      "episode : 356 | step: 1263 | episode reward : 2 | mean reward/100 eps : 2.0\n",
      "episode : 357 | step: 1266 | episode reward : 2 | mean reward/100 eps : 2.0\n",
      "episode : 358 | step: 1269 | episode reward : 2 | mean reward/100 eps : 2.0\n",
      "episode : 359 | step: 1272 | episode reward : 2 | mean reward/100 eps : 2.0\n",
      "episode : 360 | step: 1275 | episode reward : 2 | mean reward/100 eps : 2.0\n",
      "episode : 361 | step: 1278 | episode reward : 2 | mean reward/100 eps : 2.0\n",
      "episode : 362 | step: 1281 | episode reward : 2 | mean reward/100 eps : 2.0\n",
      "episode : 363 | step: 1284 | episode reward : 2 | mean reward/100 eps : 2.0\n",
      "episode : 364 | step: 1287 | episode reward : 2 | mean reward/100 eps : 2.0\n",
      "episode : 365 | step: 1290 | episode reward : 2 | mean reward/100 eps : 2.0\n",
      "episode : 366 | step: 1293 | episode reward : 2 | mean reward/100 eps : 2.0\n",
      "episode : 367 | step: 1296 | episode reward : 2 | mean reward/100 eps : 2.0\n",
      "episode : 368 | step: 1299 | episode reward : 2 | mean reward/100 eps : 2.0\n",
      "episode : 369 | step: 1302 | episode reward : 2 | mean reward/100 eps : 2.0\n",
      "episode : 370 | step: 1305 | episode reward : 2 | mean reward/100 eps : 2.0\n",
      "episode : 371 | step: 1308 | episode reward : 2 | mean reward/100 eps : 2.0\n",
      "episode : 372 | step: 1311 | episode reward : 2 | mean reward/100 eps : 2.0\n",
      "episode : 373 | step: 1314 | episode reward : 2 | mean reward/100 eps : 2.0\n",
      "episode : 374 | step: 1317 | episode reward : 2 | mean reward/100 eps : 2.0\n",
      "episode : 375 | step: 1320 | episode reward : 2 | mean reward/100 eps : 2.0\n",
      "episode : 376 | step: 1323 | episode reward : 2 | mean reward/100 eps : 2.0\n",
      "episode : 377 | step: 1326 | episode reward : 2 | mean reward/100 eps : 2.0\n",
      "episode : 378 | step: 1329 | episode reward : 2 | mean reward/100 eps : 2.0\n",
      "episode : 379 | step: 1332 | episode reward : 2 | mean reward/100 eps : 2.0\n",
      "episode : 380 | step: 1335 | episode reward : 2 | mean reward/100 eps : 2.0\n",
      "episode : 381 | step: 1338 | episode reward : 2 | mean reward/100 eps : 2.0\n",
      "episode : 382 | step: 1341 | episode reward : 2 | mean reward/100 eps : 2.0\n",
      "episode : 383 | step: 1344 | episode reward : 2 | mean reward/100 eps : 2.0\n",
      "episode : 384 | step: 1347 | episode reward : 2 | mean reward/100 eps : 2.0\n",
      "episode : 385 | step: 1350 | episode reward : 2 | mean reward/100 eps : 2.0\n",
      "episode : 386 | step: 1353 | episode reward : 2 | mean reward/100 eps : 2.0\n",
      "episode : 387 | step: 1356 | episode reward : 2 | mean reward/100 eps : 2.0\n",
      "episode : 388 | step: 1359 | episode reward : 2 | mean reward/100 eps : 2.0\n",
      "episode : 389 | step: 1362 | episode reward : 2 | mean reward/100 eps : 2.0\n",
      "episode : 390 | step: 1365 | episode reward : 2 | mean reward/100 eps : 2.0\n",
      "episode : 391 | step: 1368 | episode reward : 2 | mean reward/100 eps : 2.0\n",
      "episode : 392 | step: 1371 | episode reward : 2 | mean reward/100 eps : 2.0\n",
      "episode : 393 | step: 1374 | episode reward : 2 | mean reward/100 eps : 2.0\n",
      "episode : 394 | step: 1377 | episode reward : 2 | mean reward/100 eps : 2.0\n",
      "episode : 395 | step: 1380 | episode reward : 2 | mean reward/100 eps : 2.0\n",
      "episode : 396 | step: 1383 | episode reward : 2 | mean reward/100 eps : 2.0\n",
      "episode : 397 | step: 1386 | episode reward : 2 | mean reward/100 eps : 2.0\n",
      "episode : 398 | step: 1389 | episode reward : 2 | mean reward/100 eps : 2.0\n",
      "episode : 399 | step: 1392 | episode reward : 2 | mean reward/100 eps : 2.0\n",
      "episode : 400 | step: 1395 | episode reward : 2 | mean reward/100 eps : 2.0\n",
      "episode : 401 | step: 1398 | episode reward : 2 | mean reward/100 eps : 2.0\n",
      "episode : 402 | step: 1401 | episode reward : 2 | mean reward/100 eps : 2.0\n",
      "episode : 403 | step: 1404 | episode reward : 2 | mean reward/100 eps : 2.0\n",
      "episode : 404 | step: 1407 | episode reward : 2 | mean reward/100 eps : 2.0\n",
      "episode : 405 | step: 1410 | episode reward : 2 | mean reward/100 eps : 2.0\n",
      "episode : 406 | step: 1413 | episode reward : 2 | mean reward/100 eps : 2.0\n",
      "episode : 407 | step: 1416 | episode reward : 2 | mean reward/100 eps : 2.0\n",
      "episode : 408 | step: 1419 | episode reward : 2 | mean reward/100 eps : 2.0\n",
      "episode : 409 | step: 1422 | episode reward : 2 | mean reward/100 eps : 2.0\n",
      "episode : 410 | step: 1425 | episode reward : 2 | mean reward/100 eps : 2.0\n",
      "episode : 411 | step: 1428 | episode reward : 2 | mean reward/100 eps : 2.0\n",
      "episode : 412 | step: 1431 | episode reward : 2 | mean reward/100 eps : 2.0\n",
      "episode : 413 | step: 1434 | episode reward : 2 | mean reward/100 eps : 2.0\n",
      "episode : 414 | step: 1437 | episode reward : 2 | mean reward/100 eps : 2.0\n",
      "episode : 415 | step: 1440 | episode reward : 2 | mean reward/100 eps : 2.0\n",
      "episode : 416 | step: 1443 | episode reward : 2 | mean reward/100 eps : 2.0\n",
      "episode : 417 | step: 1446 | episode reward : 2 | mean reward/100 eps : 2.0\n",
      "episode : 418 | step: 1449 | episode reward : 2 | mean reward/100 eps : 2.0\n",
      "episode : 419 | step: 1452 | episode reward : 2 | mean reward/100 eps : 2.0\n",
      "episode : 420 | step: 1455 | episode reward : 2 | mean reward/100 eps : 2.0\n",
      "episode : 421 | step: 1458 | episode reward : 2 | mean reward/100 eps : 2.0\n",
      "episode : 422 | step: 1461 | episode reward : 2 | mean reward/100 eps : 2.0\n",
      "episode : 423 | step: 1464 | episode reward : 2 | mean reward/100 eps : 2.0\n",
      "episode : 424 | step: 1467 | episode reward : 2 | mean reward/100 eps : 2.0\n",
      "episode : 425 | step: 1470 | episode reward : 2 | mean reward/100 eps : 2.0\n",
      "episode : 426 | step: 1473 | episode reward : 2 | mean reward/100 eps : 2.0\n",
      "episode : 427 | step: 1476 | episode reward : 2 | mean reward/100 eps : 2.0\n",
      "episode : 428 | step: 1479 | episode reward : 2 | mean reward/100 eps : 2.0\n",
      "episode : 429 | step: 1482 | episode reward : 2 | mean reward/100 eps : 2.0\n",
      "episode : 430 | step: 1485 | episode reward : 2 | mean reward/100 eps : 2.0\n",
      "episode : 431 | step: 1488 | episode reward : 2 | mean reward/100 eps : 2.0\n",
      "episode : 432 | step: 1491 | episode reward : 2 | mean reward/100 eps : 2.0\n",
      "episode : 433 | step: 1494 | episode reward : 2 | mean reward/100 eps : 2.0\n",
      "episode : 434 | step: 1497 | episode reward : 2 | mean reward/100 eps : 2.0\n",
      "episode : 435 | step: 1500 | episode reward : 2 | mean reward/100 eps : 2.0\n",
      "episode : 436 | step: 1503 | episode reward : 2 | mean reward/100 eps : 2.0\n",
      "episode : 437 | step: 1506 | episode reward : 2 | mean reward/100 eps : 2.0\n",
      "episode : 438 | step: 1509 | episode reward : 2 | mean reward/100 eps : 2.0\n",
      "episode : 439 | step: 1512 | episode reward : 2 | mean reward/100 eps : 2.0\n",
      "episode : 440 | step: 1515 | episode reward : 2 | mean reward/100 eps : 2.0\n",
      "episode : 441 | step: 1518 | episode reward : 2 | mean reward/100 eps : 2.0\n",
      "episode : 442 | step: 1521 | episode reward : 2 | mean reward/100 eps : 2.0\n",
      "episode : 443 | step: 1524 | episode reward : 2 | mean reward/100 eps : 2.0\n",
      "episode : 444 | step: 1527 | episode reward : 2 | mean reward/100 eps : 2.0\n",
      "episode : 445 | step: 1530 | episode reward : 2 | mean reward/100 eps : 2.0\n",
      "episode : 446 | step: 1533 | episode reward : 2 | mean reward/100 eps : 2.0\n",
      "episode : 447 | step: 1536 | episode reward : 2 | mean reward/100 eps : 2.0\n",
      "episode : 448 | step: 1539 | episode reward : 2 | mean reward/100 eps : 2.0\n",
      "episode : 449 | step: 1542 | episode reward : 2 | mean reward/100 eps : 2.0\n",
      "episode : 450 | step: 1545 | episode reward : 2 | mean reward/100 eps : 2.0\n",
      "episode : 451 | step: 1548 | episode reward : 2 | mean reward/100 eps : 2.0\n",
      "episode : 452 | step: 1551 | episode reward : 2 | mean reward/100 eps : 2.0\n",
      "episode : 453 | step: 1554 | episode reward : 2 | mean reward/100 eps : 2.0\n",
      "episode : 454 | step: 1557 | episode reward : 2 | mean reward/100 eps : 2.0\n",
      "episode : 455 | step: 1560 | episode reward : 2 | mean reward/100 eps : 2.0\n",
      "episode : 456 | step: 1563 | episode reward : 2 | mean reward/100 eps : 2.0\n",
      "episode : 457 | step: 1566 | episode reward : 2 | mean reward/100 eps : 2.0\n",
      "episode : 458 | step: 1569 | episode reward : 2 | mean reward/100 eps : 2.0\n",
      "episode : 459 | step: 1572 | episode reward : 2 | mean reward/100 eps : 2.0\n",
      "episode : 460 | step: 1575 | episode reward : 2 | mean reward/100 eps : 2.0\n",
      "episode : 461 | step: 1578 | episode reward : 2 | mean reward/100 eps : 2.0\n",
      "episode : 462 | step: 1581 | episode reward : 2 | mean reward/100 eps : 2.0\n",
      "episode : 463 | step: 1584 | episode reward : 2 | mean reward/100 eps : 2.0\n",
      "episode : 464 | step: 1587 | episode reward : 2 | mean reward/100 eps : 2.0\n",
      "episode : 465 | step: 1590 | episode reward : 2 | mean reward/100 eps : 2.0\n",
      "episode : 466 | step: 1593 | episode reward : 2 | mean reward/100 eps : 2.0\n",
      "episode : 467 | step: 1596 | episode reward : 2 | mean reward/100 eps : 2.0\n",
      "episode : 468 | step: 1599 | episode reward : 2 | mean reward/100 eps : 2.0\n",
      "episode : 469 | step: 1602 | episode reward : 2 | mean reward/100 eps : 2.0\n",
      "episode : 470 | step: 1605 | episode reward : 2 | mean reward/100 eps : 2.0\n",
      "episode : 471 | step: 1608 | episode reward : 2 | mean reward/100 eps : 2.0\n",
      "episode : 472 | step: 1611 | episode reward : 2 | mean reward/100 eps : 2.0\n",
      "episode : 473 | step: 1614 | episode reward : 2 | mean reward/100 eps : 2.0\n",
      "episode : 474 | step: 1617 | episode reward : 2 | mean reward/100 eps : 2.0\n",
      "episode : 475 | step: 1620 | episode reward : 2 | mean reward/100 eps : 2.0\n",
      "episode : 476 | step: 1623 | episode reward : 2 | mean reward/100 eps : 2.0\n",
      "episode : 477 | step: 1626 | episode reward : 2 | mean reward/100 eps : 2.0\n",
      "episode : 478 | step: 1629 | episode reward : 2 | mean reward/100 eps : 2.0\n",
      "episode : 479 | step: 1632 | episode reward : 2 | mean reward/100 eps : 2.0\n",
      "episode : 480 | step: 1635 | episode reward : 2 | mean reward/100 eps : 2.0\n",
      "episode : 481 | step: 1638 | episode reward : 2 | mean reward/100 eps : 2.0\n",
      "episode : 482 | step: 1641 | episode reward : 2 | mean reward/100 eps : 2.0\n",
      "episode : 483 | step: 1644 | episode reward : 2 | mean reward/100 eps : 2.0\n",
      "episode : 484 | step: 1647 | episode reward : 2 | mean reward/100 eps : 2.0\n",
      "episode : 485 | step: 1650 | episode reward : 2 | mean reward/100 eps : 2.0\n",
      "episode : 486 | step: 1653 | episode reward : 2 | mean reward/100 eps : 2.0\n",
      "episode : 487 | step: 1656 | episode reward : 2 | mean reward/100 eps : 2.0\n",
      "episode : 488 | step: 1659 | episode reward : 2 | mean reward/100 eps : 2.0\n",
      "episode : 489 | step: 1662 | episode reward : 2 | mean reward/100 eps : 2.0\n",
      "episode : 490 | step: 1665 | episode reward : 2 | mean reward/100 eps : 2.0\n",
      "episode : 491 | step: 1668 | episode reward : 2 | mean reward/100 eps : 2.0\n",
      "episode : 492 | step: 1671 | episode reward : 2 | mean reward/100 eps : 2.0\n",
      "episode : 493 | step: 1674 | episode reward : 2 | mean reward/100 eps : 2.0\n",
      "episode : 494 | step: 1677 | episode reward : 2 | mean reward/100 eps : 2.0\n",
      "episode : 495 | step: 1680 | episode reward : 2 | mean reward/100 eps : 2.0\n",
      "episode : 496 | step: 1683 | episode reward : 2 | mean reward/100 eps : 2.0\n",
      "episode : 497 | step: 1686 | episode reward : 2 | mean reward/100 eps : 2.0\n",
      "episode : 498 | step: 1689 | episode reward : 2 | mean reward/100 eps : 2.0\n",
      "episode : 499 | step: 1692 | episode reward : 2 | mean reward/100 eps : 2.0\n",
      "episode : 500 | step: 1695 | episode reward : 2 | mean reward/100 eps : 2.0\n",
      "episode : 501 | step: 1698 | episode reward : 2 | mean reward/100 eps : 2.0\n",
      "episode : 502 | step: 1701 | episode reward : 2 | mean reward/100 eps : 2.0\n",
      "episode : 503 | step: 1704 | episode reward : 2 | mean reward/100 eps : 2.0\n",
      "episode : 504 | step: 1707 | episode reward : 2 | mean reward/100 eps : 2.0\n",
      "episode : 505 | step: 1710 | episode reward : 2 | mean reward/100 eps : 2.0\n",
      "episode : 506 | step: 1713 | episode reward : 2 | mean reward/100 eps : 2.0\n",
      "episode : 507 | step: 1716 | episode reward : 2 | mean reward/100 eps : 2.0\n",
      "episode : 508 | step: 1719 | episode reward : 2 | mean reward/100 eps : 2.0\n",
      "episode : 509 | step: 1722 | episode reward : 2 | mean reward/100 eps : 2.0\n",
      "episode : 510 | step: 1725 | episode reward : 2 | mean reward/100 eps : 2.0\n",
      "episode : 511 | step: 1728 | episode reward : 2 | mean reward/100 eps : 2.0\n",
      "episode : 512 | step: 1731 | episode reward : 2 | mean reward/100 eps : 2.0\n",
      "episode : 513 | step: 1734 | episode reward : 2 | mean reward/100 eps : 2.0\n",
      "episode : 514 | step: 1737 | episode reward : 2 | mean reward/100 eps : 2.0\n",
      "episode : 515 | step: 1740 | episode reward : 2 | mean reward/100 eps : 2.0\n",
      "episode : 516 | step: 1743 | episode reward : 2 | mean reward/100 eps : 2.0\n",
      "episode : 517 | step: 1746 | episode reward : 2 | mean reward/100 eps : 2.0\n",
      "episode : 518 | step: 1749 | episode reward : 2 | mean reward/100 eps : 2.0\n",
      "episode : 519 | step: 1752 | episode reward : 2 | mean reward/100 eps : 2.0\n",
      "episode : 520 | step: 1755 | episode reward : 2 | mean reward/100 eps : 2.0\n",
      "episode : 521 | step: 1758 | episode reward : 2 | mean reward/100 eps : 2.0\n",
      "episode : 522 | step: 1761 | episode reward : 2 | mean reward/100 eps : 2.0\n",
      "episode : 523 | step: 1764 | episode reward : 2 | mean reward/100 eps : 2.0\n",
      "episode : 524 | step: 1767 | episode reward : 2 | mean reward/100 eps : 2.0\n",
      "episode : 525 | step: 1770 | episode reward : 2 | mean reward/100 eps : 2.0\n",
      "episode : 526 | step: 1773 | episode reward : 2 | mean reward/100 eps : 2.0\n",
      "episode : 527 | step: 1776 | episode reward : 2 | mean reward/100 eps : 2.0\n",
      "episode : 528 | step: 1779 | episode reward : 2 | mean reward/100 eps : 2.0\n",
      "episode : 529 | step: 1782 | episode reward : 2 | mean reward/100 eps : 2.0\n",
      "episode : 530 | step: 1785 | episode reward : 2 | mean reward/100 eps : 2.0\n",
      "episode : 531 | step: 1788 | episode reward : 2 | mean reward/100 eps : 2.0\n",
      "episode : 532 | step: 1791 | episode reward : 2 | mean reward/100 eps : 2.0\n",
      "episode : 533 | step: 1794 | episode reward : 2 | mean reward/100 eps : 2.0\n",
      "episode : 534 | step: 1797 | episode reward : 2 | mean reward/100 eps : 2.0\n",
      "episode : 535 | step: 1800 | episode reward : 2 | mean reward/100 eps : 2.0\n",
      "episode : 536 | step: 1803 | episode reward : 2 | mean reward/100 eps : 2.0\n",
      "episode : 537 | step: 1806 | episode reward : 2 | mean reward/100 eps : 2.0\n",
      "episode : 538 | step: 1809 | episode reward : 2 | mean reward/100 eps : 2.0\n",
      "episode : 539 | step: 1812 | episode reward : 2 | mean reward/100 eps : 2.0\n",
      "episode : 540 | step: 1815 | episode reward : 2 | mean reward/100 eps : 2.0\n",
      "episode : 541 | step: 1818 | episode reward : 2 | mean reward/100 eps : 2.0\n",
      "episode : 542 | step: 1821 | episode reward : 2 | mean reward/100 eps : 2.0\n",
      "episode : 543 | step: 1824 | episode reward : 2 | mean reward/100 eps : 2.0\n",
      "episode : 544 | step: 1827 | episode reward : 2 | mean reward/100 eps : 2.0\n",
      "episode : 545 | step: 1830 | episode reward : 2 | mean reward/100 eps : 2.0\n",
      "episode : 546 | step: 1833 | episode reward : 2 | mean reward/100 eps : 2.0\n",
      "episode : 547 | step: 1836 | episode reward : 2 | mean reward/100 eps : 2.0\n",
      "episode : 548 | step: 1839 | episode reward : 2 | mean reward/100 eps : 2.0\n",
      "episode : 549 | step: 1842 | episode reward : 2 | mean reward/100 eps : 2.0\n",
      "episode : 550 | step: 1845 | episode reward : 2 | mean reward/100 eps : 2.0\n",
      "episode : 551 | step: 1848 | episode reward : 2 | mean reward/100 eps : 2.0\n",
      "episode : 552 | step: 1851 | episode reward : 2 | mean reward/100 eps : 2.0\n",
      "episode : 553 | step: 1854 | episode reward : 2 | mean reward/100 eps : 2.0\n",
      "episode : 554 | step: 1857 | episode reward : 2 | mean reward/100 eps : 2.0\n",
      "episode : 555 | step: 1860 | episode reward : 2 | mean reward/100 eps : 2.0\n",
      "episode : 556 | step: 1863 | episode reward : 2 | mean reward/100 eps : 2.0\n",
      "episode : 557 | step: 1866 | episode reward : 2 | mean reward/100 eps : 2.0\n",
      "episode : 558 | step: 1869 | episode reward : 2 | mean reward/100 eps : 2.0\n",
      "episode : 559 | step: 1872 | episode reward : 2 | mean reward/100 eps : 2.0\n",
      "episode : 560 | step: 1875 | episode reward : 2 | mean reward/100 eps : 2.0\n",
      "episode : 561 | step: 1878 | episode reward : 2 | mean reward/100 eps : 2.0\n",
      "episode : 562 | step: 1881 | episode reward : 2 | mean reward/100 eps : 2.0\n",
      "episode : 563 | step: 1884 | episode reward : 2 | mean reward/100 eps : 2.0\n",
      "episode : 564 | step: 1887 | episode reward : 2 | mean reward/100 eps : 2.0\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[17]\u001b[39m\u001b[32m, line 4\u001b[39m\n\u001b[32m      2\u001b[39m total_rewards = []\n\u001b[32m      3\u001b[39m episode_idx = \u001b[32m0\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m4\u001b[39m \u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mstep_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mexp\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43menumerate\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mexp_collector\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrollout\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m      5\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# exp = exp_collector.rollout()\u001b[39;49;00m\n\u001b[32m      6\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# print(exp)\u001b[39;49;00m\n\u001b[32m      7\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mexp\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mis\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m:\u001b[49m\n\u001b[32m      8\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01mcontinue\u001b[39;49;00m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[14]\u001b[39m\u001b[32m, line 36\u001b[39m, in \u001b[36mNStepCollector.rollout\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m     32\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mrollout\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[32m     33\u001b[39m     \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[32m     34\u001b[39m         \u001b[38;5;66;03m# print(f\"state: {self.state}\")\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m36\u001b[39m         state_t = \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtensor\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mstate\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfloat32\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m.unsqueeze(\u001b[32m0\u001b[39m)\n\u001b[32m     38\u001b[39m         \u001b[38;5;28;01mwith\u001b[39;00m torch.no_grad():\n\u001b[32m     39\u001b[39m             mu, std, _ = \u001b[38;5;28mself\u001b[39m.policy(state_t)\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "exp_collector = NStepCollector(env, policy, GAMMA, LAMBDA, BATCH_SIZE, device)\n",
    "total_rewards = []\n",
    "episode_idx = 0\n",
    "for step_idx, exp in enumerate(exp_collector.rollout()):\n",
    "    # exp = exp_collector.rollout()\n",
    "    # print(exp)\n",
    "    if exp is None:\n",
    "        continue\n",
    "    \n",
    "    if exp['ep_reward'] is not None:\n",
    "        # --- NEW: Update Beta when episode finishes ---\n",
    "        current_beta = beta_scheduler.update(exp['ep_reward'])\n",
    "        episode_reward = exp['ep_reward']\n",
    "        total_rewards.append(episode_reward)\n",
    "        mean_reward = float(np.mean(total_rewards[-100:]))\n",
    "        print(f\"episode : {episode_idx} | step: {step_idx} | episode reward : {episode_reward} | mean reward/100 eps : {mean_reward}\")\n",
    "        # wandb.log({\n",
    "        #     \"episode_reward\": episode_reward, \n",
    "        #     \"mean_reward_100\": mean_reward,  \n",
    "        #     \"entropy_beta\": current_beta,  # Log this to track decay!\n",
    "        #     'episode_number': episode_idx,   \n",
    "        #     \"steps_per_episode\": exp['ep_steps']\n",
    "        # }, step=step_idx)\n",
    "        episode_idx += 1\n",
    "        \n",
    "        if mean_reward>950:\n",
    "            # save_path = os.path.join(wandb.run.dir, \"policy_best.pt\")\n",
    "            # torch.save(policy.state_dict(), save_path)\n",
    "            # wandb.log({\"best_policy_path\": save_path}, step=step_idx)\n",
    "            print(f\"Solved! Mean reward > 450 at episode {episode_idx}\")\n",
    "            break\n",
    "    \n",
    "    states_list = exp['states']\n",
    "    rawactions_list = exp['actions']\n",
    "    dones_list = exp['dones']\n",
    "    deltas_list = exp['deltas']\n",
    "    values_list = exp['values']\n",
    "    adv_list = compute_gae(deltas_list, dones_list, GAMMA, LAMBDA)\n",
    "    \n",
    "    batch_states = torch.cat(states_list, dim =0)\n",
    "    batch_actions = torch.cat(rawactions_list, dim=0)\n",
    "    batch_adv = torch.tensor(adv_list, dtype = torch.float32, device=device)\n",
    "    batch_value = torch.tensor(values_list,dtype = torch.float32, device=device)\n",
    "    \n",
    "    \n",
    "    mu, std, value = policy(batch_states)\n",
    "    value_t = value.squeeze(dim=1)\n",
    "\n",
    "    returns = batch_adv + batch_value\n",
    "    loss_critic = F.mse_loss(value_t, returns.detach())\n",
    "    \n",
    "    dist = torch.distributions.Normal(mu, std)\n",
    "    logp_u = dist.log_prob(batch_actions).sum(dim=-1)\n",
    "    a_t = torch.tanh(batch_actions)\n",
    "    logp_correction = torch.log(( 1 - a_t.pow(2))+1e-6).sum(dim=-1)\n",
    "    logp = logp_u - logp_correction\n",
    "    \n",
    "    \n",
    "    # logp_u1 = dist.log_prob(batch_actions).sum(dim=-1)\n",
    "    # a_t = torch.tanh(batch_actions)\n",
    "    # logp_correction1 = torch.log(( 1 - a_t.pow(2))+1e-6).sum(dim=-1)\n",
    "    # logp1 = logp_u1 - logp_correction1\n",
    "    \n",
    "    loss_policy = -(logp * batch_adv.detach()).mean()\n",
    "    \n",
    "    entropy = dist.entropy().sum(dim=-1).mean()\n",
    "    \n",
    "    total_loss = loss_critic + loss_policy - ENTROPY_BETA*entropy\n",
    "    \n",
    "    optimizer.zero_grad()\n",
    "    total_loss.backward()\n",
    "    torch.nn.utils.clip_grad_norm_(policy.parameters(), max_norm=0.5)\n",
    "    optimizer.step()\n",
    "    scheduler.step()\n",
    "    \n",
    "    # print(f\"batch_states: {batch_states}\")\n",
    "    # print(f'batch_rawactions:{batch_actions}')\n",
    "    # print(f\"dones: {dones_list}\")\n",
    "    # print(f\"deltas: {deltas_list}\")\n",
    "    # print(f\"batch_adv: {batch_adv}\")\n",
    "    # print(f\"batch_values: {value}, {value.shape}\")\n",
    "    # # print(f\"batch_values, dim0: {value.squeeze(dim=0)}\")\n",
    "    # # print(f\"batch_values, dim1: {value.squeeze(dim=1)}\")\n",
    "    # print(f\"value_t :{value_t}\")\n",
    "    # print(f\"dist_t: {dist}\")\n",
    "    # print(f\"logp_u: {logp_u}\")\n",
    "    \n",
    "    # print(f\"logp_correction: {logp_correction}\")\n",
    "    # print(f\"logp: {logp}\")\n",
    "    # print(f'loss_critic: {loss_critic}')\n",
    "    # print(f'loss policy : {loss_policy}')\n",
    "    # print(f'entropy: {entropy}')\n",
    "    # print(f\"entropy mean:{entropy.mean()}\")\n",
    "    # print(f\"entropy sum:{entropy.sum(dim=-1)}\")\n",
    "    # print(f\"entropy sum mean (used):{entropy.sum(dim=-1).mean()}\")\n",
    "    # print(f\"log_p: {logp_correction.sum(dim=-1)}\")\n",
    "    # print(f\"total_loss: {total_loss}\")\n",
    "    \n",
    "    \n",
    "    # returns = \n",
    "    \n",
    "    \n",
    "    # loss_critic = \n",
    "    \n",
    "    # break\n",
    "    if step_idx > 200000:\n",
    "        break\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd39549b",
   "metadata": {},
   "outputs": [],
   "source": [
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a18a070",
   "metadata": {},
   "outputs": [],
   "source": [
    "# state: [-0.00128835  0.00534891 -0.31165499  0.69753022]\n",
    "# mu tensor([[-0.1096]], device='mps:0')\n",
    "# std tensor([[1.]], device='mps:0')\n",
    "# {'states': [tensor([[ 0.0089, -0.0047,  0.0056,  0.0059]], device='mps:0'), tensor([[-0.0085,  0.0360, -0.8698,  2.0097]], device='mps:0'), tensor([[-0.0615,  0.1570, -1.7760,  4.0253]], device='mps:0'), tensor([[ 0.0050, -0.0086, -0.0011, -0.0086]], device='mps:0'), tensor([[-0.0013,  0.0053, -0.3117,  0.6975]], device='mps:0')], 'actions': [tensor([[-1.3766]], device='mps:0'), tensor([[-1.6407]], device='mps:0'), tensor([[-1.1694]], device='mps:0'), tensor([[-0.3236]], device='mps:0'), tensor([[0.6305]], device='mps:0')], 'dones': [False, False, True, False, False], 'deltas': [1.0049174800515175, 1.0309211984276772, 0.08663583546876907, 1.0279532670974731, 0.93100406229496]}\n",
    "# batch_states: tensor([[ 8.8630e-03, -4.7227e-03,  5.5838e-03,  5.9092e-03],\n",
    "#         [-8.4699e-03,  3.6001e-02, -8.6976e-01,  2.0097e+00],\n",
    "#         [-6.1460e-02,  1.5701e-01, -1.7760e+00,  4.0253e+00],\n",
    "#         [ 4.9838e-03, -8.5719e-03, -1.1198e-03, -8.6451e-03],\n",
    "#         [-1.2883e-03,  5.3489e-03, -3.1165e-01,  6.9753e-01]], device='mps:0')\n",
    "# batch_rawactions:tensor([[-1.3766],\n",
    "#         [-1.6407],\n",
    "#         [-1.1694],\n",
    "#         [-0.3236],\n",
    "#         [ 0.6305]], device='mps:0')\n",
    "# dones: [False, False, True, False, False]\n",
    "# deltas: [1.0049174800515175, 1.0309211984276772, 0.08663583546876907, 1.0279532670974731, 0.93100406229496]\n",
    "# batch_adv: tensor([1.0049, 1.0309, 0.9655, 1.0280, 0.9310], device='mps:0')\n",
    "# batch_values: tensor([[-0.1225],\n",
    "#         [-0.1176],\n",
    "#         [-0.0866],\n",
    "#         [-0.1232],\n",
    "#         [-0.0953]], device='mps:0', grad_fn=<LinearBackward0>), torch.Size([5, 1])\n",
    "# value_t :tensor([-0.1225, -0.1176, -0.0866, -0.1232, -0.0953], device='mps:0',\n",
    "#        grad_fn=<SqueezeBackward1>)\n",
    "# dist_t: Normal(loc: torch.Size([5, 1]), scale: torch.Size([5, 1]))\n",
    "# logp_u: tensor([-1.7717, -1.9905, -1.3410, -0.9512, -1.1928], device='mps:0',\n",
    "#        grad_fn=<SumBackward1>)\n",
    "# logp_correction: tensor([-1.4904, -1.9689, -1.1366, -0.1030, -0.3737], device='mps:0')\n",
    "# logp: tensor([-0.2813, -0.0216, -0.2044, -0.8482, -0.8192], device='mps:0',\n",
    "#        grad_fn=<SubBackward0>)\n",
    "# loss_critic: 0.012116288766264915\n",
    "# loss policy : 0.42738208174705505\n",
    "# entropy: tensor([[1.4189],\n",
    "#         [1.4189],\n",
    "#         [1.4189],\n",
    "#         [1.4189],\n",
    "#         [1.4189]], device='mps:0', grad_fn=<AddBackward0>)\n",
    "# entropy mean:1.4189385175704956\n",
    "# entropy sum:tensor([1.4189, 1.4189, 1.4189, 1.4189, 1.4189], device='mps:0',\n",
    "#        grad_fn=<SumBackward1>)\n",
    "# entropy sum mean (used):1.418938398361206"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e87af48f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "drl_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
