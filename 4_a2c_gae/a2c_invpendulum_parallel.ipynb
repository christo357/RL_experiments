{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "9d2a7499",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device : mps\n"
     ]
    }
   ],
   "source": [
    "import gymnasium as gym \n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import typing as tt\n",
    "import torch  \n",
    "import torch.nn as nn \n",
    "import torch.nn.functional as F\n",
    "import wandb\n",
    "import os\n",
    "\n",
    "from gymnasium.wrappers.vector import RecordEpisodeStatistics\n",
    "\n",
    "HIDDEN_LAYER1  = 256\n",
    "# ALPHA = 0.95\n",
    "GAMMA = 0.95 # DISCOUNT FACTOR\n",
    "LAMBDA = 0.95 # FOR GAE\n",
    "LR = 3e-4\n",
    "# N_STEPS = 20\n",
    "ENV_ID = 'InvertedPendulum-v5'\n",
    "N_ENVS = 3\n",
    "N_STEPS = 5\n",
    "BATCH_SIZE = N_ENVS * N_STEPS\n",
    "\n",
    "ENTROPY_BETA = 0.01\n",
    "ENTROPY_BETA_MIN = 1e-5\n",
    "entropy_smoothing_factor = 0.05\n",
    "total_updates = 500000 // BATCH_SIZE\n",
    "\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    device = 'cuda'\n",
    "elif torch.backends.mps.is_available():\n",
    "    device = 'mps'\n",
    "else:\n",
    "    device = 'cpu' \n",
    "print(f'Using device : {device}')\n",
    "\n",
    "\n",
    "# env = gym.make(ENV_ID)\n",
    "envs = gym.make_vec(ENV_ID, num_envs=N_ENVS, vectorization_mode='async' )\n",
    "envs = RecordEpisodeStatistics(envs) #handles reward logging\n",
    "eval_env = gym.make(ENV_ID, render_mode='rgb_array')\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "def smooth(old: tt.Optional[float], val: float, alpha: float = 0.95,) -> float:\n",
    "    if old is None:\n",
    "        return val\n",
    "    return old * alpha + (1-alpha)*val    \n",
    "\n",
    "def record_video(env, policy, device, low, high, max_steps=500, ):\n",
    "    \"\"\"Record a single episode and return frames + reward\"\"\"\n",
    "    frames = []\n",
    "    state, _ = env.reset()\n",
    "    done = False\n",
    "    total_reward = 0\n",
    "    steps = 0\n",
    "    frame = env.render()\n",
    "    if frame is not None:\n",
    "        frames.append(np.array(frame, copy=True))\n",
    "    while not done and steps < max_steps:\n",
    "        state_tensor = torch.tensor(state, dtype=torch.float32, device=device).unsqueeze(0)\n",
    "        with torch.no_grad():\n",
    "            mu, std, _ = policy(state_tensor)\n",
    "        dist = torch.distributions.Normal(mu, std)\n",
    "        action = torch.clamp(dist.sample(), low, high)\n",
    "\n",
    "        state, reward, terminated, truncated, _ = env.step(action.squeeze(0).cpu().numpy())\n",
    "        total_reward += reward\n",
    "        done = terminated or truncated\n",
    "        steps += 1\n",
    "\n",
    "        frame = env.render()\n",
    "        if frame is not None:\n",
    "            frames.append(np.array(frame, copy=True))\n",
    "        \n",
    "    return frames, total_reward, steps\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class PolicyNet(nn.Module):\n",
    "    def __init__(self, input_size, fc, action_dim, log_std_min, log_std_max):\n",
    "        super().__init__()\n",
    "        self.input_size = input_size\n",
    "        self.fc = fc\n",
    "        self.action_dim = action_dim\n",
    "        self.log_std_min = log_std_min\n",
    "        self.log_std_max = log_std_max\n",
    "        \n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(self.input_size, self.fc), \n",
    "            nn.ReLU(), \n",
    "            nn.Linear(self.fc, self.fc), \n",
    "            nn.ReLU()\n",
    "        )\n",
    "        \n",
    "        self.mu = nn.Linear(self.fc, self.action_dim)\n",
    "        \n",
    "        self.log_std = nn.Parameter(torch.zeros(self.action_dim))\n",
    "        \n",
    "        self.critic_head = nn.Linear(self.fc, 1)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.net(x)\n",
    "        \n",
    "        mu = self.mu(x)\n",
    "        std = torch.exp(torch.clamp(self.log_std, self.log_std_min, self.log_std_max))\n",
    "        std = std.expand_as(mu)\n",
    "        \n",
    "        val = self.critic_head(x)\n",
    "        return mu, std, val\n",
    "    \n",
    "class LinearBetaScheduler:\n",
    "    def __init__(self, beta_start, beta_end, total_steps):\n",
    "        self.start = beta_start\n",
    "        self.end = beta_end\n",
    "        self.total_steps = total_steps\n",
    "\n",
    "    def update(self, current_step):\n",
    "        # Linearly decay beta based on step count\n",
    "        frac = min(1.0, current_step / self.total_steps)\n",
    "        return self.start + frac * (self.end - self.start)\n",
    "    \n",
    "class BetaScheduler:\n",
    "    def __init__(self, target_reward, beta_start, beta_min=1e-4, smoothing_factor=0.01):\n",
    "        self.target = target_reward\n",
    "        self.start = beta_start\n",
    "        self.min = beta_min\n",
    "        self.alpha = smoothing_factor\n",
    "        self.ema_reward = None  # Exponential Moving Average of Reward\n",
    "        self.current_beta = beta_start\n",
    "\n",
    "    def update(self, reward):\n",
    "        # 1. Update EMA of Reward\n",
    "        if self.ema_reward is None:\n",
    "            self.ema_reward = reward\n",
    "        else:\n",
    "            self.ema_reward = (self.ema_reward * (1 - self.alpha)) + (reward * self.alpha)\n",
    "        \n",
    "        # 2. Calculate Progress (0.0 to 1.0) based on EMA\n",
    "        # If ema_reward is negative, treat progress as 0\n",
    "        progress = max(0.0, min(1.0, self.ema_reward / self.target))\n",
    "        \n",
    "        # 3. Decay Beta linearly with progress\n",
    "        self.current_beta = self.start * (1.0 - progress)\n",
    "        \n",
    "        # 4. Clamp to minimum\n",
    "        self.current_beta = max(self.current_beta, self.min)\n",
    "        \n",
    "        return self.current_beta\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "c4ade1cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_gae(rewards, values, next_values, dones, gamma, lam):\n",
    "    \n",
    "    rewards_t = torch.stack(rewards, dim=0)\n",
    "    dones_t = torch.stack(dones, dim=0)\n",
    "    print(f\"REWARDS:{rewards_t}\")\n",
    "    print(f'DONES: {dones_t}')\n",
    "    \n",
    "    mask = 1.0 - dones_t\n",
    "    print('MASK', mask)\n",
    "\n",
    "    delta_t = rewards + gamma*next_values - values\n",
    "    \n",
    "    T = delta_t.shape[0]\n",
    "    adv = torch.zeros_like(delta_t)\n",
    "    gae = 0.0\n",
    "\n",
    "    for t in reversed(range(T)):\n",
    "        gae = delta_t[t] + gamma * lam * mask[t] * gae\n",
    "        adv[t] = gae\n",
    "\n",
    "    return adv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6d3a32e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy import dtype\n",
    "\n",
    "\n",
    "class VectorCollector:\n",
    "    def __init__(self, envs, policy, gamma, lam, n_steps,action_low, action_high,  device):\n",
    "        # super().__init__(self,)\n",
    "        self.env = envs\n",
    "        self.policy = policy\n",
    "        self.gamma = gamma\n",
    "        self.lam = lam\n",
    "        self.n_steps = n_steps\n",
    "        self.device = device\n",
    "        \n",
    "        self.ep_reward = 0\n",
    "        \n",
    "        self.state, _ = envs.reset()\n",
    "        print(self.state)\n",
    "        self.action_bias = (action_high + action_low) / 2\n",
    "        self.action_scale = (action_high - action_low) / 2\n",
    "                \n",
    "        self.states = []\n",
    "        self.raw_actions = []\n",
    "        self.rewards = []\n",
    "        self.dones = []\n",
    "        self.next_states = []\n",
    "        self.deltas = []\n",
    "        self.values = []         \n",
    "          \n",
    "    def rollout(self):\n",
    "        \n",
    "        \n",
    "        batch_states = []\n",
    "        batch_actions = []\n",
    "        batch_rewards = []\n",
    "        batch_dones = []\n",
    "        batch_values = []\n",
    "        batch_deltas = []\n",
    "        \n",
    "        episode_rewards = []\n",
    "        batch_value_next = []\n",
    "        \n",
    "        for _ in range(self.n_steps):\n",
    "            # print(f\"state: {self.state}\")\n",
    "            \n",
    "            state_t = torch.tensor(self.state, dtype=torch.float32, device=device)#.unsqueeze(0)\n",
    "            print(state_t)\n",
    "            with torch.no_grad():\n",
    "                mu, std, val = self.policy(state_t)\n",
    "            # print(f\"mu: {mu}\")\n",
    "            # print(f'std: {std}')\n",
    "            # print(f\"val: {val}\")\n",
    "            # return\n",
    "            # # print('mu', mu)\n",
    "            # # print('std', std)\n",
    "            dist = torch.distributions.Normal(mu,std)\n",
    "            u = dist.sample()\n",
    "            a = torch.tanh(u)\n",
    "            \n",
    "            action = a*self.action_scale + self.action_bias\n",
    "            print(f\"action:{action}\")\n",
    "            action_env = action.detach().cpu().numpy()\n",
    "            # action_env = action.squeeze(0).detach().cpu().numpy()\n",
    "            # print(f\"action env:{action_env}\")\n",
    "            # return\n",
    "            next_state, rew, term, trunc, info = self.env.step(action_env)\n",
    "            # self.next_state_t = torch.Tensor(next_state, dtype=torch.float32, device=device).unsqueeze(0)\n",
    "            done = term | trunc\n",
    "            done_t = torch.tensor(done, dtype=torch.float32, device=self.device)\n",
    "            rew_t = torch.tensor(rew, dtype=torch.float32, device=self.device)\n",
    "            # print(f'next_state: {next_state}')\n",
    "            # print(f\"done: {done}\")\n",
    "            # print(f\"rew: {rew}\")\n",
    "            print(f\"info: {info}\")\n",
    "            \n",
    "            \n",
    "            \n",
    "            batch_states.append(state_t)\n",
    "            batch_actions.append(u)\n",
    "            batch_rewards.append(torch.tensor(rew, dtype=torch.float32, device=self.device)) # rew is converted to tensor to seperate each n_steps\n",
    "            batch_dones.append(torch.tensor(rew, dtype=torch.float32, device=self.device))\n",
    "            batch_values.append(val.squeeze(dim=-1))\n",
    "            print(f\"batch_actions: {batch_actions}\")\n",
    "            print(f\"batch_states: {batch_states}\")\n",
    "            print(f\"batch_rewards: {batch_rewards}\")\n",
    "            print(f\"batch_dones: {batch_dones}\")\n",
    "            print(f\"batch_values : {batch_values}\")\n",
    "            # yield None\n",
    "            # continue\n",
    "            if '_episode' in info:\n",
    "                for idx, has_ep in enumerate(info['_episode']):\n",
    "                    if has_ep:\n",
    "                        if 'episode' in info:\n",
    "                            print(f'idx: {idx}')\n",
    "                            print(f\"episode r: {info['episode']['r']}\")\n",
    "                            episode_rewards.append(info['episode']['r'][idx])\n",
    "            else: \n",
    "                episode_rewards = []\n",
    "            print(f'{episode_rewards}')\n",
    "            \n",
    "            self.state = next_state\n",
    "\n",
    "            # yield None\n",
    "            # continue\n",
    "            \n",
    "            \n",
    "            \n",
    "        # bootstrapping\n",
    "        with torch.no_grad():\n",
    "            next_state_t = torch.tensor(next_state, dtype=torch.float32, device=device)\n",
    "            _, _, nxt_val = self.policy(next_state_t)\n",
    "            print(f'next_val before squeeze dim=-1:{nxt_val}')\n",
    "            nxt_val = nxt_val.squeeze(dim=-1)\n",
    "            print(f'next_val after squeeze dim=-1:{nxt_val}')\n",
    "            \n",
    "        T_rewards = torch.stack(batch_rewards, dim=0)\n",
    "        T_dones = torch.stack(batch_dones, dim=0)\n",
    "        T_values = torch.stack(batch_values, dim=0)\n",
    "        \n",
    "        print(f\"values_t : {T_values}, {T_values.shape}\")\n",
    "        print(f'next values after unsqueeze: {nxt_val}, {nxt_val.unsqueeze(dim=0).shape}')\n",
    "        T_values_next = torch.cat((T_values[1:], nxt_val.unsqueeze(dim=0)), dim=0)\n",
    "        print(f\"values_t : {T_values_next}, {T_values_next.shape}\")\n",
    "    \n",
    "        batch_adv = compute_gae(rewards=batch_rewards, \n",
    "                                values=batch_values,  \n",
    "                                next_values=nxt_val, \n",
    "                                dones=batch_dones, \n",
    "                                gamma=self.gamma, \n",
    "                                lam=self.lam )\n",
    "        \n",
    "        batch_returns = batch_adv + T_values\n",
    "        print(f'batch adv: {batch_adv}')\n",
    "        \n",
    "        yield {\n",
    "                'states':batch_states, \n",
    "                'actions':batch_actions, \n",
    "                'done':batch_dones, \n",
    "                'adv':batch_adv,\n",
    "                'ep_rewards': finished_episode_rewards,\n",
    "                'values':batch_values, \n",
    "                'returns':batch_returns\n",
    "        }\n",
    "            \n",
    "            self.states.clear()\n",
    "            self.rewards.clear()\n",
    "            self.dones.clear()\n",
    "            self.next_states.clear()\n",
    "            self.deltas.clear()\n",
    "            self.raw_actions.clear()\n",
    "            self.values.clear()\n",
    "            finished_episode_rewards = []\n",
    "        \n",
    "        else: \n",
    "            yield None\n",
    "            \n",
    "        self.state = next_state\n",
    "        if term or trunc:\n",
    "            # print(\"reset\")\n",
    "            self.state, _ = self.env.reset()\n",
    "            self.ep_reward = 0\n",
    "        \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "c0f05f0b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-0.00051659  0.00568842 -0.00109743 -0.00413287]\n",
      " [ 0.00467107  0.00543563 -0.00926312  0.00737832]\n",
      " [ 0.00911001 -0.00822641 -0.0076141  -0.00345078]]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "policy = PolicyNet(\n",
    "    input_size=envs.single_observation_space.shape[0], \n",
    "    fc = HIDDEN_LAYER1, \n",
    "    action_dim=envs.single_action_space.shape[0], \n",
    "    log_std_min=-20, \n",
    "    log_std_max=1,\n",
    ").to(device)\n",
    "\n",
    "optimizer = torch.optim.Adam(policy.parameters(), lr = LR)\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
    "    optimizer, \n",
    "    mode='max', \n",
    "    factor=0.5, \n",
    "    patience=500,  # If reward doesn't go up for 500 steps, lower LR\n",
    ")\n",
    "\n",
    "current_beta = ENTROPY_BETA\n",
    "beta_scheduler = LinearBetaScheduler(\n",
    "    beta_start=ENTROPY_BETA, \n",
    "    beta_end=ENTROPY_BETA_MIN, \n",
    "    total_steps=total_updates   # Decay fully in the first 33% of training\n",
    ")\n",
    "# beta_scheduler = BetaScheduler(\n",
    "#     target_reward=950, \n",
    "#     beta_start=ENTROPY_BETA, \n",
    "#     beta_min=ENTROPY_BETA_MIN, \n",
    "#     smoothing_factor=entropy_smoothing_factor\n",
    "# )\n",
    "\n",
    "action_low = torch.tensor(envs.single_action_space.low, dtype=torch.float32, device=device)\n",
    "action_high = torch.tensor(envs.single_action_space.high, dtype=torch.float32, device=device)\n",
    "exp_collector = VectorCollector(envs, policy, GAMMA, LAMBDA, N_STEPS,action_low, action_high, device)\n",
    "total_rewards = []\n",
    "episode_idx = 0\n",
    "mu_old = 0\n",
    "adv_smoothed = None\n",
    "l_entropy = None\n",
    "l_policy = None\n",
    "l_value = None\n",
    "l_total = None\n",
    "mean_reward = 0.0\n",
    "solved = False\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "cdae5a5c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.0005,  0.0057, -0.0011, -0.0041],\n",
      "        [ 0.0047,  0.0054, -0.0093,  0.0074],\n",
      "        [ 0.0091, -0.0082, -0.0076, -0.0035]], device='mps:0')\n",
      "action:tensor([[ 2.5149],\n",
      "        [-0.0852],\n",
      "        [-0.0649]], device='mps:0')\n",
      "info: {'reward_survive': array([1, 1, 1]), '_reward_survive': array([ True,  True,  True])}\n",
      "batch_actions: [tensor([[ 1.2155],\n",
      "        [-0.0284],\n",
      "        [-0.0216]], device='mps:0')]\n",
      "batch_states: [tensor([[-0.0005,  0.0057, -0.0011, -0.0041],\n",
      "        [ 0.0047,  0.0054, -0.0093,  0.0074],\n",
      "        [ 0.0091, -0.0082, -0.0076, -0.0035]], device='mps:0')]\n",
      "batch_rewards: [tensor([1., 1., 1.], device='mps:0')]\n",
      "batch_dones: [tensor([1., 1., 1.], device='mps:0')]\n",
      "batch_values : [tensor([0.0349, 0.0334, 0.0349], device='mps:0')]\n",
      "[]\n",
      "tensor([[ 0.0162, -0.0329,  0.8322, -1.9078],\n",
      "        [ 0.0037,  0.0072, -0.0382,  0.0799],\n",
      "        [ 0.0084, -0.0075, -0.0284,  0.0385]], device='mps:0')\n",
      "action:tensor([[ 0.1841],\n",
      "        [-0.4402],\n",
      "        [ 2.0464]], device='mps:0')\n",
      "info: {'reward_survive': array([1, 1, 1]), '_reward_survive': array([ True,  True,  True])}\n",
      "batch_actions: [tensor([[ 1.2155],\n",
      "        [-0.0284],\n",
      "        [-0.0216]], device='mps:0'), tensor([[ 0.0615],\n",
      "        [-0.1478],\n",
      "        [ 0.8331]], device='mps:0')]\n",
      "batch_states: [tensor([[-0.0005,  0.0057, -0.0011, -0.0041],\n",
      "        [ 0.0047,  0.0054, -0.0093,  0.0074],\n",
      "        [ 0.0091, -0.0082, -0.0076, -0.0035]], device='mps:0'), tensor([[ 0.0162, -0.0329,  0.8322, -1.9078],\n",
      "        [ 0.0037,  0.0072, -0.0382,  0.0799],\n",
      "        [ 0.0084, -0.0075, -0.0284,  0.0385]], device='mps:0')]\n",
      "batch_rewards: [tensor([1., 1., 1.], device='mps:0'), tensor([1., 1., 1.], device='mps:0')]\n",
      "batch_dones: [tensor([1., 1., 1.], device='mps:0'), tensor([1., 1., 1.], device='mps:0')]\n",
      "batch_values : [tensor([0.0349, 0.0334, 0.0349], device='mps:0'), tensor([0.0988, 0.0233, 0.0289], device='mps:0')]\n",
      "[]\n",
      "tensor([[ 5.0421e-02, -1.1020e-01,  8.8244e-01, -1.9692e+00],\n",
      "        [-7.4628e-04,  1.7253e-02, -1.8469e-01,  4.2022e-01],\n",
      "        [ 2.0884e-02, -3.7606e-02,  6.5124e-01, -1.5265e+00]], device='mps:0')\n",
      "action:tensor([[1.8736],\n",
      "        [1.5709],\n",
      "        [2.9286]], device='mps:0')\n",
      "info: {'reward_survive': array([0, 1, 1]), '_reward_survive': array([ True,  True,  True]), 'episode': {'r': array([2., 0., 0.]), 'l': array([3, 0, 0]), 't': array([1.113438, 0.      , 0.      ])}, '_episode': array([ True, False, False])}\n",
      "batch_actions: [tensor([[ 1.2155],\n",
      "        [-0.0284],\n",
      "        [-0.0216]], device='mps:0'), tensor([[ 0.0615],\n",
      "        [-0.1478],\n",
      "        [ 0.8331]], device='mps:0'), tensor([[0.7324],\n",
      "        [0.5813],\n",
      "        [2.2097]], device='mps:0')]\n",
      "batch_states: [tensor([[-0.0005,  0.0057, -0.0011, -0.0041],\n",
      "        [ 0.0047,  0.0054, -0.0093,  0.0074],\n",
      "        [ 0.0091, -0.0082, -0.0076, -0.0035]], device='mps:0'), tensor([[ 0.0162, -0.0329,  0.8322, -1.9078],\n",
      "        [ 0.0037,  0.0072, -0.0382,  0.0799],\n",
      "        [ 0.0084, -0.0075, -0.0284,  0.0385]], device='mps:0'), tensor([[ 5.0421e-02, -1.1020e-01,  8.8244e-01, -1.9692e+00],\n",
      "        [-7.4628e-04,  1.7253e-02, -1.8469e-01,  4.2022e-01],\n",
      "        [ 2.0884e-02, -3.7606e-02,  6.5124e-01, -1.5265e+00]], device='mps:0')]\n",
      "batch_rewards: [tensor([1., 1., 1.], device='mps:0'), tensor([1., 1., 1.], device='mps:0'), tensor([0., 1., 1.], device='mps:0')]\n",
      "batch_dones: [tensor([1., 1., 1.], device='mps:0'), tensor([1., 1., 1.], device='mps:0'), tensor([0., 1., 1.], device='mps:0')]\n",
      "batch_values : [tensor([0.0349, 0.0334, 0.0349], device='mps:0'), tensor([0.0988, 0.0233, 0.0289], device='mps:0'), tensor([ 0.0984, -0.0006,  0.0943], device='mps:0')]\n",
      "idx: 0\n",
      "episode r: [2. 0. 0.]\n",
      "[np.float64(2.0)]\n",
      "tensor([[ 9.7967e-02, -2.1700e-01,  1.4921e+00, -3.3619e+00],\n",
      "        [ 2.3358e-03,  9.8141e-03,  3.3709e-01, -7.7690e-01],\n",
      "        [ 6.6200e-02, -1.4221e-01,  1.6110e+00, -3.6856e+00]], device='mps:0')\n",
      "action:tensor([[0.3404],\n",
      "        [2.5975],\n",
      "        [2.7614]], device='mps:0')\n",
      "info: {'reward_survive': array([0, 1, 0]), '_reward_survive': array([False,  True,  True]), 'episode': {'r': array([0., 0., 3.]), 'l': array([0, 0, 4]), 't': array([0.      , 0.      , 1.179842])}, '_episode': array([False, False,  True])}\n",
      "batch_actions: [tensor([[ 1.2155],\n",
      "        [-0.0284],\n",
      "        [-0.0216]], device='mps:0'), tensor([[ 0.0615],\n",
      "        [-0.1478],\n",
      "        [ 0.8331]], device='mps:0'), tensor([[0.7324],\n",
      "        [0.5813],\n",
      "        [2.2097]], device='mps:0'), tensor([[0.1139],\n",
      "        [1.3162],\n",
      "        [1.5921]], device='mps:0')]\n",
      "batch_states: [tensor([[-0.0005,  0.0057, -0.0011, -0.0041],\n",
      "        [ 0.0047,  0.0054, -0.0093,  0.0074],\n",
      "        [ 0.0091, -0.0082, -0.0076, -0.0035]], device='mps:0'), tensor([[ 0.0162, -0.0329,  0.8322, -1.9078],\n",
      "        [ 0.0037,  0.0072, -0.0382,  0.0799],\n",
      "        [ 0.0084, -0.0075, -0.0284,  0.0385]], device='mps:0'), tensor([[ 5.0421e-02, -1.1020e-01,  8.8244e-01, -1.9692e+00],\n",
      "        [-7.4628e-04,  1.7253e-02, -1.8469e-01,  4.2022e-01],\n",
      "        [ 2.0884e-02, -3.7606e-02,  6.5124e-01, -1.5265e+00]], device='mps:0'), tensor([[ 9.7967e-02, -2.1700e-01,  1.4921e+00, -3.3619e+00],\n",
      "        [ 2.3358e-03,  9.8141e-03,  3.3709e-01, -7.7690e-01],\n",
      "        [ 6.6200e-02, -1.4221e-01,  1.6110e+00, -3.6856e+00]], device='mps:0')]\n",
      "batch_rewards: [tensor([1., 1., 1.], device='mps:0'), tensor([1., 1., 1.], device='mps:0'), tensor([0., 1., 1.], device='mps:0'), tensor([0., 1., 0.], device='mps:0')]\n",
      "batch_dones: [tensor([1., 1., 1.], device='mps:0'), tensor([1., 1., 1.], device='mps:0'), tensor([0., 1., 1.], device='mps:0'), tensor([0., 1., 0.], device='mps:0')]\n",
      "batch_values : [tensor([0.0349, 0.0334, 0.0349], device='mps:0'), tensor([0.0988, 0.0233, 0.0289], device='mps:0'), tensor([ 0.0984, -0.0006,  0.0943], device='mps:0'), tensor([0.1126, 0.0856, 0.1171], device='mps:0')]\n",
      "idx: 2\n",
      "episode r: [0. 0. 3.]\n",
      "[np.float64(2.0), np.float64(3.0)]\n",
      "tensor([[ 7.2625e-03, -6.8536e-03, -1.7925e-03,  5.7917e-03],\n",
      "        [ 3.2949e-02, -5.9850e-02,  1.1915e+00, -2.6918e+00],\n",
      "        [ 1.4818e-01, -3.2823e-01,  2.4784e+00, -5.5871e+00]], device='mps:0')\n",
      "action:tensor([[-0.7898],\n",
      "        [ 1.2660],\n",
      "        [ 1.3022]], device='mps:0')\n",
      "info: {'reward_survive': array([1, 1, 0]), '_reward_survive': array([ True,  True, False])}\n",
      "batch_actions: [tensor([[ 1.2155],\n",
      "        [-0.0284],\n",
      "        [-0.0216]], device='mps:0'), tensor([[ 0.0615],\n",
      "        [-0.1478],\n",
      "        [ 0.8331]], device='mps:0'), tensor([[0.7324],\n",
      "        [0.5813],\n",
      "        [2.2097]], device='mps:0'), tensor([[0.1139],\n",
      "        [1.3162],\n",
      "        [1.5921]], device='mps:0'), tensor([[-0.2696],\n",
      "        [ 0.4501],\n",
      "        [ 0.4649]], device='mps:0')]\n",
      "batch_states: [tensor([[-0.0005,  0.0057, -0.0011, -0.0041],\n",
      "        [ 0.0047,  0.0054, -0.0093,  0.0074],\n",
      "        [ 0.0091, -0.0082, -0.0076, -0.0035]], device='mps:0'), tensor([[ 0.0162, -0.0329,  0.8322, -1.9078],\n",
      "        [ 0.0037,  0.0072, -0.0382,  0.0799],\n",
      "        [ 0.0084, -0.0075, -0.0284,  0.0385]], device='mps:0'), tensor([[ 5.0421e-02, -1.1020e-01,  8.8244e-01, -1.9692e+00],\n",
      "        [-7.4628e-04,  1.7253e-02, -1.8469e-01,  4.2022e-01],\n",
      "        [ 2.0884e-02, -3.7606e-02,  6.5124e-01, -1.5265e+00]], device='mps:0'), tensor([[ 9.7967e-02, -2.1700e-01,  1.4921e+00, -3.3619e+00],\n",
      "        [ 2.3358e-03,  9.8141e-03,  3.3709e-01, -7.7690e-01],\n",
      "        [ 6.6200e-02, -1.4221e-01,  1.6110e+00, -3.6856e+00]], device='mps:0'), tensor([[ 7.2625e-03, -6.8536e-03, -1.7925e-03,  5.7917e-03],\n",
      "        [ 3.2949e-02, -5.9850e-02,  1.1915e+00, -2.6918e+00],\n",
      "        [ 1.4818e-01, -3.2823e-01,  2.4784e+00, -5.5871e+00]], device='mps:0')]\n",
      "batch_rewards: [tensor([1., 1., 1.], device='mps:0'), tensor([1., 1., 1.], device='mps:0'), tensor([0., 1., 1.], device='mps:0'), tensor([0., 1., 0.], device='mps:0'), tensor([1., 1., 0.], device='mps:0')]\n",
      "batch_dones: [tensor([1., 1., 1.], device='mps:0'), tensor([1., 1., 1.], device='mps:0'), tensor([0., 1., 1.], device='mps:0'), tensor([0., 1., 0.], device='mps:0'), tensor([1., 1., 0.], device='mps:0')]\n",
      "batch_values : [tensor([0.0349, 0.0334, 0.0349], device='mps:0'), tensor([0.0988, 0.0233, 0.0289], device='mps:0'), tensor([ 0.0984, -0.0006,  0.0943], device='mps:0'), tensor([0.1126, 0.0856, 0.1171], device='mps:0'), tensor([0.0338, 0.1075, 0.1302], device='mps:0')]\n",
      "[]\n",
      "next_val before squeeze dim=-1:tensor([[-0.0108],\n",
      "        [ 0.1136],\n",
      "        [ 0.0350]], device='mps:0')\n",
      "next_val after squeeze dim=-1:tensor([-0.0108,  0.1136,  0.0350], device='mps:0')\n",
      "values_t : tensor([[ 0.0349,  0.0334,  0.0349],\n",
      "        [ 0.0988,  0.0233,  0.0289],\n",
      "        [ 0.0984, -0.0006,  0.0943],\n",
      "        [ 0.1126,  0.0856,  0.1171],\n",
      "        [ 0.0338,  0.1075,  0.1302]], device='mps:0'), torch.Size([5, 3])\n",
      "next values after unsqueeze: tensor([-0.0108,  0.1136,  0.0350], device='mps:0'), torch.Size([1, 3])\n",
      "values_t : tensor([[ 0.0988,  0.0233,  0.0289],\n",
      "        [ 0.0984, -0.0006,  0.0943],\n",
      "        [ 0.1126,  0.0856,  0.1171],\n",
      "        [ 0.0338,  0.1075,  0.1302],\n",
      "        [-0.0108,  0.1136,  0.0350]], device='mps:0'), torch.Size([5, 3])\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "compute_gae() got an unexpected keyword argument 'values_t'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mTypeError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[79]\u001b[39m\u001b[32m, line 15\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# print(\"Recording initial video (before training)...\")\u001b[39;00m\n\u001b[32m      2\u001b[39m \u001b[38;5;66;03m# initial_frames, initial_reward, initial_steps = record_video(eval_env, policy, device, low = action_low, high = action_high)\u001b[39;00m\n\u001b[32m      3\u001b[39m \u001b[38;5;66;03m# wandb.log({\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m     11\u001b[39m \u001b[38;5;66;03m# }, step=0)\u001b[39;00m\n\u001b[32m     12\u001b[39m \u001b[38;5;66;03m# print(f\"Initial reward: {initial_reward}, steps: {initial_steps}\")\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m15\u001b[39m \u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mstep_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mexp\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43menumerate\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mexp_collector\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrollout\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m     16\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# exp = exp_collector.rollout()\u001b[39;49;00m\n\u001b[32m     17\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# print(exp)\u001b[39;49;00m\n\u001b[32m     18\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mprint\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     19\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# if exp is None:\u001b[39;49;00m\n\u001b[32m     20\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m#     continue\u001b[39;49;00m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[77]\u001b[39m\u001b[32m, line 127\u001b[39m, in \u001b[36mVectorCollector.rollout\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    121\u001b[39m T_values_next = torch.cat((T_values[\u001b[32m1\u001b[39m:], nxt_val.unsqueeze(dim=\u001b[32m0\u001b[39m)), dim=\u001b[32m0\u001b[39m)\n\u001b[32m    122\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mvalues_t : \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mT_values_next\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m, \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mT_values_next.shape\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m127\u001b[39m batch_adv = \u001b[43mcompute_gae\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrewards\u001b[49m\u001b[43m=\u001b[49m\u001b[43mbatch_rewards\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[32m    128\u001b[39m \u001b[43m                        \u001b[49m\u001b[43mvalues_t\u001b[49m\u001b[43m=\u001b[49m\u001b[43mbatch_values\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\n\u001b[32m    129\u001b[39m \u001b[43m                        \u001b[49m\u001b[43mnext_values\u001b[49m\u001b[43m=\u001b[49m\u001b[43mnxt_val\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[32m    130\u001b[39m \u001b[43m                        \u001b[49m\u001b[43mdones\u001b[49m\u001b[43m=\u001b[49m\u001b[43mbatch_dones\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[32m    131\u001b[39m \u001b[43m                        \u001b[49m\u001b[43mgamma\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mgamma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[32m    132\u001b[39m \u001b[43m                        \u001b[49m\u001b[43mlam\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mlam\u001b[49m\u001b[43m \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    135\u001b[39m batch_deltas.append(delta)\n\u001b[32m    136\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mbatch_deltas : \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mbatch_deltas\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n",
      "\u001b[31mTypeError\u001b[39m: compute_gae() got an unexpected keyword argument 'values_t'"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# print(\"Recording initial video (before training)...\")\n",
    "# initial_frames, initial_reward, initial_steps = record_video(eval_env, policy, device, low = action_low, high = action_high)\n",
    "# wandb.log({\n",
    "#     \"video\": wandb.Video(\n",
    "#         np.array(initial_frames).transpose(0, 3, 1, 2), \n",
    "#         fps=30, \n",
    "#         format=\"mp4\",\n",
    "#         caption=f\"Initial (untrained) - Reward: {initial_reward}, Steps: {initial_steps}\"\n",
    "#     ),\n",
    "#     \"initial_reward\": initial_reward\n",
    "# }, step=0)\n",
    "# print(f\"Initial reward: {initial_reward}, steps: {initial_steps}\")\n",
    "\n",
    "\n",
    "for step_idx, exp in enumerate(exp_collector.rollout()):\n",
    "    # exp = exp_collector.rollout()\n",
    "    # print(exp)\n",
    "    print()\n",
    "    # if exp is None:\n",
    "    #     continue\n",
    "    if step_idx>1:\n",
    "        break\n",
    "    else:\n",
    "        continue\n",
    "    if len(exp['ep_rewards'])>0 :\n",
    "        for ep_rew in exp['ep_rewards']:\n",
    "            # Update Beta / Logger for EACH episode found\n",
    "            current_beta = beta_scheduler.update(ep_rew)\n",
    "            total_rewards.append(ep_rew)\n",
    "            mean_reward = float(np.mean(total_rewards[-100:]))\n",
    "            \n",
    "            print(f\"Episode: {episode_idx} |Steps: {step_idx} | Reward: {ep_rew} | Mean: {mean_reward:.1f}\")\n",
    "            \n",
    "            # wandb.log({\n",
    "            #     \"episode_reward\": ep_rew, \n",
    "            #     \"mean_reward_100\": mean_reward,\n",
    "            #     \"episode_number\": episode_idx\n",
    "            # }, step=step_idx)\n",
    "            \n",
    "            episode_idx += 1\n",
    "            \n",
    "            \n",
    "        \n",
    "            if mean_reward>950:\n",
    "                # save_path = os.path.join(wandb.run.dir, \"policy_best.pt\")\n",
    "                # torch.save(policy.state_dict(), save_path)\n",
    "                # wandb.log({\"best_policy_path\": save_path}, step=step_idx)\n",
    "                # print(f\"Solved! Mean reward > 950 at episode {episode_idx}\")\n",
    "                solved = True\n",
    "                break\n",
    "        if solved: \n",
    "            break\n",
    "        \n",
    "    \n",
    "    states_list = exp['states']\n",
    "    raw_actions_list = exp['actions']\n",
    "    done_list = exp['done']\n",
    "    deltas_list = exp['deltas']\n",
    "    values_list = exp['values']\n",
    "    \n",
    "    batch_adv_t = compute_gae(deltas_list, done_list, GAMMA, LAMBDA).to(device)\n",
    "    \n",
    "    \n",
    "    batch_states_t = torch.cat(states_list, dim =0)\n",
    "    batch_actions_t = torch.cat(raw_actions_list, dim=0)\n",
    "    batch_value = torch.tensor(values_list,dtype = torch.float32, device=device)\n",
    "\n",
    "    # batch_adv_t = torch.tensor(adv_list, dtype = torch.float32, device=device)\n",
    "    \n",
    "    \n",
    "    mu_new, std, value = policy(batch_states_t)\n",
    "    value_t = value.squeeze(dim=1)\n",
    "    returns = batch_adv_t + batch_value\n",
    "    # loss_value = F.mse_loss(value_t, returns.detach())\n",
    "    #huberloss\n",
    "    delta = 1.0\n",
    "    loss_value = F.smooth_l1_loss(value_t,returns.detach(), beta=delta)\n",
    "      \n",
    "    \n",
    "    dist_t = torch.distributions.Normal(mu_new, std)\n",
    "    logp_u = dist_t.log_prob(batch_actions_t).sum(dim=-1)\n",
    "    a_t = torch.tanh(batch_actions_t)\n",
    "    logp_correction = torch.log(( 1 - a_t.pow(2))+1e-6).sum(dim=-1)\n",
    "    logp = logp_u - logp_correction\n",
    "    \n",
    "    \n",
    "    batch_adv_t = (batch_adv_t - batch_adv_t.mean())/(batch_adv_t.std() + 1e-8) # normalize adv_t after returns\n",
    "\n",
    "    loss_policy = -(logp * batch_adv_t.detach()).mean()\n",
    "    \n",
    "    \n",
    "    \n",
    "    entropy = dist_t.entropy().sum(dim=-1).mean()\n",
    "    \n",
    "    loss_total = loss_value + loss_policy - current_beta*entropy\n",
    "    \n",
    "    optimizer.zero_grad()\n",
    "    loss_total.backward()\n",
    "    torch.nn.utils.clip_grad_norm_(policy.parameters(), max_norm=0.5)\n",
    "    optimizer.step()\n",
    "    scheduler.step(mean_reward)\n",
    "    \n",
    "    \n",
    "    \n",
    "    with torch.no_grad():\n",
    "        \n",
    "        mu_t, std_t, v_t = policy(batch_states_t)\n",
    "        new_dist_t = torch.distributions.Normal(mu_t, std_t)\n",
    "        \n",
    "        kl_div = torch.distributions.kl_divergence(dist_t, new_dist_t).mean()\n",
    "        \n",
    "    grad_max = 0.0\n",
    "    grad_means = 0.0\n",
    "    grad_count = 0\n",
    "    for p in policy.parameters():\n",
    "\n",
    "        grad_max = max(grad_max, p.grad.abs().max().item())\n",
    "        grad_means += (p.grad ** 2).mean().sqrt().item()\n",
    "        grad_count += 1\n",
    "        \n",
    "        \n",
    "    adv_smoothed = smooth(\n",
    "                    adv_smoothed,\n",
    "                    float(np.mean(batch_adv_t.abs().mean().item()))\n",
    "                )\n",
    "    l_entropy = smooth(l_entropy, entropy.item())\n",
    "    l_policy = smooth(l_policy, loss_policy.item())\n",
    "    l_value = smooth(l_value, loss_value.item())\n",
    "    l_total = smooth(l_total, loss_total.item())\n",
    "    \n",
    "    \n",
    "    \n",
    "    # # break\n",
    "    # # print(f\"Episode: {episode_idx} |Steps: {step_idx} | Reward: {ep_rew} | Mean: {mean_reward:.1f}\")\n",
    "    # wandb.log({\n",
    "    #     # 'baseline':baseline,\n",
    "    #     'entropy_beta':current_beta,\n",
    "    #     'advantage':adv_smoothed,\n",
    "    #     'entropy':entropy,\n",
    "    #     'loss_policy':l_policy,\n",
    "    #     'loss_value':l_value,\n",
    "    #     'loss_entropy': l_entropy, \n",
    "    #     'loss_total': l_total,\n",
    "    #     'kl div': kl_div.item(),\n",
    "    #     \"mu_delta\": (mu_new - mu_old).abs().mean().item(),\n",
    "    #     \"std\": std.mean().item(),\n",
    "    #     \"adv_abs\": batch_adv_t.abs().mean().item(),\n",
    "    #     'grad_l2':grad_means/grad_count if grad_count else 0.0,\n",
    "    #     'grad_max':grad_max,\n",
    "    #     'batch_returns': returns,\n",
    "    #     \"current_episode\": episode_idx, \n",
    "    #     'saturation_fractions':(a_t.abs() > 0.99).float().mean().item(),\n",
    "    #     'action_mean': batch_actions_t.mean().item(),\n",
    "    #     'action_std': batch_actions_t.std().item(),\n",
    "    #     'action_clamp_rate': (\n",
    "    #         ((batch_actions_t <= action_low + 0.01).any(dim=-1) | \n",
    "    #         (batch_actions_t >= action_high - 0.01).any(dim=-1))\n",
    "    #         .float().mean().item()\n",
    "    #     ),\n",
    "    #     'mu_mean': mu_new.mean().item(),\n",
    "    #     'mu_std': mu_new.std().item(),\n",
    "    #     'policy_std_mean': std.mean().item(),\n",
    "    # }, step = step_idx)\n",
    "    \n",
    "    # # batch_raw_actions.clear()\n",
    "    # # batch_returns.clear()\n",
    "    # # batch_states.clear()\n",
    "    # mu_old = mu_new\n",
    "   \n",
    "\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "bab34bde",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.00233244 -0.00373462 -0.00850175 -0.00153787]\n",
      " [-0.00755476 -0.00759368  0.00877398  0.00321834]\n",
      " [ 0.00169804  0.00408513  0.00831098  0.00715005]]\n",
      "tensor([[ 0.0023, -0.0037, -0.0085, -0.0015],\n",
      "        [-0.0076, -0.0076,  0.0088,  0.0032],\n",
      "        [ 0.0017,  0.0041,  0.0083,  0.0072]], device='mps:0')\n"
     ]
    }
   ],
   "source": [
    "state, _ = envs.reset()\n",
    "print(state)\n",
    "state_t = torch.tensor(state, dtype=torch.float32, device=device)#.unsqueeze(0)\n",
    "print(state_t)\n",
    "mu, std, val = policy(state_t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bf12c153",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PolicyNet(\n",
       "  (net): Sequential(\n",
       "    (0): Linear(in_features=16, out_features=256, bias=True)\n",
       "    (1): ReLU()\n",
       "    (2): Linear(in_features=256, out_features=256, bias=True)\n",
       "    (3): ReLU()\n",
       "  )\n",
       "  (mu): Linear(in_features=256, out_features=16, bias=True)\n",
       "  (critic_head): Linear(in_features=256, out_features=1, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "b36d7da9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Box(-3.0, 3.0, (16, 1), float32)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "envs.action_space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "52525cff",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Box(-inf, inf, (16, 4), float64)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "envs.observation_space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "8af06376",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method Box.sample of Box(-3.0, 3.0, (16, 1), float32)>"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "envs.action_space.sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "8e952aa7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method Box.sample of Box(-inf, inf, (16, 4), float64)>"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "envs.observation_space.sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5adc7f0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "drl_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
