{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "99390014",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym \n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import typing as tt\n",
    "import torch  \n",
    "import torch.nn as nn \n",
    "import torch.nn.functional as F\n",
    "import wandb\n",
    "from gymnasium.wrappers import NormalizeObservation, NormalizeReward\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "e93beee5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device : mps\n"
     ]
    }
   ],
   "source": [
    "HIDDEN_LAYER1  = 256\n",
    "\n",
    "GAMMA = 0.99\n",
    "LR = 1e-4\n",
    "\n",
    "N_STEPS = 1\n",
    "ENTROPY_BETA = 0.01\n",
    "ENV_ID = 'InvertedPendulum-v5'\n",
    "N_ENV = 1\n",
    "BATCH_SIZE = 5\n",
    "\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    device = 'cuda'\n",
    "elif torch.backends.mps.is_available():\n",
    "    device = 'mps'\n",
    "else:\n",
    "    device = 'cpu' \n",
    "print(f'Using device : {device}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d48be493",
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make(ENV_ID)\n",
    "eval_env = gym.make(ENV_ID, render_mode='rgb_array')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c0297277",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((4,), (1,))"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.observation_space.shape, env.action_space.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a7da7513",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([ 0.26376065,  1.75201758, -0.5726076 , -1.58489694]),\n",
       " array([-0.769586], dtype=float32))"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.observation_space.sample(), env.action_space.sample()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "07475875",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(Box(-inf, inf, (4,), float32), Box(-3.0, 3.0, (1,), float32))"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.observation_space, env.action_space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ee371bc6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([-0.2851903 ,  1.5047672 , -0.98539096, -0.18656994], dtype=float32),\n",
       " array([-1.2208982], dtype=float32))"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env = NormalizeObservation(env)\n",
    "env.observation_space.sample(), env.action_space.sample()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4024470e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PolicyNet(nn.Module):\n",
    "    def __init__(self, input_size, fc, action_dim):\n",
    "        super().__init__()\n",
    "        self.input_size = input_size\n",
    "        self.fc = fc \n",
    "        self.action_dim = action_dim\n",
    "        \n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(self.input_size, self.fc), \n",
    "            nn.ReLU(), \n",
    "            \n",
    "        )\n",
    "        \n",
    "        self.mu = nn.Sequential(\n",
    "            nn.Linear(self.fc, self.action_dim), \n",
    "            # nn.Tanh()\n",
    "\n",
    "        )\n",
    "        self.log_std = nn.Parameter(torch.zeros(action_dim))\n",
    "\n",
    "        \n",
    "        self.critic_head = nn.Linear(self.fc, 1)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.net(x)\n",
    "        mu = self.mu(x)\n",
    "        # Use learned constant std (common in simple continuous control)\n",
    "        std = torch.exp(self.log_std.clamp(-2, 0.5))  # exp(-2)=0.135, exp(0.5)=1.65\n",
    "        std = std.expand_as(mu)  # Broadcast to batch size\n",
    "        \n",
    "        v = self.critic_head(x)\n",
    "        return mu, std, v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "bdae4360",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PolicyNet(\n",
       "  (net): Sequential(\n",
       "    (0): Linear(in_features=4, out_features=256, bias=True)\n",
       "    (1): ReLU()\n",
       "  )\n",
       "  (mu): Sequential(\n",
       "    (0): Linear(in_features=256, out_features=1, bias=True)\n",
       "  )\n",
       "  (critic_head): Linear(in_features=256, out_features=1, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "policy = PolicyNet(\n",
    "    env.observation_space.shape[0], \n",
    "    HIDDEN_LAYER1, \n",
    "    env.action_space.shape[0]\n",
    ").to(device)\n",
    "policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3414ed4f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mu tensor([[-0.1050]], device='mps:0', grad_fn=<LinearBackward0>)\n",
      "var:  tensor([[1.]], device='mps:0', grad_fn=<ExpandBackward0>)\n",
      "Normal(loc: tensor([[-0.1050]], device='mps:0', grad_fn=<LinearBackward0>), scale: tensor([[1.]], device='mps:0', grad_fn=<ExpandBackward0>))\n",
      "a:tensor([[-0.2339]], device='mps:0')\n",
      "action:tensor([[-0.7018]], device='mps:0')\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([-0.7017853], dtype=float32)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "low = torch.tensor(env.action_space.low, dtype=torch.float32, device=device)\n",
    "high = torch.tensor(env.action_space.high, dtype=torch.float32, device=device)\n",
    "state, _ = env.reset()\n",
    "state_t = torch.tensor(state, dtype=torch.float32, device = device).unsqueeze(0)\n",
    "mu_v, std_v,  value = policy(state_t)\n",
    "print('mu', mu_v)\n",
    "print('var: ', std_v)\n",
    "# break\n",
    "# mu = mu_v.data.cpu().numpy()\n",
    "# print('mu', mu)\n",
    "# dist = torch.distributions.Normal()\n",
    "# action = dist.sample().item()\n",
    "# std = torch.sqrt(var_v)\n",
    "dist = torch.distributions.Normal(mu_v, std_v)\n",
    "print(dist)\n",
    "u = dist.sample()\n",
    "a = torch.tanh(u)\n",
    "print(f'a:{a}')\n",
    "action = low + (a+1) * (high-low)*0.5\n",
    "print(f'action:{action}')\n",
    "action_env = action.squeeze(0).detach().cpu().numpy()\n",
    "action_env\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e0b4ba2c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-3.0 2.9996337890625\n"
     ]
    }
   ],
   "source": [
    "# sanity check for action range.\n",
    "with torch.no_grad():\n",
    "    samples = dist.sample((10000000,))          # [10000, batch, act_dim]\n",
    "    a = torch.tanh(samples)\n",
    "    actions = low + (a + 1) * (high - low) * 0.5\n",
    "    # actions = torch.clamp(u, low, high)\n",
    "    print(actions.min().item(), actions.max().item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "dd06b292",
   "metadata": {},
   "outputs": [],
   "source": [
    "def experience_generator(env, policy, gamma, n_steps):\n",
    "    while True: \n",
    "        state_list = []\n",
    "        raw_action_list = []\n",
    "        reward_list = []\n",
    "        return_list = []\n",
    "        done_list = []\n",
    "        last_state_list = []\n",
    "        \n",
    "        done = False\n",
    "        ep_rew = 0\n",
    "        state, _ = env.reset()\n",
    "        while not done:\n",
    "            state_t = torch.tensor(state, dtype=torch.float32, device = device).unsqueeze(0)\n",
    "            with torch.no_grad():\n",
    "                mu_v, std_v,  value = policy(state_t)\n",
    "            dist = torch.distributions.Normal(mu_v, std_v)\n",
    "            # print(dist)\n",
    "            u = dist.sample()\n",
    "            a = torch.tanh(u)\n",
    "            # print(f'a:{a}')\n",
    "            action = low + (a+1) * (high-low)*0.5\n",
    "            # print(f'action:{action}')\n",
    "            # print(action)\n",
    "            action_env = action.squeeze(0).detach().cpu().numpy()\n",
    "            new_state, rew, term, trunc, info = env.step(action_env)\n",
    "            done = term or trunc\n",
    "            ep_rew += rew\n",
    "            state_list.append(state_t)\n",
    "            raw_action_list.append(u)\n",
    "            reward_list.append(rew)\n",
    "            done_list.append(done)\n",
    "            \n",
    "            last_state_list.append(new_state)\n",
    "                \n",
    "            if len(reward_list)>=n_steps:\n",
    "                ret = sum([reward_list[i]* (gamma**i) for i in range(n_steps)])\n",
    "                \n",
    "                yield { \n",
    "                    'state':state_list[0], \n",
    "                    'raw_action':raw_action_list[0],\n",
    "                    'ret':ret,\n",
    "                    'done':done,\n",
    "                    'last_state':last_state_list[n_steps-1] if not done else None, \n",
    "                    'ep_reward': ep_rew if done_list[0] else None, \n",
    "                    'reward_list':reward_list,\n",
    "                }\n",
    "                \n",
    "                state_list.pop(0)\n",
    "                raw_action_list.pop(0)\n",
    "                reward_list.pop(0)\n",
    "                done_list.pop(0)\n",
    "                last_state_list.pop(0)\n",
    "                \n",
    "            state = new_state\n",
    "                \n",
    "        else:\n",
    "            while len(reward_list)>0:\n",
    "                ret = sum([reward_list[i]* (gamma**i) for i in range(len(reward_list))])\n",
    "                \n",
    "                yield { \n",
    "                    'state':state_list[0], \n",
    "                    'raw_action':raw_action_list[0],\n",
    "                    'ret':ret,\n",
    "                    'done':done,\n",
    "                    'last_state': None, \n",
    "                    'ep_reward': ep_rew if done_list[0] else None,\n",
    "                    'reward_list':reward_list,\n",
    "                }\n",
    "                \n",
    "                state_list.pop(0)\n",
    "                raw_action_list.pop(0)\n",
    "                reward_list.pop(0)\n",
    "                done_list.pop(0)\n",
    "                last_state_list.pop(0)\n",
    "                \n",
    "def record_video(env, policy, device, max_steps=500):\n",
    "    \"\"\"Record a single episode and return frames + reward\"\"\"\n",
    "    frames = []\n",
    "    state, _ = env.reset()\n",
    "    done = False\n",
    "    total_reward = 0\n",
    "    steps = 0\n",
    "    low = torch.tensor(env.action_space.low, dtype=torch.float32, device=device)\n",
    "    high = torch.tensor(env.action_space.high, dtype=torch.float32, device=device)\n",
    "    \n",
    "    while not done and steps < max_steps:\n",
    "        frame = env.render()\n",
    "        frames.append(frame)        \n",
    "        state_tensor = torch.tensor(state, dtype=torch.float32, device=device).unsqueeze(0)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            mu, std, val = policy(state_tensor)\n",
    "        dist = torch.distributions.Normal(mu, std)\n",
    "        u = dist.sample()\n",
    "        a = torch.tanh(u)\n",
    "        action = low + (a+1) * (high-low)*0.5\n",
    "            \n",
    "        action_env = action.squeeze(0).detach().cpu().numpy()\n",
    "        state, reward, terminated, truncated, _ = env.step(action_env)\n",
    "        total_reward += reward\n",
    "        done = terminated or truncated\n",
    "        steps += 1\n",
    "        \n",
    "    return frames, total_reward, steps\n",
    "\n",
    "def smooth(old: tt.Optional[float], val: float, alpha: float = 0.95) -> float:\n",
    "    if old is None:\n",
    "        return val\n",
    "    return old * alpha + (1-alpha)*val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "2b6fec64",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "state:tensor([[-0.1718,  0.2611, -0.3080,  0.2984]], device='mps:0')\n",
      "actions : tensor([[-0.5496]], device='mps:0')\n",
      "done:False\n",
      "last_state:[-0.34773558  0.44511846 -0.9187471   0.92713106]\n",
      "bs_val;0.18055394291877747\n",
      "ret:1.0\n",
      "ep_reward:None\n",
      "reward_list:[1]\n",
      "return after bootstrap: 1.1787484034895896\n",
      "\n",
      "state:tensor([[-0.3477,  0.4451, -0.9187,  0.9271]], device='mps:0')\n",
      "actions : tensor([[0.3344]], device='mps:0')\n",
      "done:False\n",
      "last_state:[-0.58407205  0.68568045 -0.51969343  0.5064329 ]\n",
      "bs_val;0.11198025941848755\n",
      "ret:1.0\n",
      "ep_reward:None\n",
      "reward_list:[1]\n",
      "return after bootstrap: 1.1108604568243026\n",
      "\n",
      "state:tensor([[-0.5841,  0.6857, -0.5197,  0.5064]], device='mps:0')\n",
      "actions : tensor([[0.4569]], device='mps:0')\n",
      "done:False\n",
      "last_state:[-0.55482924  0.654547   -0.00333064 -0.00120364]\n",
      "bs_val;0.03157982975244522\n",
      "ret:1.0\n",
      "ep_reward:None\n",
      "reward_list:[1]\n",
      "return after bootstrap: 1.0312640314549208\n",
      "\n",
      "state:tensor([[-0.5548,  0.6545, -0.0033, -0.0012]], device='mps:0')\n",
      "actions : tensor([[-0.0277]], device='mps:0')\n",
      "done:False\n",
      "last_state:[-0.38969362  0.50290066 -0.04922104  0.08587797]\n",
      "bs_val;0.03403971344232559\n",
      "ret:1.0\n",
      "ep_reward:None\n",
      "reward_list:[1]\n",
      "return after bootstrap: 1.0336993163079022\n",
      "\n",
      "state:tensor([[-0.3897,  0.5029, -0.0492,  0.0859]], device='mps:0')\n",
      "actions : tensor([[1.3436]], device='mps:0')\n",
      "done:False\n",
      "last_state:[ 0.0697705   0.06371056  1.0071211  -0.96875274]\n",
      "bs_val;-0.017310991883277893\n",
      "ret:1.0\n",
      "ep_reward:None\n",
      "reward_list:[1]\n",
      "return after bootstrap: 0.9828621180355549\n",
      "batch_actions: [tensor([[-0.5496]], device='mps:0'), tensor([[0.3344]], device='mps:0'), tensor([[0.4569]], device='mps:0'), tensor([[-0.0277]], device='mps:0'), tensor([[1.3436]], device='mps:0')]\n",
      "batch_states_t: tensor([[-0.1718,  0.2611, -0.3080,  0.2984],\n",
      "        [-0.3477,  0.4451, -0.9187,  0.9271],\n",
      "        [-0.5841,  0.6857, -0.5197,  0.5064],\n",
      "        [-0.5548,  0.6545, -0.0033, -0.0012],\n",
      "        [-0.3897,  0.5029, -0.0492,  0.0859]], device='mps:0')\n",
      "batch_actions_t: tensor([[-0.5496],\n",
      "        [ 0.3344],\n",
      "        [ 0.4569],\n",
      "        [-0.0277],\n",
      "        [ 1.3436]], device='mps:0')\n",
      "batch_returns_t: tensor([1.1787, 1.1109, 1.0313, 1.0337, 0.9829], device='mps:0')\n",
      "logp_u:tensor([-1.2552, -0.9201, -0.9314, -0.9852, -1.4540], device='mps:0',\n",
      "       grad_fn=<SumBackward1>)\n",
      "log_prob_correction:tensor([-2.8794e-01, -1.0982e-01, -2.0191e-01, -7.6448e-04, -1.4326e+00],\n",
      "       device='mps:0')\n",
      "logp(after correction):tensor([-0.9673, -0.8103, -0.7295, -0.9845, -0.0214], device='mps:0',\n",
      "       grad_fn=<SubBackward0>)\n",
      "Returns:tensor([1.1787, 1.1109, 1.0313, 1.0337, 0.9829], device='mps:0')\n",
      "value_t:tensor([0.1272, 0.1806, 0.1120, 0.0316, 0.0340], device='mps:0',\n",
      "       grad_fn=<SqueezeBackward1>)\n",
      "dis_t: Normal(loc: torch.Size([5, 1]), scale: torch.Size([5, 1]))\n",
      "actions_prob_t:tensor([-0.9673, -0.8103, -0.7295, -0.9845, -0.0214], device='mps:0',\n",
      "       grad_fn=<SubBackward0>)\n",
      "loss_value: 0.9441758990287781\n",
      "adv_t: tensor([1.0516, 0.9303, 0.9193, 1.0021, 0.9488], device='mps:0')\n",
      "value_t: tensor([0.1272, 0.1806, 0.1120, 0.0316, 0.0340], device='mps:0',\n",
      "       grad_fn=<SqueezeBackward1>)\n",
      "loss_policy: 0.6896985769271851\n",
      "entropy: 1.4189385175704956\n",
      "loss_entropy: -0.014189384877681732\n",
      "\n",
      "state:tensor([[ 0.0698,  0.0637,  1.0071, -0.9688]], device='mps:0')\n",
      "actions : tensor([[0.7946]], device='mps:0')\n",
      "done:False\n",
      "last_state:[ 1.0633785 -0.904408   1.7893094 -1.7220125]\n",
      "bs_val;0.21109706163406372\n",
      "ret:1.0\n",
      "ep_reward:None\n",
      "reward_list:[1]\n",
      "return after bootstrap: 1.208986091017723\n",
      "\n",
      "state:tensor([[ 1.0634, -0.9044,  1.7893, -1.7220]], device='mps:0')\n",
      "actions : tensor([[1.1020]], device='mps:0')\n",
      "done:True\n",
      "last_state:None\n",
      "bs_val;0\n",
      "ret:0.0\n",
      "ep_reward:6\n",
      "reward_list:[0]\n",
      "return after bootstrap: 0.0\n",
      "episode : 0 | step: 6 | episode reward : 6 | mean reward/100 eps : 6.0\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "policy = PolicyNet(\n",
    "    env.observation_space.shape[0], \n",
    "    HIDDEN_LAYER1, \n",
    "    env.action_space.shape[0]\n",
    ").to(device)\n",
    "optimizer = torch.optim.Adam(policy.parameters(),lr=LR, )\n",
    "\n",
    "\n",
    "\n",
    "batch_states = []\n",
    "batch_returns = []\n",
    "batch_raw_actions = []\n",
    "batch_values = []\n",
    "done_list = []\n",
    "last_state_list = []\n",
    "total_rewards = []\n",
    "adv_smoothed = l_entropy = l_policy = l_value = l_total = None\n",
    "episode_idx = 0\n",
    "# BATCH_SIZE = 1 * REWARD_STEP  # n_env * reward_steps\n",
    "\n",
    "for step_idx, exp in enumerate(experience_generator(env, policy, GAMMA, N_STEPS)):\n",
    "    batch_states.append(exp['state']) \n",
    "    batch_raw_actions.append(exp['raw_action'])\n",
    "\n",
    "    print(f'\\nstate:{exp['state']}')\n",
    "    print(f'actions : {exp['raw_action']}')\n",
    "    \n",
    "    print(f'done:{exp['done']}')\n",
    "    print(f'last_state:{exp['last_state']}')\n",
    "    ## bootstrapping if the episode is not completed withing REWARD_STEP\n",
    "    if exp['last_state'] is not None:\n",
    "        last_state = exp['last_state']\n",
    "        last_state_t = torch.tensor(last_state, dtype=torch.float32, device=device).unsqueeze(0)\n",
    "        with torch.no_grad():\n",
    "            _, _, bs_val = policy(last_state_t)\n",
    "        bs_val = bs_val.item()\n",
    "        ret = exp['ret'] +  (bs_val) * (GAMMA**N_STEPS) \n",
    "        batch_returns.append(ret)\n",
    "    else:\n",
    "        batch_returns.append(exp['ret'])\n",
    "        ret =exp['ret']\n",
    "        bs_val = 0\n",
    "        \n",
    "    print(f'bs_val;{bs_val}')\n",
    "    print(f'ret:{exp['ret']}')\n",
    "    print(f'ep_reward:{exp['ep_reward']}')\n",
    "    print(f'reward_list:{exp['reward_list']}')\n",
    "    print(f\"return after bootstrap: {ret}\")\n",
    "        \n",
    "    if exp['ep_reward'] is not None:\n",
    "        episode_reward = exp['ep_reward']\n",
    "        total_rewards.append(episode_reward)\n",
    "        mean_reward = float(np.mean(total_rewards[-100:]))\n",
    "        print(f\"episode : {episode_idx} | step: {step_idx} | episode reward : {episode_reward} | mean reward/100 eps : {mean_reward}\")\n",
    "        # wandb.log({\n",
    "        #     \"episode_reward\": episode_reward, \n",
    "        #     \"mean_reward_100\": mean_reward,  \n",
    "        #     'episode_number': episode_idx,   \n",
    "        #     \"steps_per_episode\": step_idx / max(episode_idx, 1)\n",
    "        # }, step=step_idx)\n",
    "        episode_idx += 1\n",
    "        break\n",
    "        if mean_reward>950:\n",
    "            print(f\"Solved! Mean reward > 450 at episode {episode_idx}\")\n",
    "            break\n",
    "        \n",
    "        \n",
    "        \n",
    "    if len(batch_states) < BATCH_SIZE:\n",
    "        continue\n",
    "    print(f\"batch_actions: {batch_raw_actions}\")\n",
    "    batch_states_t = torch.cat(batch_states, dim=0)\n",
    "    batch_actions_t = torch.cat(batch_raw_actions, dim=0).to(device).float()  # each element in batch_raw_actions is [1, act_dim]\n",
    "    batch_returns_t = torch.tensor(batch_returns, dtype=torch.float32, device=device)\n",
    "    print(f\"batch_states_t: {batch_states_t}\")\n",
    "    print(f\"batch_actions_t: {batch_actions_t}\")\n",
    "    # print(f\"batch_actions_t sum: {batch_actions_t.sum(dim=-1)}\")\n",
    "    print(f\"batch_returns_t: {batch_returns_t}\")\n",
    "    mu, std, value_t = policy(batch_states_t)\n",
    "    value_t = value_t.squeeze(-1)\n",
    "    # std = std.clamp(1e-3, 2.0)\n",
    "\n",
    "    \n",
    "    \n",
    "    dist_t = torch.distributions.Normal(mu, std)\n",
    "    \n",
    "    # u_t = batch_actions_t                         # pre-tanh actions, [B, act_dim]\n",
    "    logp_u = dist_t.log_prob(batch_actions_t).sum(dim=-1)     # [B]\n",
    "\n",
    "    a_t = torch.tanh(batch_actions_t)\n",
    "    log_prob_correction = torch.log(1.0 - a_t.pow(2) + 1e-6).sum(dim=-1)  # [B]\n",
    "    logp = logp_u - log_prob_correction                      # [B]\n",
    "\n",
    "    print(f\"logp_u:{logp_u}\")\n",
    "    print(f\"log_prob_correction:{log_prob_correction}\")\n",
    "    print(f\"logp(after correction):{logp}\")\n",
    "    \n",
    "    \n",
    "    \n",
    "    adv_t = (batch_returns_t - value_t).detach()\n",
    "    loss_policy = - (logp * adv_t).mean()\n",
    "\n",
    "    returns = adv_t + value_t.detach()\n",
    "    print(f\"Returns:{returns}\")\n",
    "    print(f\"value_t:{value_t}\")\n",
    "    loss_value = F.mse_loss(value_t, batch_returns_t.detach())\n",
    "    \n",
    "    entropy = dist_t.entropy().mean()\n",
    "    loss_entropy = -ENTROPY_BETA*entropy\n",
    "    \n",
    "    \n",
    "    print(f\"dis_t: {dist_t}\")\n",
    "    print(f'actions_prob_t:{logp}')\n",
    "    \n",
    "    print(f\"loss_value: {loss_value}\")\n",
    "\n",
    "    print(f\"adv_t: {adv_t}\")\n",
    "    print(f\"value_t: {value_t}\")\n",
    "    print(f\"loss_policy: {loss_policy}\")\n",
    "    print(f\"entropy: {entropy}\")\n",
    "    print(f\"loss_entropy: {loss_entropy}\")\n",
    "    # break\n",
    "\n",
    "    loss_total = loss_value + loss_policy + loss_entropy\n",
    "    \n",
    "    optimizer.zero_grad()\n",
    "    loss_total.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    \n",
    "    # with torch.no_grad():\n",
    "    #     mu_t, std_t, v_t = policy(batch_states_t)\n",
    "    #     new_dist_t = torch.distributions.Normal(mu_t, std_t)\n",
    "        \n",
    "    #     kl_div = torch.distributions.kl_divergence(dist_t, new_dist_t).mean()\n",
    "        \n",
    "    # grad_max = 0.0\n",
    "    # grad_means = 0.0\n",
    "    # grad_count = 0\n",
    "    # for p in policy.parameters():\n",
    "    #     grad_max = max(grad_max, p.grad.abs().max().item())\n",
    "    #     grad_means += (p.grad ** 2).mean().sqrt().item()\n",
    "    #     grad_count += 1\n",
    "        \n",
    "        \n",
    "    # adv_smoothed = smooth(\n",
    "    #                 adv_smoothed,\n",
    "    #                 float(np.mean(adv_t.mean().item()))\n",
    "    #             )\n",
    "    # l_entropy = smooth(l_entropy, loss_entropy.item())\n",
    "    # l_policy = smooth(l_policy, loss_policy.item())\n",
    "    # l_value = smooth(l_value, loss_value.item())\n",
    "    # l_total = smooth(l_total, loss_total.item())\n",
    "    \n",
    "    \n",
    "    \n",
    "    # # break\n",
    "\n",
    "    # # wandb.log({\n",
    "    # #     # 'baseline':baseline,\n",
    "    # #     'advantage':adv_smoothed,\n",
    "    # #     'entropy':entropy,\n",
    "    # #     'loss_policy':l_policy,\n",
    "    # #     'loss_value':l_value,\n",
    "    # #     'loss_entropy': l_entropy, \n",
    "    # #     'loss_total': l_total,\n",
    "    # #     'kl div': kl_div.item(),\n",
    "    # #     'grad_l2':grad_means/grad_count,\n",
    "    # #     'grad_max':grad_max,\n",
    "    # #     'batch_scales': batch_returns,\n",
    "    # #     \"current_episode\": episode_idx\n",
    "    # # }, step = step_idx)\n",
    "    \n",
    "    batch_raw_actions.clear()\n",
    "    batch_returns.clear()\n",
    "    batch_states.clear()\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "ae289492",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_states_t: tensor([[-8.6029e-03, -8.4917e-03, -1.2738e-04, -1.9363e-03],\n",
      "        [-1.2055e-02, -7.2918e-04, -1.7203e-01,  3.8616e-01],\n",
      "        [-1.5743e-02,  6.9793e-03, -1.3122e-02,  6.1377e-03],\n",
      "        [-3.1883e-02,  4.3436e-02, -7.9160e-01,  1.7981e+00],\n",
      "        [-5.8900e-02,  1.0370e-01, -5.6137e-01,  1.2355e+00],\n",
      "        [-9.5712e-02,  1.8669e-01, -1.2763e+00,  2.8993e+00],\n",
      "        [-9.0386e-03, -1.0421e-03,  6.5123e-03, -9.4464e-03],\n",
      "        [-7.6496e-03, -4.0059e-03,  6.2797e-02, -1.3754e-01],\n",
      "        [-1.4319e-02,  1.1792e-02, -3.9493e-01,  9.1576e-01],\n",
      "        [-3.4847e-02,  5.8686e-02, -6.3142e-01,  1.4306e+00],\n",
      "        [-5.0815e-02,  9.4200e-02, -1.6951e-01,  3.6809e-01],\n",
      "        [-5.9134e-02,  1.1383e-01, -2.4642e-01,  6.1380e-01],\n",
      "        [-8.3554e-02,  1.7322e-01, -9.7174e-01,  2.3377e+00],\n",
      "        [ 7.0420e-03,  6.5816e-03, -1.0610e-03, -5.2243e-03],\n",
      "        [ 2.2962e-02, -3.0341e-02,  7.9487e-01, -1.8224e+00],\n",
      "        [ 3.8894e-02, -6.5366e-02,  5.5118e-03,  3.7025e-02],\n",
      "        [ 2.7404e-02, -3.8075e-02, -5.7868e-01,  1.3156e+00],\n",
      "        [ 1.7676e-02, -1.8390e-02,  8.9481e-02, -3.0502e-01],\n",
      "        [ 4.0711e-02, -7.5510e-02,  1.0593e+00, -2.5289e+00],\n",
      "        [-4.5954e-03, -9.9504e-03,  9.0218e-03, -5.6347e-04],\n",
      "        [-4.8785e-04, -1.8776e-02,  1.9585e-01, -4.3634e-01],\n",
      "        [ 8.2459e-03, -3.8188e-02,  2.4105e-01, -5.3667e-01],\n",
      "        [ 6.7600e-03, -3.4106e-02, -3.1348e-01,  7.2394e-01],\n",
      "        [-1.2251e-02,  8.4948e-03, -6.3679e-01,  1.4050e+00],\n",
      "        [-2.9742e-02,  4.5070e-02, -2.4001e-01,  4.4480e-01],\n",
      "        [-2.4765e-02,  2.9639e-02,  4.8668e-01, -1.1967e+00],\n",
      "        [-1.6316e-02,  8.9577e-03, -6.1813e-02,  1.4004e-01],\n",
      "        [-1.8705e-02,  1.4423e-02, -5.7796e-02,  1.3440e-01],\n",
      "        [-6.7514e-03, -1.2989e-02,  6.5351e-01, -1.4877e+00],\n",
      "        [ 2.6620e-02, -8.7928e-02,  1.0148e+00, -2.2616e+00],\n",
      "        [ 5.5813e-02, -1.5156e-01,  4.4872e-01, -9.5420e-01],\n",
      "        [-9.5385e-03,  6.9939e-03, -2.5717e-03, -4.7094e-03],\n",
      "        [-2.5205e-02,  4.2911e-02, -7.7852e-01,  1.7820e+00],\n",
      "        [-4.6801e-02,  9.1244e-02, -3.0420e-01,  6.6122e-01],\n",
      "        [-5.4406e-02,  1.0824e-01, -7.7146e-02,  1.9854e-01],\n",
      "        [-4.5088e-02,  8.9488e-02,  5.4156e-01, -1.1221e+00],\n",
      "        [-2.1371e-02,  4.2629e-02,  6.4498e-01, -1.2287e+00],\n",
      "        [-1.3510e-02,  3.6899e-02, -2.4856e-01,  9.1127e-01],\n",
      "        [-2.5794e-02,  7.8554e-02, -3.6597e-01,  1.1757e+00],\n",
      "        [-4.3301e-02,  1.3247e-01, -5.0961e-01,  1.5249e+00],\n",
      "        [-6.0997e-02,  1.8824e-01, -3.7671e-01,  1.2790e+00],\n",
      "        [-5.1802e-03,  3.1961e-03, -8.9385e-03, -7.8667e-03],\n",
      "        [-1.7280e-02,  3.0098e-02, -5.9442e-01,  1.3391e+00],\n",
      "        [-5.3048e-02,  1.1057e-01, -1.1925e+00,  2.6781e+00],\n",
      "        [-4.6542e-04,  3.2211e-03,  9.4837e-04,  8.9879e-04],\n",
      "        [ 4.7576e-03, -8.6313e-03,  2.5951e-01, -5.8761e-01],\n",
      "        [ 7.4965e-03, -1.4034e-02, -1.2108e-01,  3.0385e-01],\n",
      "        [ 1.7928e-03, -4.1635e-04, -1.6424e-01,  3.7869e-01],\n",
      "        [ 8.5356e-04,  1.3765e-03,  1.1623e-01, -2.7945e-01],\n",
      "        [ 1.6767e-02, -3.5489e-02,  6.7811e-01, -1.5530e+00]], device='mps:0')\n",
      "batch_actions_t: tensor([[-0.1753],\n",
      "        [ 0.1585],\n",
      "        [-1.0496],\n",
      "        [ 0.2276],\n",
      "        [-0.9134],\n",
      "        [-0.1378],\n",
      "        [ 0.0568],\n",
      "        [-0.4964],\n",
      "        [-0.2477],\n",
      "        [ 0.4995],\n",
      "        [-0.0696],\n",
      "        [-0.9202],\n",
      "        [-0.2189],\n",
      "        [ 1.1010],\n",
      "        [-1.0568],\n",
      "        [-0.6850],\n",
      "        [ 0.7901],\n",
      "        [ 2.2001],\n",
      "        [ 0.8178],\n",
      "        [ 0.1891],\n",
      "        [ 0.0466],\n",
      "        [-0.6297],\n",
      "        [-0.3467],\n",
      "        [ 0.4121],\n",
      "        [ 0.9345],\n",
      "        [-0.6037],\n",
      "        [ 0.0043],\n",
      "        [ 0.8988],\n",
      "        [ 0.3921],\n",
      "        [-0.6387],\n",
      "        [ 0.3129],\n",
      "        [-1.0429],\n",
      "        [ 0.5097],\n",
      "        [ 0.2391],\n",
      "        [ 0.7489],\n",
      "        [ 0.1223],\n",
      "        [-1.3963],\n",
      "        [-0.1204],\n",
      "        [-0.1453],\n",
      "        [ 0.1390],\n",
      "        [ 0.9460],\n",
      "        [-0.6744],\n",
      "        [-0.7074],\n",
      "        [-0.9347],\n",
      "        [ 0.2665],\n",
      "        [-0.3992],\n",
      "        [-0.0468],\n",
      "        [ 0.2870],\n",
      "        [ 0.6431],\n",
      "        [ 0.4634]], device='mps:0')\n",
      "batch_returns_t: tensor([ 4.9010,  3.9404,  2.9701,  1.9900,  1.0000,  0.0000,  5.8520,  4.9010,\n",
      "         3.9404,  2.9701,  1.9900,  1.0000,  0.0000,  4.9010,  3.9404,  2.9701,\n",
      "         1.9900,  1.0000,  0.0000, 10.4662,  9.5618,  8.6483,  7.7255,  6.7935,\n",
      "         5.8520,  4.9010,  3.9404,  2.9701,  1.9900,  1.0000,  0.0000,  8.6483,\n",
      "         7.7255,  6.7935,  5.8520,  4.9010,  3.9404,  2.9701,  1.9900,  1.0000,\n",
      "         0.0000,  1.9900,  1.0000,  0.0000,  6.7935,  5.8520,  4.9010,  3.9404,\n",
      "         2.9701,  1.9900], device='mps:0')\n",
      "dis_t: Normal(loc: torch.Size([50, 1]), scale: torch.Size([50, 1]))\n",
      "actions_prob_t:tensor([-0.5028, -0.4661, -1.9383, -0.4378, -1.5565,  0.1218, -0.4519, -0.8210,\n",
      "        -0.4185, -0.8966, -0.4383, -1.5742, -0.0766, -1.9042, -1.9084, -1.0685,\n",
      "        -1.6951, -6.0627, -1.0203, -0.4839, -0.4747, -1.0551, -0.5111, -0.7540,\n",
      "        -1.6146, -1.1198, -0.4403, -1.4393, -0.6556, -1.1724, -0.6057, -1.9213,\n",
      "        -1.0308, -0.4988, -1.1047, -0.5452, -2.9632, -0.3783, -0.3319, -0.3039,\n",
      "        -1.8831, -1.0833, -0.9659, -2.1060, -0.5225, -0.7531, -0.4350, -0.5463,\n",
      "        -0.8903, -0.7070], device='mps:0', grad_fn=<SumBackward1>)\n",
      "loss_value: 21.011167526245117\n",
      "adv_t: tensor([ 4.8683,  3.8321,  2.9386,  1.6917,  0.7557, -0.3964,  5.8221,  4.8941,\n",
      "         3.7419,  2.7050,  1.9016,  0.8597, -0.3568,  4.8697,  4.2023,  2.9198,\n",
      "         1.7431,  1.0148,  0.3966, 10.4329,  9.5981,  8.6943,  7.5548,  6.5345,\n",
      "         5.7406,  5.0457,  3.8832,  2.9151,  2.1864,  1.3411,  0.0940,  8.6185,\n",
      "         7.4277,  6.6425,  5.8021,  5.0405,  4.0958,  2.7553,  1.7362,  0.7012,\n",
      "        -0.2751,  1.9597,  0.7457, -0.3744,  6.7613,  5.9076,  4.8033,  3.8314,\n",
      "         2.9883,  2.1989], device='mps:0')\n",
      "value_t: tensor([ 0.0327,  0.1083,  0.0315,  0.2983,  0.2443,  0.3964,  0.0299,  0.0069,\n",
      "         0.1985,  0.2651,  0.0884,  0.1403,  0.3568,  0.0313, -0.2619,  0.0503,\n",
      "         0.2469, -0.0148, -0.3966,  0.0333, -0.0363, -0.0460,  0.1707,  0.2590,\n",
      "         0.1114, -0.1448,  0.0572,  0.0550, -0.1964, -0.3411, -0.0940,  0.0298,\n",
      "         0.2978,  0.1509,  0.0499, -0.1395, -0.1554,  0.2148,  0.2538,  0.2988,\n",
      "         0.2751,  0.0303,  0.2543,  0.3744,  0.0321, -0.0556,  0.0977,  0.1090,\n",
      "        -0.0182, -0.2089], device='mps:0', grad_fn=<SqueezeBackward1>)\n",
      "loss_policy: 3.4928359985351562\n",
      "entropy: 0.9008358716964722\n",
      "loss_entropy: -0.009008358232676983\n"
     ]
    }
   ],
   "source": [
    "\n",
    "batch_states_t = torch.cat(batch_states, dim=0)\n",
    "batch_actions_t = torch.cat(batch_raw_actions).float()\n",
    "batch_returns_t = torch.tensor(batch_returns, dtype=torch.float32, device=device)\n",
    "\n",
    "mu, std, value_t = policy(batch_states_t)\n",
    "value_t = value_t.squeeze(-1)\n",
    "dist_t = torch.distributions.Normal(mu, std)\n",
    "actions_prob_t = dist_t.log_prob(batch_actions_t).sum(dim=-1)\n",
    "\n",
    "loss_value = F.mse_loss(value_t, batch_returns_t.detach())\n",
    "\n",
    "\n",
    "adv_t = (batch_returns_t - value_t).detach()\n",
    "loss_policy = - (actions_prob_t * adv_t).mean()\n",
    "\n",
    "\n",
    "\n",
    "entropy = dist_t.entropy().mean()\n",
    "loss_entropy = -ENTROPY_BETA*entropy\n",
    "print(f\"batch_states_t: {batch_states_t}\")\n",
    "print(f\"batch_actions_t: {batch_actions_t}\")\n",
    "# print(f\"batch_actions_t sum: {batch_actions_t.sum(dim=-1)}\")\n",
    "print(f\"batch_returns_t: {batch_returns_t}\")\n",
    "\n",
    "print(f\"dis_t: {dist_t}\")\n",
    "print(f'actions_prob_t:{actions_prob_t}')\n",
    "\n",
    "print(f\"loss_value: {loss_value}\")\n",
    "\n",
    "print(f\"adv_t: {adv_t}\")\n",
    "print(f\"value_t: {value_t}\")\n",
    "print(f\"loss_policy: {loss_policy}\")\n",
    "print(f\"entropy: {entropy}\")\n",
    "print(f\"loss_entropy: {loss_entropy}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7e5de70",
   "metadata": {},
   "outputs": [],
   "source": [
    "# state:tensor([[-0.0619,  0.1597, -0.2782,  0.7257]], device='mps:0')\n",
    "# actions : tensor([[0.6015]], device='mps:0')\n",
    "\n",
    "\n",
    "#single loop outputs ( batchsize = 32)\n",
    "\n",
    "# batch_actions: [tensor([[1.2787]], device='mps:0'), tensor([[-0.5051]], device='mps:0'), tensor([[0.9226]], device='mps:0'), tensor([[-0.4800]], device='mps:0'), tensor([[-0.1513]], device='mps:0'), tensor([[-0.3652]], device='mps:0'), tensor([[-0.6029]], device='mps:0'), tensor([[0.1439]], device='mps:0'), tensor([[-0.1041]], device='mps:0'), tensor([[-0.4831]], device='mps:0'), tensor([[-0.1976]], device='mps:0'), tensor([[-0.3689]], device='mps:0'), tensor([[1.4857]], device='mps:0'), tensor([[-0.0416]], device='mps:0'), tensor([[0.3282]], device='mps:0'), tensor([[-0.3393]], device='mps:0'), tensor([[0.2818]], device='mps:0'), tensor([[-0.7668]], device='mps:0'), tensor([[1.3477]], device='mps:0'), tensor([[-0.0385]], device='mps:0'), tensor([[0.2682]], device='mps:0'), tensor([[0.4369]], device='mps:0'), tensor([[-0.4063]], device='mps:0'), tensor([[0.4942]], device='mps:0'), tensor([[0.1169]], device='mps:0'), tensor([[-1.5994]], device='mps:0'), tensor([[0.8888]], device='mps:0'), tensor([[1.2380]], device='mps:0'), tensor([[0.5268]], device='mps:0'), tensor([[0.9113]], device='mps:0'), tensor([[-0.2788]], device='mps:0'), tensor([[-0.7627]], device='mps:0'), tensor([[0.2360]], device='mps:0'), tensor([[1.4677]], device='mps:0'), tensor([[0.4888]], device='mps:0'), tensor([[0.0271]], device='mps:0'), tensor([[-0.5957]], device='mps:0'), tensor([[0.0911]], device='mps:0'), tensor([[1.0240]], device='mps:0'), tensor([[1.6481]], device='mps:0'), tensor([[-0.3191]], device='mps:0'), tensor([[0.0115]], device='mps:0'), tensor([[-1.0483]], device='mps:0'), tensor([[-0.6342]], device='mps:0'), tensor([[1.1283]], device='mps:0'), tensor([[-0.3149]], device='mps:0'), tensor([[-0.8459]], device='mps:0'), tensor([[0.2149]], device='mps:0'), tensor([[0.2619]], device='mps:0'), tensor([[0.6015]], device='mps:0')]\n",
    "# batch_states_t: tensor([[ 4.5612e-03, -5.9440e-03, -4.8425e-03,  5.3283e-03],\n",
    "#         [ 2.1461e-02, -4.5292e-02,  8.4738e-01, -1.9524e+00],\n",
    "#         [ 4.5854e-02, -1.0036e-01,  3.7533e-01, -8.2890e-01],\n",
    "#         [ 7.5390e-02, -1.6780e-01,  1.0988e+00, -2.5269e+00],\n",
    "#         [-3.5143e-03, -5.7780e-03,  8.2755e-03, -5.4098e-03],\n",
    "#         [-6.1724e-03,  8.4030e-04, -1.4078e-01,  3.3289e-01],\n",
    "#         [-1.8738e-02,  2.9888e-02, -4.8678e-01,  1.1141e+00],\n",
    "#         [-4.8847e-02,  9.8466e-02, -1.0175e+00,  2.3092e+00],\n",
    "#         [-8.6524e-02,  1.8344e-01, -8.6848e-01,  1.9619e+00],\n",
    "#         [-9.9881e-03,  8.2343e-03,  4.9069e-03,  1.2848e-03],\n",
    "#         [-1.8769e-02,  2.9196e-02, -4.4270e-01,  1.0363e+00],\n",
    "#         [-4.0273e-02,  7.8945e-02, -6.3264e-01,  1.4545e+00],\n",
    "#         [-7.2532e-02,  1.5312e-01, -9.7968e-01,  2.2549e+00],\n",
    "#         [ 8.1176e-04, -5.4549e-03, -2.0898e-03,  9.0584e-03],\n",
    "#         [-9.1878e-05, -3.2760e-03, -4.2993e-02,  9.9050e-02],\n",
    "#         [ 4.5314e-03, -1.4091e-02,  2.7323e-01, -6.3168e-01],\n",
    "#         [ 8.8704e-03, -2.3743e-02, -5.4890e-02,  1.3633e-01],\n",
    "#         [ 1.2224e-02, -3.1638e-02,  2.2169e-01, -5.2333e-01],\n",
    "#         [ 8.2108e-03, -2.2864e-02, -4.2026e-01,  9.4330e-01],\n",
    "#         [ 9.0227e-03, -2.7142e-02,  4.5771e-01, -1.1289e+00],\n",
    "#         [ 2.6448e-02, -6.9611e-02,  4.1456e-01, -1.0047e+00],\n",
    "#         [ 4.8237e-02, -1.2202e-01,  6.7462e-01, -1.6164e+00],\n",
    "#         [-3.7639e-03,  8.2444e-03, -5.4942e-03, -2.0568e-03],\n",
    "#         [-1.1695e-02,  2.6153e-02, -3.8999e-01,  8.8843e-01],\n",
    "#         [-1.8094e-02,  4.0083e-02,  6.8090e-02, -1.7408e-01],\n",
    "#         [-1.3169e-02,  2.8968e-02,  1.7802e-01, -3.8096e-01],\n",
    "#         [-2.4560e-02,  5.7393e-02, -7.4454e-01,  1.7766e+00],\n",
    "#         [-4.0037e-02,  9.4836e-02, -3.2908e-02,  1.2778e-01],\n",
    "#         [-2.4730e-02,  6.3336e-02,  7.9642e-01, -1.6853e+00],\n",
    "#         [ 1.6366e-02, -2.2557e-02,  1.2584e+00, -2.6133e+00],\n",
    "#         [ 8.0660e-02, -1.5706e-01,  1.9538e+00, -4.1080e+00],\n",
    "#         [ 2.8991e-03,  1.2733e-03, -8.0541e-04,  7.9827e-03],\n",
    "#         [-9.9628e-03,  3.1270e-02, -6.4051e-01,  1.4769e+00],\n",
    "#         [-3.0804e-02,  7.8464e-02, -4.0343e-01,  9.0071e-01],\n",
    "#         [-2.9077e-02,  7.3944e-02,  4.8683e-01, -1.0998e+00],\n",
    "#         [-8.9494e-04,  1.2327e-02,  9.2215e-01, -1.9816e+00],\n",
    "#         [ 3.6189e-02, -6.5191e-02,  9.3342e-01, -1.9101e+00],\n",
    "#         [ 6.2722e-02, -1.1600e-01,  3.9649e-01, -6.5983e-01],\n",
    "#         [ 8.0529e-02, -1.4827e-01,  4.9392e-01, -9.5565e-01],\n",
    "#         [-2.4337e-03, -8.6594e-03,  6.9423e-04,  3.7023e-03],\n",
    "#         [ 1.6138e-02, -5.1468e-02,  9.2517e-01, -2.1218e+00],\n",
    "#         [ 4.6761e-02, -1.2045e-01,  6.0870e-01, -1.3530e+00],\n",
    "#         [ 7.1371e-02, -1.7598e-01,  6.2259e-01, -1.4331e+00],\n",
    "#         [ 8.6990e-03, -6.3233e-03,  4.1285e-04,  7.0600e-03],\n",
    "#         [-2.4667e-03,  1.9695e-02, -5.5719e-01,  1.2810e+00],\n",
    "#         [-8.4385e-03,  3.2398e-02,  2.5537e-01, -6.1651e-01],\n",
    "#         [-4.4765e-03,  2.3398e-02, -5.5927e-02,  1.5399e-01],\n",
    "#         [-2.0489e-02,  6.1659e-02, -7.4270e-01,  1.7433e+00],\n",
    "#         [-4.5841e-02,  1.2080e-01, -5.2696e-01,  1.2333e+00],\n",
    "#         [-6.1909e-02,  1.5966e-01, -2.7817e-01,  7.2573e-01]], device='mps:0')\n",
    "# batch_actions_t: tensor([[ 1.2787],\n",
    "#         [-0.5051],\n",
    "#         [ 0.9226],\n",
    "#         [-0.4800],\n",
    "#         [-0.1513],\n",
    "#         [-0.3652],\n",
    "#         [-0.6029],\n",
    "#         [ 0.1439],\n",
    "#         [-0.1041],\n",
    "#         [-0.4831],\n",
    "#         [-0.1976],\n",
    "#         [-0.3689],\n",
    "#         [ 1.4857],\n",
    "#         [-0.0416],\n",
    "#         [ 0.3282],\n",
    "#         [-0.3393],\n",
    "#         [ 0.2818],\n",
    "#         [-0.7668],\n",
    "#         [ 1.3477],\n",
    "#         [-0.0385],\n",
    "#         [ 0.2682],\n",
    "#         [ 0.4369],\n",
    "#         [-0.4063],\n",
    "#         [ 0.4942],\n",
    "#         [ 0.1169],\n",
    "#         [-1.5994],\n",
    "#         [ 0.8888],\n",
    "#         [ 1.2380],\n",
    "#         [ 0.5268],\n",
    "#         [ 0.9113],\n",
    "#         [-0.2788],\n",
    "#         [-0.7627],\n",
    "#         [ 0.2360],\n",
    "#         [ 1.4677],\n",
    "#         [ 0.4888],\n",
    "#         [ 0.0271],\n",
    "#         [-0.5957],\n",
    "#         [ 0.0911],\n",
    "#         [ 1.0240],\n",
    "#         [ 1.6481],\n",
    "#         [-0.3191],\n",
    "#         [ 0.0115],\n",
    "#         [-1.0483],\n",
    "#         [-0.6342],\n",
    "#         [ 1.1283],\n",
    "#         [-0.3149],\n",
    "#         [-0.8459],\n",
    "#         [ 0.2149],\n",
    "#         [ 0.2619],\n",
    "#         [ 0.6015]], device='mps:0')\n",
    "# batch_actions_t sum: tensor([ 1.2787, -0.5051,  0.9226, -0.4800, -0.1513, -0.3652, -0.6029,  0.1439,\n",
    "#         -0.1041, -0.4831, -0.1976, -0.3689,  1.4857, -0.0416,  0.3282, -0.3393,\n",
    "#          0.2818, -0.7668,  1.3477, -0.0385,  0.2682,  0.4369, -0.4063,  0.4942,\n",
    "#          0.1169, -1.5994,  0.8888,  1.2380,  0.5268,  0.9113, -0.2788, -0.7627,\n",
    "#          0.2360,  1.4677,  0.4888,  0.0271, -0.5957,  0.0911,  1.0240,  1.6481,\n",
    "#         -0.3191,  0.0115, -1.0483, -0.6342,  1.1283, -0.3149, -0.8459,  0.2149,\n",
    "#          0.2619,  0.6015], device='mps:0')\n",
    "# batch_returns_t: tensor([ 2.9701,  1.9900,  1.0000,  0.0000,  3.9404,  2.9701,  1.9900,  1.0000,\n",
    "#          0.0000,  2.9701,  1.9900,  1.0000,  0.0000,  7.7255,  6.7935,  5.8520,\n",
    "#          4.9010,  3.9404,  2.9701,  1.9900,  1.0000,  0.0000,  7.7255,  6.7935,\n",
    "#          5.8520,  4.9010,  3.9404,  2.9701,  1.9900,  1.0000,  0.0000,  6.7935,\n",
    "#          5.8520,  4.9010,  3.9404,  2.9701,  1.9900,  1.0000,  0.0000,  2.9701,\n",
    "#          1.9900,  1.0000,  0.0000, 18.2093, 17.3831, 16.5486, 15.7057, 14.8542,\n",
    "#         13.9942, 13.1254], device='mps:0')\n",
    "# dis_t: Normal(loc: torch.Size([50, 1]), scale: torch.Size([50, 1]))\n",
    "# actions_prob_t:tensor([-0.4574, -0.7943, -0.4546, -0.7709, -0.6558, -0.8042, -1.0410, -0.7419,\n",
    "#         -0.8810, -0.7865, -0.8240, -0.9640,  0.1293, -0.6181, -0.4856, -0.6520,\n",
    "#         -0.5033, -0.8499, -0.1618, -0.5619, -0.4774, -0.4338, -0.7566, -0.4353,\n",
    "#         -0.5437, -1.8201, -0.1695, -0.4007, -0.3870, -0.3706, -0.6982, -0.9345,\n",
    "#         -0.6105, -0.2013, -0.4091, -0.5548, -0.8383, -0.5283, -0.4903, -0.7511,\n",
    "#         -0.7024, -0.5511, -1.1298, -0.8620, -0.1132, -0.6405, -1.0252, -0.6416,\n",
    "#         -0.5891, -0.3968], device='mps:0', grad_fn=<SubBackward0>)\n",
    "# loss_value: 48.21304702758789\n",
    "# adv_t: tensor([ 3.1246e+00,  2.3859e+00,  1.2986e+00,  4.4293e-01,  4.0973e+00,\n",
    "#          3.1171e+00,  2.0845e+00,  9.4994e-01, -5.3820e-03,  3.1267e+00,\n",
    "#          2.0919e+00,  1.0555e+00, -4.6113e-02,  7.8807e+00,  6.9455e+00,\n",
    "#          6.1175e+00,  5.0521e+00,  4.1826e+00,  3.0799e+00,  2.3132e+00,\n",
    "#          1.3146e+00,  3.7432e-01,  7.8808e+00,  6.9078e+00,  6.0290e+00,\n",
    "#          5.1133e+00,  3.9548e+00,  3.1183e+00,  2.3629e+00,  1.4416e+00,\n",
    "#          5.4254e-01,  6.9476e+00,  5.9029e+00,  5.0145e+00,  4.2586e+00,\n",
    "#          3.3650e+00,  2.3852e+00,  1.2755e+00,  3.1532e-01,  3.1266e+00,\n",
    "#          2.3989e+00,  1.3518e+00,  3.6137e-01,  1.8363e+01,  1.7457e+01,\n",
    "#          1.6806e+01,  1.5854e+01,  1.4873e+01,  1.4075e+01,  1.3249e+01],\n",
    "#        device='mps:0')\n",
    "# value_t: tensor([-0.1545, -0.3959, -0.2986, -0.4429, -0.1569, -0.1470, -0.0945,  0.0501,\n",
    "#          0.0054, -0.1566, -0.1019, -0.0555,  0.0461, -0.1552, -0.1521, -0.2655,\n",
    "#         -0.1511, -0.2422, -0.1098, -0.3232, -0.3146, -0.3743, -0.1552, -0.1143,\n",
    "#         -0.1771, -0.2123, -0.0144, -0.1482, -0.3729, -0.4416, -0.5425, -0.1541,\n",
    "#         -0.0509, -0.1135, -0.3182, -0.3949, -0.3952, -0.2755, -0.3153, -0.1565,\n",
    "#         -0.4089, -0.3518, -0.3614, -0.1536, -0.0741, -0.2573, -0.1479, -0.0183,\n",
    "#         -0.0805, -0.1232], device='mps:0', grad_fn=<SqueezeBackward1>)\n",
    "# loss_policy: 3.0877444744110107\n",
    "# entropy: 1.0807201862335205\n",
    "# loss_entropy: -0.010807201266288757"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9c35813",
   "metadata": {},
   "outputs": [],
   "source": [
    "# bs val: tensor([[0.1607]], device='mps:0', grad_fn=<LinearBackward0>)\n",
    "# ret tensor([[9.7072]], device='mps:0', grad_fn=<AddBackward0>)\n",
    "\n",
    "# # with .item(). ( correct)\n",
    "# bs val: 0.12476468086242676\n",
    "# ret 9.78975201174128\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "drl_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
