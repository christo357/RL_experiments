{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "99390014",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym \n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import typing as tt\n",
    "import torch  \n",
    "import torch.nn as nn \n",
    "import torch.nn.functional as F\n",
    "import wandb\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e93beee5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device : mps\n"
     ]
    }
   ],
   "source": [
    "HIDDEN_LAYER1  = 128\n",
    "BATCH_SIZE = 8\n",
    "GAMMA = 0.99\n",
    "LR = 0.001\n",
    "\n",
    "REWARD_STEP = 10\n",
    "ENTROPY_BETA = 0.01\n",
    "\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    device = 'cuda'\n",
    "elif torch.backends.mps.is_available():\n",
    "    device = 'mps'\n",
    "else:\n",
    "    device = 'cpu' \n",
    "print(f'Using device : {device}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d48be493",
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('CartPole-v1')\n",
    "eval_env = gym.make('CartPole-v1', render_mode='rgb_array')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c0297277",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4,)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.observation_space.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "4024470e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PolicyNet(nn.Module):\n",
    "    def __init__(self, input_size, fc, n_actions):\n",
    "        super().__init__()\n",
    "        self.input_size = input_size\n",
    "        self.fc = fc \n",
    "        self.n_actions = n_actions\n",
    "        \n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(self.input_size, self.fc), \n",
    "            nn.ReLU(), \n",
    "            \n",
    "        )\n",
    "        \n",
    "        self.policy_head = nn.Linear(self.fc, self.n_actions)\n",
    "        \n",
    "        self.critic_head = nn.Linear(self.fc, 1)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.net(x)\n",
    "        return self.policy_head(x), self.critic_head(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd06b292",
   "metadata": {},
   "outputs": [],
   "source": [
    "def experience_generator(env, policy, gamma, n_steps):\n",
    "    while True: \n",
    "        state_list = []\n",
    "        action_list = []\n",
    "        reward_list = []\n",
    "        return_list = []\n",
    "        done_list = []\n",
    "        last_state_list = []\n",
    "        \n",
    "        done = False\n",
    "        ep_rew = 0\n",
    "        state, _ = env.reset()\n",
    "        while not done:\n",
    "            state_t = torch.tensor(state, dtype=torch.float32, device = device).unsqueeze(0)\n",
    "            logits, value = policy(state_t)\n",
    "            # print(logits)\n",
    "            dist = torch.distributions.Categorical(logits=logits)\n",
    "            action = dist.sample().item()\n",
    "            # print(action)\n",
    "            \n",
    "            new_state, rew, term, trunc, info = env.step(action)\n",
    "            done = term or trunc\n",
    "            ep_rew += rew\n",
    "            state_list.append(state_t)\n",
    "            action_list.append(action)\n",
    "            reward_list.append(rew)\n",
    "            done_list.append(done)\n",
    "            \n",
    "            last_state_list.append(new_state)\n",
    "                \n",
    "            if len(reward_list)>=n_steps:\n",
    "                ret = sum([reward_list[i]* (gamma**i) for i in range(n_steps)])\n",
    "                \n",
    "                yield { \n",
    "                    'state':state_list[0], \n",
    "                    'action':int(action_list[0]),\n",
    "                    'ret':ret,\n",
    "                    'done':done,\n",
    "                    'last_state':last_state_list[n_steps-1], \n",
    "                    'ep_reward': None\n",
    "                }\n",
    "                \n",
    "                state_list.pop(0)\n",
    "                action_list.pop(0)\n",
    "                reward_list.pop(0)\n",
    "                done_list.pop(0)\n",
    "                last_state_list.pop(0)\n",
    "                \n",
    "            state = new_state\n",
    "                \n",
    "        else:\n",
    "            while len(reward_list)>0:\n",
    "                ret = sum([reward_list[i]* (gamma**i) for i in range(len(reward_list))])\n",
    "                \n",
    "                yield { \n",
    "                    'state':state_list[0], \n",
    "                    'action':int(action_list[0]),\n",
    "                    'ret':ret,\n",
    "                    'done':done,\n",
    "                    'last_state': None, \n",
    "                    'ep_reward': ep_rew if done_list[0] else None\n",
    "                }\n",
    "                \n",
    "                state_list.pop(0)\n",
    "                action_list.pop(0)\n",
    "                reward_list.pop(0)\n",
    "                done_list.pop(0)\n",
    "                last_state_list.pop(0)\n",
    "                \n",
    "def record_video(env, policy, device, max_steps=500):\n",
    "    \"\"\"Record a single episode and return frames + reward\"\"\"\n",
    "    frames = []\n",
    "    state, _ = env.reset()\n",
    "    done = False\n",
    "    total_reward = 0\n",
    "    steps = 0\n",
    "    \n",
    "    while not done and steps < max_steps:\n",
    "        frames.append(env.render())\n",
    "        state_tensor = torch.tensor(state, dtype=torch.float32, device=device).unsqueeze(0)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            logits = policy(state_tensor)\n",
    "            action = torch.distributions.Categorical(logits=logits).sample().item()\n",
    "            \n",
    "        state, reward, terminated, truncated, _ = env.step(action)\n",
    "        total_reward += reward\n",
    "        done = terminated or truncated\n",
    "        steps += 1\n",
    "        \n",
    "    return frames, total_reward, steps\n",
    "\n",
    "def smooth(old: tt.Optional[float], val: float, alpha: float = 0.95) -> float:\n",
    "    if old is None:\n",
    "        return val\n",
    "    return old * alpha + (1-alpha)*val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b6fec64",
   "metadata": {},
   "outputs": [
    {
     "ename": "Error",
     "evalue": "You must call wandb.init() before wandb.log()",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mError\u001b[39m                                     Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[81]\u001b[39m\u001b[32m, line 145\u001b[39m\n\u001b[32m    139\u001b[39m l_total = smooth(l_total, loss_total.item())\n\u001b[32m    143\u001b[39m \u001b[38;5;66;03m# break\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m145\u001b[39m \u001b[43mwandb\u001b[49m\u001b[43m.\u001b[49m\u001b[43mlog\u001b[49m\u001b[43m(\u001b[49m\u001b[43m{\u001b[49m\n\u001b[32m    146\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# 'baseline':baseline,\u001b[39;49;00m\n\u001b[32m    147\u001b[39m \u001b[43m    \u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43madvantage\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43madv_smoothed\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    148\u001b[39m \u001b[43m    \u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mentropy\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43mentropy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    149\u001b[39m \u001b[43m    \u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mloss_policy\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43ml_policy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    150\u001b[39m \u001b[43m    \u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mloss_value\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43ml_value\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    151\u001b[39m \u001b[43m    \u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mloss_entropy\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43ml_entropy\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[32m    152\u001b[39m \u001b[43m    \u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mloss_total\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43ml_total\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    153\u001b[39m \u001b[43m    \u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mkl div\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mkl_div\u001b[49m\u001b[43m.\u001b[49m\u001b[43mitem\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    154\u001b[39m \u001b[43m    \u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mgrad_l2\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43mgrad_means\u001b[49m\u001b[43m/\u001b[49m\u001b[43mgrad_count\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    155\u001b[39m \u001b[43m    \u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mgrad_max\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43mgrad_max\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    156\u001b[39m \u001b[43m    \u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mbatch_scales\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_returns\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    157\u001b[39m \u001b[43m    \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mcurrent_episode\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mepisode_idx\u001b[49m\n\u001b[32m    158\u001b[39m \u001b[43m\u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstep\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mstep_idx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    160\u001b[39m batch_actions.clear()\n\u001b[32m    161\u001b[39m batch_returns.clear()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniforge3/envs/drl_env/lib/python3.12/site-packages/wandb/sdk/lib/preinit.py:36\u001b[39m, in \u001b[36mPreInitCallable.<locals>.preinit_wrapper\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m     35\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mpreinit_wrapper\u001b[39m(*args: Any, **kwargs: Any) -> Any:\n\u001b[32m---> \u001b[39m\u001b[32m36\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m wandb.Error(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mYou must call wandb.init() before \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m()\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[31mError\u001b[39m: You must call wandb.init() before wandb.log()"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "policy = PolicyNet(\n",
    "    env.observation_space.shape[0], \n",
    "    128, \n",
    "    env.action_space.n\n",
    ").to(device)\n",
    "optimizer = torch.optim.Adam(policy.parameters(),lr=LR, )\n",
    "\n",
    "batch_states = []\n",
    "batch_returns = []\n",
    "batch_actions = []\n",
    "batch_values = []\n",
    "done_list = []\n",
    "last_state_list = []\n",
    "total_rewards = []\n",
    "adv_smoothed = l_entropy = l_policy = l_value = l_total = None\n",
    "episode_idx = 0\n",
    "\n",
    "for step_idx, exp in enumerate(experience_generator(env, policy, GAMMA, REWARD_STEP)):\n",
    "    batch_states.append(exp['state']) \n",
    "    batch_actions.append(exp['action'])\n",
    "    # print(exp['done'])\n",
    "    \n",
    "    ## bootstrapping if the episode is not completed withing REWARD_STEP\n",
    "    if exp['last_state'] is not None:\n",
    "        last_state = exp['last_state']\n",
    "        last_state_t = torch.tensor(last_state, dtype=torch.float32, device=device).unsqueeze(0)\n",
    "        _, bs_val = policy(last_state_t)\n",
    "        ret = exp['ret'] +  (bs_val.item()) * (GAMMA**REWARD_STEP) \n",
    "        batch_returns.append(ret)\n",
    "    else:\n",
    "        batch_returns.append(exp['ret'])\n",
    "        \n",
    "        \n",
    "    if exp['done']==True:\n",
    "        episode_reward = exp['ep_reward']\n",
    "        total_rewards.append(episode_reward)\n",
    "        mean_reward = float(np.mean(total_rewards[-100:]))\n",
    "        print(f\"episode : {episode_idx} | step: {step_idx} | episode reward : {episode_reward} | mean reward/100 eps : {mean_reward}\")\n",
    "        wandb.log({\n",
    "            \"episode_reward\": episode_reward, \n",
    "            \"mean_reward_100\": mean_reward,  # FIXED: No spaces in metric name\n",
    "            'episode_number': episode_idx,   # FIXED: More descriptive name\n",
    "            \"steps_per_episode\": step_idx / max(episode_idx, 1)\n",
    "        }, step=step_idx)\n",
    "        episode_idx += 1\n",
    "        \n",
    "        if mean_reward>450:\n",
    "            print(f\"Solved! Mean reward > 450 at episode {episode_idx}\")\n",
    "            break\n",
    "        \n",
    "        \n",
    "    # eval logging - periodic videos\n",
    "    if (episode_idx%1000==0 and episode_idx>0):\n",
    "        print(f\"Recording periodic video at episode {episode_idx}...\")\n",
    "        frames, eval_reward, eval_steps = record_video(eval_env, policy, device)\n",
    "            \n",
    "        wandb.log({\n",
    "            \"video\": wandb.Video(\n",
    "                np.array(frames).transpose(0, 3, 1, 2), \n",
    "                fps=30, \n",
    "                format=\"mp4\",\n",
    "                caption=f\"Episode {episode_idx} - Reward: {eval_reward}, Steps: {eval_steps}, Mean100: {mean_reward:.1f}\"\n",
    "            ),\n",
    "            \"eval_reward\": eval_reward\n",
    "        }, step=step_idx)\n",
    "        print(f\"Eval reward: {eval_reward}, steps: {eval_steps}\")\n",
    "        \n",
    "        \n",
    "        \n",
    "    if len(batch_states) < BATCH_SIZE:\n",
    "        continue\n",
    "    # print(f\"batch_actions: {batch_actions}\")\n",
    "    batch_states_t = torch.cat(batch_states, dim=0)\n",
    "    batch_actions_t = torch.tensor(batch_actions, dtype=torch.long, device=device)\n",
    "    batch_returns_t = torch.tensor(batch_returns, dtype=torch.float32, device=device)\n",
    "    \n",
    "    logits_t, value_t = policy(batch_states_t)\n",
    "    value_t = value_t.squeeze(-1)\n",
    "    dist_t = torch.distributions.Categorical(logits=logits_t)\n",
    "    actions_prob_t = dist_t.log_prob(batch_actions_t)\n",
    "    \n",
    "    loss_value = F.mse_loss(value_t, batch_returns_t.detach())\n",
    "    \n",
    "    \n",
    "    adv_t = (batch_returns_t - value_t).detach()\n",
    "    loss_policy = - (actions_prob_t * adv_t).mean()\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "    entropy = dist_t.entropy().mean()\n",
    "    loss_entropy = -ENTROPY_BETA*entropy\n",
    "    # print(f\"batch_states_t: {batch_states_t}\")\n",
    "    # print(f\"batch_actions_t: {batch_actions_t}\")\n",
    "\n",
    "    # print(f\"batch_returns_t: {batch_returns_t}\")\n",
    "    \n",
    "    # print(f\"dis_t: {dist_t}\")\n",
    "    \n",
    "    # print(f\"loss_value: {loss_value}\")\n",
    "\n",
    "    # print(f\"adv_t: {adv_t}\")\n",
    "    # print(f\"value_t: {value_t}\")\n",
    "    # print(f\"loss_policy: {loss_policy}\")\n",
    "    # print(f\"entropy: {entropy}\")\n",
    "    # print(f\"loss_entropy: {loss_entropy}\")\n",
    "\n",
    "    \n",
    "    loss_total = loss_value + loss_policy + loss_entropy\n",
    "    \n",
    "    optimizer.zero_grad()\n",
    "    loss_total.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    \n",
    "    with torch.no_grad():\n",
    "        l_t, v_t = policy(batch_states_t)\n",
    "        new_dist_t = torch.distributions.Categorical(logits=l_t)\n",
    "        \n",
    "        kl_div = torch.distributions.kl_divergence(dist_t, new_dist_t).mean()\n",
    "        \n",
    "    grad_max = 0.0\n",
    "    grad_means = 0.0\n",
    "    grad_count = 0\n",
    "    for p in policy.parameters():\n",
    "        grad_max = max(grad_max, p.grad.abs().max().item())\n",
    "        grad_means += (p.grad ** 2).mean().sqrt().item()\n",
    "        grad_count += 1\n",
    "        \n",
    "        \n",
    "    adv_smoothed = smooth(\n",
    "                    adv_smoothed,\n",
    "                    float(np.mean(adv_t.mean().item()))\n",
    "                )\n",
    "    l_entropy = smooth(l_entropy, loss_entropy.item())\n",
    "    l_policy = smooth(l_policy, loss_policy.item())\n",
    "    l_value = smooth(l_value, loss_value.item())\n",
    "    l_total = smooth(l_total, loss_total.item())\n",
    "    \n",
    "    \n",
    "    \n",
    "    # break\n",
    "\n",
    "    wandb.log({\n",
    "        # 'baseline':baseline,\n",
    "        'advantage':adv_smoothed,\n",
    "        'entropy':entropy,\n",
    "        'loss_policy':l_policy,\n",
    "        'loss_value':l_value,\n",
    "        'loss_entropy': l_entropy, \n",
    "        'loss_total': l_total,\n",
    "        'kl div': kl_div.item(),\n",
    "        'grad_l2':grad_means/grad_count,\n",
    "        'grad_max':grad_max,\n",
    "        'batch_scales': batch_returns,\n",
    "        \"current_episode\": episode_idx\n",
    "    }, step = step_idx)\n",
    "    \n",
    "    batch_actions.clear()\n",
    "    batch_returns.clear()\n",
    "    batch_states.clear()\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7e5de70",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # logit, actions without .unsqueeze(0)\n",
    "# tensor([0.1524, 0.2040], device='mps:0', grad_fn=<LinearBackward0>)\n",
    "# tensor(1, device='mps:0')\n",
    "\n",
    "# # logit, actions with .unsqueeze(0)\n",
    "# tensor([[-0.1604, -0.1145]], device='mps:0', grad_fn=<LinearBackward0>)\n",
    "# tensor([0], device='mps:0')\n",
    "\n",
    "#single loop outputs\n",
    "# batch_states_t: tensor([[-0.0132, -0.0058,  0.0260, -0.0497],\n",
    "#         [-0.0133,  0.1889,  0.0250, -0.3341],\n",
    "#         [-0.0096, -0.0066,  0.0183, -0.0336],\n",
    "#         [-0.0097,  0.1883,  0.0176, -0.3205],\n",
    "#         [-0.0059,  0.3832,  0.0112, -0.6075],\n",
    "#         [ 0.0017,  0.5781, -0.0009, -0.8966],\n",
    "#         [ 0.0133,  0.3830, -0.0188, -0.6043],\n",
    "#         [ 0.0210,  0.1882, -0.0309, -0.3176]], device='mps:0')\n",
    "# batch_actions_t: tensor([1, 0, 1, 1, 1, 0, 0, 0], device='mps:0')\n",
    "# batch_returns_t: tensor([9.7298, 9.6851, 9.7275, 9.7533, 9.7653, 9.7534, 9.7271, 9.6851],\n",
    "#        device='mps:0')\n",
    "# dis_t: Categorical(probs: torch.Size([8, 2]), logits: torch.Size([8, 2]))\n",
    "# loss_value: 91.0837631225586\n",
    "# adv_t: tensor([9.5897, 9.4994, 9.5912, 9.5700, 9.5535, 9.5284, 9.5153, 9.5023],\n",
    "#        device='mps:0', grad_fn=<SubBackward0>)\n",
    "# value_t: tensor([0.1401, 0.1857, 0.1364, 0.1833, 0.2117, 0.2250, 0.2118, 0.1828],\n",
    "#        device='mps:0', grad_fn=<SqueezeBackward0>)\n",
    "# loss_policy: 6.806088447570801\n",
    "# entropy: 0.6919518113136292\n",
    "# loss_entropy: -0.0069195181131362915"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9c35813",
   "metadata": {},
   "outputs": [],
   "source": [
    "# bs val: tensor([[0.1607]], device='mps:0', grad_fn=<LinearBackward0>)\n",
    "# ret tensor([[9.7072]], device='mps:0', grad_fn=<AddBackward0>)\n",
    "\n",
    "# # with .item(). ( correct)\n",
    "# bs val: 0.12476468086242676\n",
    "# ret 9.78975201174128\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "drl_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
